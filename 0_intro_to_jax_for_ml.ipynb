{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# **Intro to JAX for ML**\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/google/jax/main/images/jax_logo_250px.png\" width=\"60%\" />\n",
        "\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2022/blob/main/prac1/prac_0_intro_to_jax_using_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "Â© Deep Learning Indaba 2022. Apache License 2.0.\n",
        "\n",
        "**Authors:** Kale-ab Tessera\n",
        "\n",
        "**Introduction:** \n",
        "\n",
        "JAX is a python package for writing composable numerical transformations [[1]](https://jax.readthedocs.io/en/latest/index.html). It leverages [Autograd](https://github.com/hips/autograd) and [XLA](https://www.tensorflow.org/xla) (Accelerated Linear Algebra), to achieve high-performance numerical computing (particularly relevant in machine learning). It provides functionality such as automatic differentiation (`Grad`), parallelization (`pmap`), vectorization (`vmap`), just-in-time compilation (`JIT`), and more.  \n",
        "\n",
        "JAX is **not** a replacement for Pytorch or Tensorflow, but a lower-level library commonly used with higher-level neural network libraries such as [Haiku](https://github.com/deepmind/dm-haiku) or [Flax](https://github.com/google/flax).  \n",
        "\n",
        "**Topics:** \n",
        "\n",
        "Content: <font color='orange'>`Numerical Computing`</font>  \n",
        "Level: <font color='grey'>`Beginner`</font>\n",
        "\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "- Learn the basics of JAX and its similiarities and differences with numpy.\n",
        "- Learn how to use JAX transforms - `jit`, `grad`, `vmap` and `pmap`.  \n",
        "- Learn how to build simple classifiers using JAX. \n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "- Basic knowledge of [NumPy](https://github.com/numpy/numpy).\n",
        "- Basic knowledge of [functional programming](https://en.wikipedia.org/wiki/Functional_programming). \n",
        "\n",
        "**Outline:** \n",
        "\n",
        ">[Part 1 - Basics of JAX](#scrollTo=Enx0WUr8tIPf)\n",
        "\n",
        ">>[1.1 From NumPy âž¡ Jax - Beginner](#scrollTo=-ZUp8i37dFbU)\n",
        "\n",
        ">>>[JAX and NumPy - Similarities  ðŸ¤](#scrollTo=CbOEYsWQ6tHv)\n",
        "\n",
        ">>>[JAX and NumPy - Differences âŒ](#scrollTo=lg4__l4A7yqc)\n",
        "\n",
        "\n",
        ">>[1.2 Acceleration in JAX ðŸš€](#scrollTo=TSj972IWxTo2)\n",
        "\n",
        ">>>[JAX is backend Agnostic - Beginner](#scrollTo=_bQ9QqT-yKbs)\n",
        "\n",
        ">>>[JAX Transformations](#scrollTo=JM_08mXEBRIK)\n",
        "\n",
        ">>>>[Basic JAX Transformations - JIT and GRAD - Beginner](#scrollTo=cOGuGWtLmP7n)\n",
        "\n",
        ">>>>[More Advanced Transforms - VMAP and PMAP - Intermediate, Advanced](#scrollTo=tvBzh8wiGuLf)\n",
        "\n",
        ">>[Section Quiz](#scrollTo=WILOYJH4gCnD)\n",
        "\n",
        ">[Part 2 - From Linear to Non-Linear Regression - WIP](#scrollTo=aB0503xgmSFh)\n",
        "\n",
        ">>[Linear Regression](#scrollTo=XrWSN-zaWAhJ)\n",
        "\n",
        ">[Conclusion](#scrollTo=fV3YG7QOZD-B)\n",
        "\n",
        ">>[Feedback](#scrollTo=o1ndpYE50BpG)\n",
        "\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "For this practical, you will need to use a GPU to speed up training. To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"GPU\" in the \"Hardware accelerator\" box.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4boGA9rYdt9l"
      },
      "outputs": [],
      "source": [
        "## Install and import anything required. Capture hides the output from the cell. \n",
        "#@title Install and import required packages. (Run Cell)\n",
        "\n",
        "import os \n",
        "\n",
        "# https://stackoverflow.com/questions/68340858/in-google-colab-is-there-a-programing-way-to-check-which-runtime-like-gpu-or-tpu\n",
        "if int(os.environ[\"COLAB_GPU\"]) > 0:\n",
        "  print(\"a GPU is connected.\")\n",
        "elif \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
        "  print(\"A TPU is connected.\")\n",
        "  import jax.tools.colab_tpu\n",
        "  jax.tools.colab_tpu.setup_tpu()\n",
        "else:\n",
        "  print(\"Only CPU accelerator is connected.\")\n",
        "  # x8 cpu devices - number of (emulated) host devices\n",
        "  os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=8'\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap, pmap\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper Functions. (Run Cell)\n",
        "import copy\n",
        "from typing import Dict \n",
        "def plot_performance(data:Dict, title: str):\n",
        "  runs = list(data.keys())\n",
        "  time = list(data.values())\n",
        "  \n",
        "  # creating the bar plot\n",
        "  plt.bar(runs, time, width = 0.35)\n",
        "  \n",
        "  plt.xlabel(\"Implementation\")\n",
        "  plt.ylabel(\"Average time taken (in s)\")\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "\n",
        "  best_perf_key = min(data, key=data.get)\n",
        "  all_runs_key = copy.copy(runs)\n",
        "\n",
        "  # all_runs_key_except_best\n",
        "  all_runs_key.remove(best_perf_key)\n",
        "\n",
        "  for k in all_runs_key:\n",
        "    print(f\"{best_perf_key} was {round((data[k]/data[best_perf_key]),2)} times faster than {k} !!!\")"
      ],
      "metadata": {
        "id": "YQe1CfDyrkdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Check the device you are using (Run Cell)\n",
        "print(f\"Num devices: {jax.device_count()}\")\n",
        "print(f\" Devices: {jax.devices()}\")"
      ],
      "metadata": {
        "id": "yFzjRHUsUQqq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 - Basics of JAX"
      ],
      "metadata": {
        "id": "Enx0WUr8tIPf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZUp8i37dFbU"
      },
      "source": [
        "## 1.1 From NumPy âž¡ Jax - <font color='blue'>`Beginner`</font>\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JAX and NumPy - Similarities  ðŸ¤\n",
        "\n",
        "The main similiarity between JAX and NumPy is that they share a similiar interface and often times, JAX and NumPy arrays can be used interchanbly. "
      ],
      "metadata": {
        "id": "CbOEYsWQ6tHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the sine functions using numpy."
      ],
      "metadata": {
        "id": "KbYfoaujT2F7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 100 linearly spaced numbers from -np.pi to np.pi\n",
        "x = np.linspace(-np.pi,np.pi,100)\n",
        "\n",
        "# the function, which is y = sin(x) here\n",
        "y = np.sin(x)\n",
        "\n",
        "# plot the functions\n",
        "plt.plot(x,y, 'b', label='y=sin(x)')\n",
        "\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sgRLq58OTz1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now using jax.numpy - `jnp` \n",
        "\n",
        "(We already imported this in the first cell as follows- `import jax.numpy as jnp`)"
      ],
      "metadata": {
        "id": "XCEnlC-PU3ps"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 100 linearly spaced numbers from -np.pi to np.pi\n",
        "x = jnp.linspace(-np.pi,np.pi,100)\n",
        "\n",
        "# the function, which is y = sin(x) here\n",
        "y = jnp.sin(x)\n",
        "\n",
        "# plot the functions\n",
        "plt.plot(x,y, 'b', label='y=sin(x)')\n",
        "\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kRQf2mNRTlt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Task:** Can you plot the cosine function using `jnp`?"
      ],
      "metadata": {
        "id": "wuNscwHeV_dn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot Cosine using jnp. (UPDATE ME)\n",
        "\n",
        "# 100 linearly spaced numbers\n",
        "# UPDATE ME\n",
        "x = ...\n",
        "\n",
        "# UPDATE ME\n",
        "y = ...  \n",
        "\n",
        "if (y == ... or x == ...):\n",
        "  raise Exception(\"Update ME!\")\n",
        "\n",
        "# plot the functions\n",
        "plt.plot(x,y, 'b', label='y=cos(x)')\n",
        "\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5svZFPUCQNsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!') \n",
        "# 100 linearly spaced numbers\n",
        "x = jnp.linspace(-np.pi,np.pi,100)\n",
        "\n",
        "y = jnp.cos(x)  \n",
        "\n",
        "# plot the functions\n",
        "plt.plot(x,y, 'b', label='y=cos(x)')\n",
        "\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "# show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m4AVrGzy6JWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JAX and NumPy - Differences âŒ\n",
        "\n",
        "Although JAX and NumPy have some similiarities, they do have some important differences:\n",
        "- Jax arrays are **immutable** (they can't be modified after they are created).\n",
        "- The way they handle **randomness**."
      ],
      "metadata": {
        "id": "lg4__l4A7yqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### JAX arrays are immutable, while NumPy arrays are not.\n",
        "\n",
        "JAX and NumPy arrays are often interchangeable, **but** Jax arrays are **immutable** (they can't be modified after they are created). \n",
        "\n",
        "Let's see this in practice by adding a number to the beginning of an array. "
      ],
      "metadata": {
        "id": "dPbOnhE4ZSTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NumPy: mutable arrays\n",
        "x = np.arange(10)\n",
        "x[0] = 10\n",
        "print(x)"
      ],
      "metadata": {
        "id": "7r-Los6YZR-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JAX: immutable arrays\n",
        "x = jnp.arange(10)\n",
        "x[0] = 10"
      ],
      "metadata": {
        "id": "OxjkKpqAZxWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it fails! We can't mutate a JAX array once it has been created. To update JAX arrays, we need to use [helper functions](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.ndarray.at.html) that return an updated copy of the JAX array. \n",
        "\n",
        "Instead of doing this `x[idx] = y`, we need to do this `x = x.at[idx].set(y)`. "
      ],
      "metadata": {
        "id": "VoWT5RBUagW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = jnp.arange(10)\n",
        "x = x.at[0].set(10)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "qJYxkh4qagwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Randomness in NumPy vs JAX \n",
        "\n",
        "JAX is more explicit in Pseudo Random Number Generation (PRNG) than NumPy and other libraries (such as tensorflow or pytorch). [PRNG](https://en.wikipedia.org/wiki/Pseudorandom_number_generator) is the process of algorithmically generating a sequence of numbers, which *approximate* the properties of a sequence of random numbers.  \n",
        "\n",
        "Let's see the differences in how JAX and NumPy generate random numberss."
      ],
      "metadata": {
        "id": "oAH4c_smdGQU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### In Numpy, PRNG is based on a global `state`.\n",
        "\n",
        "Let's set the initial seed."
      ],
      "metadata": {
        "id": "Q2m376Ethf8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed\n",
        "np.random.seed(42)\n",
        "prng_state = np.random.get_state()"
      ],
      "metadata": {
        "id": "-0t3sjxzdgmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper function to compare prng keys (Run Cell)\n",
        "def is_prng_state_the_same(prng_1,prng_2):\n",
        "  \"\"\"Helper function to compare two prng keys.\"\"\"\n",
        "  # concat all elements in prng tuple \n",
        "  list_prng_data_equal = [(a == b) for a, b in zip(prng_1,prng_2)]\n",
        "  # stack all elements together\n",
        "  list_prng_data_equal = np.hstack(list_prng_data_equal)\n",
        "  # check if all elements are the same\n",
        "  is_prng_equal=all(list_prng_data_equal)\n",
        "  return is_prng_equal"
      ],
      "metadata": {
        "id": "QKVz5atZMMOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a few samples from a Gaussian (normal) Distribution and check if prng keys change."
      ],
      "metadata": {
        "id": "nloZ9abah3J3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"sample 1 = {np.random.normal()} Did prng state change: {not is_prng_state_the_same(prng_state,np.random.get_state())}\")\n",
        "prng_state = np.random.get_state()\n",
        "print(f\"sample 2 = {np.random.normal()} Did prng state change: {not is_prng_state_the_same(prng_state,np.random.get_state())}\")\n",
        "prng_state = np.random.get_state()\n",
        "print(f\"sample 3 = {np.random.normal()} Did prng state change: {not is_prng_state_the_same(prng_state,np.random.get_state())}\")"
      ],
      "metadata": {
        "id": "aiUcfX7iSenY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numpy's global random state is updated everytime a random number is generated, so sample 1 != sample 2 != sample 3. \n",
        "\n",
        "Having the state automatically updated, makes it difficult to handle randomness in a **reproducible** way across threads, processes and devices. "
      ],
      "metadata": {
        "id": "nuHkW6V4iLa9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### In JAX, PRNG is more explicit.\n",
        "\n",
        "In JAX, for each random number generation, you need to explicitly pass in a random key/state."
      ],
      "metadata": {
        "id": "lGDU6ckKkzqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Passing the same state/key results in the same number being generated. This is generally undesirable."
      ],
      "metadata": {
        "id": "6oKdk5CSmD-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import random\n",
        "key = random.PRNGKey(42)\n",
        "print(f\"sample 1 = {random.normal(key)}\")\n",
        "print(f\"sample 2 = {random.normal(key)}\")\n",
        "print(f\"sample 3 = {random.normal(key)}\")"
      ],
      "metadata": {
        "id": "Y-6B0hjtlTmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate different and indepedent samples, you need to manually **split** the keys. "
      ],
      "metadata": {
        "id": "l0KcwEbZqIaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import random\n",
        "key = random.PRNGKey(42)\n",
        "print(f\"sample 1 = {random.normal(key)}\")\n",
        "\n",
        "# We split the key -> key and subkey\n",
        "key, subkey = random.split(key)\n",
        "\n",
        "# We use the subkey immediately and keep the key for future splits. It doesn't really matter which key we keep and which one we use immediately. \n",
        "print(f\"sample 2 = {random.normal(subkey)}\")\n",
        "\n",
        "# We split the new key -> key and subkey\n",
        "key, subkey = random.split(key)\n",
        "print(f\"sample 3 = {random.normal(subkey)}\")"
      ],
      "metadata": {
        "id": "v-7BhY0MmEhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By using JAX, we can more easily reproduce random number generation in parallel across threads, processes or even devices by explicitly passing and keeping track of the prng key (without relying on a global state that automatically gets updated). For more details on PRNG in JAX, you can read more [here](https://github.com/google/jax/blob/main/docs/design_notes/prng.md). "
      ],
      "metadata": {
        "id": "2VnTDptmuk-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Acceleration in JAX ðŸš€ \n",
        "\n",
        "JAX leverages Autograd and XLA for accelerating numerical computation. The use of Autograd allows for automatic differentiation (`grad`), while XLA allows JAX to run on multiple accelerators/backends and run transforms like `jit`, `vmap` and `pmap`.  "
      ],
      "metadata": {
        "id": "TSj972IWxTo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JAX is backend Agnostic - <font color='blue'>`Beginner`</font>\n",
        "\n",
        "Using JAX, you can run the same code on different backends/AI accelerators (e.g. CPU/GPU/TPU), **with no changes in code** (no more `.to(device)` - from frameworks like PyTorch). This means we can easily run linear algebra operations directly on gpu/tpu."
      ],
      "metadata": {
        "id": "_bQ9QqT-yKbs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PbcFsfAibBu"
      },
      "source": [
        "**Multiplying Matrices**\n",
        "\n",
        "Dot products are a common operation in numerical computing and a central part of modern deep learning. They are defined over [vectors](https://en.wikipedia.org/wiki/Coordinate_vector), which can loosely be thought of as a list of multiple scalers (single values). \n",
        "\n",
        "Formally, given two vectors $\\boldsymbol{x}$,$\\boldsymbol{y}$ $\\in R^n$, their dot product is defined as:\n",
        "\n",
        "<center>$\\boldsymbol{x}^{\\top} \\boldsymbol{y}=\\sum_{i=1}^{n} x_{i} y_{i}$</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dot Product in NumPy (will run on cpu)"
      ],
      "metadata": {
        "id": "AY1RsVkXaokP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "size=1000\n",
        "x = np.random.normal(size=(size, size))\n",
        "y = np.random.normal(size=(size, size))\n",
        "numpy_time = %timeit -o -n 10 a_np = np.dot(y,x.T)"
      ],
      "metadata": {
        "id": "yj59KkD_HDOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dot Product using JAX (will run on current runtime - e.g. GPU)."
      ],
      "metadata": {
        "id": "6c_kl-u0KPVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "size=1000\n",
        "key = jax.random.PRNGKey(42)\n",
        "x = jax.random.normal(key,shape=(size, size))\n",
        "y = jax.random.normal(key,shape=(size, size))\n",
        "jax_time = %timeit -o -n 10 jnp.dot(y, x.T).block_until_ready()"
      ],
      "metadata": {
        "id": "PHRcHK86KO3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When timing JAX functions, we use `.block_until_ready()` because JAX uses [asynchronous dispatch](https://jax.readthedocs.io/en/latest/async_dispatch.html#async-dispatch). This means JAX doesn't wait for the operation to complete before returning control to your code. To fairly compute the time taken for JAX operations, we therefore block until the operation is done."
      ],
      "metadata": {
        "id": "LMTSpEG3TNah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How much faster was the dot product in JAX (Using GPU)?"
      ],
      "metadata": {
        "id": "S3vwh6Q724gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np_average_time=np.mean(numpy_time.all_runs)\n",
        "jax_average_time=np.mean(jax_time.all_runs)\n",
        "data = {'numpy':np_average_time, 'jax':jax_average_time}\n",
        "\n",
        "plot_performance(data,title=\"Average time taken per framework to run dot product\")"
      ],
      "metadata": {
        "id": "UkASX9p34A1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are running on accelerator, you should see a considerable performance benefit of using JAX, without making any changes to your code! "
      ],
      "metadata": {
        "id": "U9ChePvzAVaa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JAX Transformations \n",
        "\n",
        "JAX transforms (e.g. jit, grad, vmap, pmap) first convert python functions into an intermediate language called jaxpr. Transforms are then applied to this jaxpr representation.\n",
        "\n",
        "JAX generates jaxpr, in a process known as **tracing**. During tracing, function inputs are wrapped by a tracer object and then JAX records all operations (including regular python code) that occur during the function call. These recorded operations are used to reconstruct the function. Any python side-effects are not recording during tracing. For more on tracing and jaxpr, you can read [here](https://jax.readthedocs.io/en/latest/jaxpr.html).\n",
        "\n"
      ],
      "metadata": {
        "id": "JM_08mXEBRIK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOGuGWtLmP7n"
      },
      "source": [
        "#### Basic JAX Transformations - `JIT` and `GRAD` - <font color='blue'>`Beginner`</font>\n",
        "\n",
        "In this section, we will explore two basic JAX transforms: \n",
        "- JIT (Just-in-time compilation) - compiles and caches JAX Python functions so that they can be run efficiently on XLA - `speed up functions`.\n",
        "- Grad - Automatically compute gradients - `automatic differentiation`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### JIT\n",
        "\n",
        "Jax dispatches operations to accelerators one at a time. If we have repeated operations, we can use `jit` to compile the function the first time it is called, then subsequent calls will be cached. "
      ],
      "metadata": {
        "id": "QsJE_U-ZzVol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compile [ReLU (Rectified Linear Unit)](https://arxiv.org/abs/1803.08375), a popular activation function in deep learning. \n",
        "\n",
        "ReLU is defined as follows:\n",
        "<center>$f(x)=max(0,x)$</center>\n",
        "\n",
        "It can be visualized as follows:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png\" width=\"35%\" />\n",
        "</center>,\n",
        "\n",
        "where $x$ is the input to the function and $y$ is output of ReLU.\n"
      ],
      "metadata": {
        "id": "uIYsqIp_-Dly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$f(x)=\\max (0, x)=\\left\\{\\begin{array}{l}x_{i} \\text { if } x_{i}>0 \\\\ 0 \\text { if } x_{i}<=0\\end{array}\\right.$$"
      ],
      "metadata": {
        "id": "Vm-bN9sQETLV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Task:** Complete the ReLU implementation below."
      ],
      "metadata": {
        "id": "dFiuu3BFAKdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement ReLU.\n",
        "def relu(x):\n",
        "  if x > 0:\n",
        "    return\n",
        "    # TODO Implement me! \n",
        "  else:\n",
        "    return\n",
        "    # TODO Implement me! "
      ],
      "metadata": {
        "id": "1_qMJJbs-Cbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run to test your ReLU function.\n",
        "\n",
        "def plot_relu(relu_function):\n",
        "  max_int = 5\n",
        "  # Generete 100 evenly spaced points from -max_int to max_int \n",
        "  x = np.linspace(-max_int,max_int,1000)\n",
        "  y = np.array([relu_function(xi) for xi in x])\n",
        "  plt.plot(x, y,label='ReLU')\n",
        "  plt.legend(loc=\"upper left\")\n",
        "  plt.xticks(np.arange(min(x), max(x)+1, 1))\n",
        "  plt.show()\n",
        "\n",
        "def check_relu_function(relu_function):\n",
        "  # Generete 100 evenly spaced points from -100 to -1\n",
        "  x = np.linspace(-100,-1,100)\n",
        "  y = np.array([relu_function(xi) for xi in x])\n",
        "  assert (y == 0).all()\n",
        "\n",
        "  # Check if x == 0\n",
        "  x = 0\n",
        "  y = relu_function(x)\n",
        "  assert y == 0\n",
        "\n",
        "  # Generete 100 evenly spaced points from 0 to 100\n",
        "  x = np.linspace(0,100,100)\n",
        "  y = np.array([relu_function(xi) for xi in x])\n",
        "  assert np.allclose(x, y)\n",
        "\n",
        "  print(\"Your ReLU function is correct!\")\n",
        "\n",
        "check_relu_function(relu)\n",
        "plot_relu(relu)"
      ],
      "metadata": {
        "id": "zCobLakM1esy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!') \n",
        "def relu(x):\n",
        "  if x > 0:\n",
        "    return x\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "check_relu_function(relu)\n",
        "plot_relu(relu)"
      ],
      "metadata": {
        "id": "Kken6_XvDdOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try to `jit` this function to speed up compilation and try to call it."
      ],
      "metadata": {
        "id": "2mgIAyE2Fx3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "relu_jit = jax.jit(relu)\n",
        "\n",
        "# Gen 1000000 random numbers and pass them to relu\n",
        "num_random_numbers=1000000\n",
        "x = jax.random.normal(key, (num_random_numbers,))\n",
        "\n",
        "try:# Should raise an error. \n",
        "  relu_jit(x) \n",
        "except Exception as e:\n",
        "  print(\"Exception {}\".format(e))"
      ],
      "metadata": {
        "id": "4YDkiNlRF6jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why does this fail?**\n",
        "\n",
        "As mentioned above, JAX transforms first convert python functions into an intermediate language called jaxpr. Jaxpr only captures what is executed on the parameters given to it during tracing, so this means during conditional calls, jaxpr only considers the branch taken. \n",
        "\n",
        "When jit-compiling a function, we want to compile and cache a version of the function that can handle multiple different arguement types (so we don't have recompile for each function evaluation). For example, when we compile a function on an array `jnp.array([1., 2., 3.], jnp.float32)`, we would likely also want to used the compiled function for `jnp.array([4., 5., 6.], jnp.float32)`. \n",
        "\n",
        "To achieve this, JAX traces your code based on abstract values. The default abstraction level is a ShapedArray - array that has a fixed size and dtype, for example, if we trace a function using `ShapedArray((3,), jnp.float32)`,  it can be reused for any concrete array of size 3, and float32 dtype. \n",
        "\n",
        "This does come with some challenges. Tracing that relies on concrete values become tricky and sometimes results in `ConcretizationTypeError` as in the relu function above. Furtermore, when tracing function with conditional statements (\"if ...\"), JAX doesn't know which branch to take when tracing and so tracing can't occur.\n",
        "\n"
      ],
      "metadata": {
        "id": "y7q33C4pHOQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To solve this, we have two options:\n",
        "- Use static arguements to make sure JAX traces on a concrete value level - not ideal, if you need to retrace a lot. Example - bottom of this [section](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#python-control-flow-jit).\n",
        "- Use builtin JAX condition flow primitives such as [`lax.cond`](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.cond.html) or [`jnp.where`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.where.html).  "
      ],
      "metadata": {
        "id": "uLswU8aMEQ9K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Task** : Let's convert our ReLU function above to use [`jnp.where`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.where.html) (you can also use `jnp.maximum`, if you prefer.) "
      ],
      "metadata": {
        "id": "SX8k4R7daBpP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement ReLU using jnp.where.\n",
        "def relu(x):\n",
        "  # TODO Implement ME! \n",
        "  return"
      ],
      "metadata": {
        "id": "p-4mXLwqaK-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check ReLU function\n",
        "check_relu_function(relu)"
      ],
      "metadata": {
        "id": "B5fq_QRoaaG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!') \n",
        "def relu(x):\n",
        "  # TODO Implement ME! \n",
        "  return jnp.maximum(x,0)\n",
        "\n",
        "check_relu_function(relu)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "XLtBaplGxlS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see the performance benefit of using jit!"
      ],
      "metadata": {
        "id": "CSA0_24Nbo-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "relu_jit = jax.jit(relu)\n",
        "key = jax.random.PRNGKey(42)\n",
        "\n",
        "num_random_numbers=1000000\n",
        "x = jax.random.normal(key, (num_random_numbers,))\n",
        "\n",
        "\n",
        "jax_time = %timeit -o -n 10 relu(x).block_until_ready()\n",
        "\n",
        "# Warm up/Compile - first run \n",
        "relu_jit(x).block_until_ready()\n",
        "jax_jit_time = %timeit -o -n 10 relu_jit(x).block_until_ready()\n",
        "\n",
        "# Let's plot the performance difference\n",
        "jax_avg_time=np.mean(jax_time.all_runs)\n",
        "jax_jit_avg_time=np.mean(jax_jit_time.all_runs)\n",
        "data = {'JAX (no jit)':jax_avg_time, 'JAX (with jit)':jax_jit_avg_time}\n",
        "\n",
        "plot_performance(data,title=\"Average time taken for ReLU function\")"
      ],
      "metadata": {
        "id": "KYogDOCLiLXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Grad\n",
        "\n",
        "`grad` is used to automatically compute the gradient of a function in JAX. It can be applied to Python and NumPy functions, which means you can differentiate through loops, branches, recursion and closures.  \n",
        "\n",
        "`grad` takes in a function `f` and returns a function. If `f` is a mathematical function $f$, then `grad(f)` correspondes to $f'(x)$, with `grad(f(x))` corresponding to $\\Delta{f(x)}$.\n"
      ],
      "metadata": {
        "id": "dxq-z-xzs40s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a simple function $f(x)=6x^4-9x+4$"
      ],
      "metadata": {
        "id": "C49R8EOs-GHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = lambda x: 6*x**4 - 9*x + 4"
      ],
      "metadata": {
        "id": "lUMepl6J-dQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can compute the gradient of this function - $\\Delta{f(x)}$ and evaluate it at $x=3$."
      ],
      "metadata": {
        "id": "9ayvrkpiBiu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dfdx = grad(f)\n",
        "dfdx_3 = dfdx(3.0)"
      ],
      "metadata": {
        "id": "YNm9hS2S-vJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Math Task**: Can you calculate $f'(2)$ by hand?"
      ],
      "metadata": {
        "id": "UcRUywsnF3LZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer =  0#@param {type:\"integer\"}\n",
        "\n",
        "dfdx_2 = dfdx(2.0)\n",
        "\n",
        "assert answer == dfdx_2, \"Incorrect answer, hint ...\"\n",
        "\n",
        "print(\"Nice, you got the correct answer!\")"
      ],
      "metadata": {
        "id": "PybYK6NEFWrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also chain `grad` to calculate higher order deratives. \n",
        "\n",
        "We can calculate $f'''(x)$ as follows:"
      ],
      "metadata": {
        "id": "wcB5ZjojH67Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d3dx = grad(grad(grad(f)))"
      ],
      "metadata": {
        "id": "013SFq7BE54W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Math Task**: How about $f'''(2)$ by hand?"
      ],
      "metadata": {
        "id": "7_r9VQGoIsa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer =  0#@param {type:\"integer\"}\n",
        "\n",
        "d3dx_2 = d3dx(2.0)\n",
        "\n",
        "assert answer == d3dx_2, \"Incorrect answer, hint ...\"\n",
        "\n",
        "print(\"Nice, you got the correct answer!\")"
      ],
      "metadata": {
        "id": "WZUArv4TInPg",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another useful method is `value_and_grad`, where we can get the value ($f(x)$) and gradient ($f'(x)$). "
      ],
      "metadata": {
        "id": "c3QgJNU9XYyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import value_and_grad\n",
        "f_x,dy_dx = value_and_grad(f)(2.0)\n",
        "print(f\"f(x): {f_x} fâ€²(x): {dy_dx} \")"
      ],
      "metadata": {
        "id": "x3zeSv6gXuyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Group Task:** Chat with neighbour/think about how JAX's automatic differentiation compares to other libraries such as Pytorch or Tensorflow. "
      ],
      "metadata": {
        "id": "MktOLPnwvnH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### More Advanced Transforms - `VMAP` and `PMAP` - <font color='orange'>`Intermediate`</font>, <font color='green'>`Advanced`</font>\n",
        "\n",
        "JAX also provides transforms that allow you automatically vectorize (`vmap`) and parallelize (`pmap`) your code. "
      ],
      "metadata": {
        "id": "tvBzh8wiGuLf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### VMAP - <font color='orange'>`Intermediate`</font>\n",
        "\n",
        "VMAP (Vectorizing map) automatically vectorizes your python functions. "
      ],
      "metadata": {
        "id": "RCUB9YkCnCFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a simple function that calculate the min and max of an input."
      ],
      "metadata": {
        "id": "e858lqfYKd4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def min_max(x):\n",
        "  return jnp.array([jnp.min(x), jnp.max(x)])"
      ],
      "metadata": {
        "id": "-6qalyXgDsKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can apply this function to the vector - `[0, 1, 2, 3, 4]` and get the min and max values."
      ],
      "metadata": {
        "id": "muSIsUkgKlxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = jnp.arange(5)\n",
        "min_max(x)"
      ],
      "metadata": {
        "id": "F5wIeGieKsWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What about if we want to apply this to a batch/list of vectors (i.e. calculate the min and max independently across multiple batches)? \n",
        "\n",
        "*We will see in part 2 why this relevant to ML - hint - batched gradient descent!*"
      ],
      "metadata": {
        "id": "_PkC7NnPLNXq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create our batch - 3 vectors of size 5."
      ],
      "metadata": {
        "id": "hRngFfwCMHLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batched_x = np.arange(15).reshape((3, 5))\n",
        "print(batched_x)"
      ],
      "metadata": {
        "id": "EKuh459OD6jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_max(batched_x)"
      ],
      "metadata": {
        "id": "G0gK--O4Ny7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: What do you think would be the result if we passed batch_x into `min_max`?"
      ],
      "metadata": {
        "id": "hApYpVEvNS1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_min_max_output = [0,14] #@param [\"[[0,4],[5,9],[10,14]]\", \"[[0,10],[1,11],[2,12],[3,13],[4,14]]\", \"[0,14]\"] {type:\"raw\"}\n",
        "\n",
        "assert (batch_min_max_output == np.array(min_max(batched_x))).all(), \"Incorrect answer, hint ...\"\n",
        "\n",
        "print(\"Nice, you got the correct answer!\")"
      ],
      "metadata": {
        "id": "gu6C3J0kMrtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the above is not what we want. The `min` and `max` is applied across the entire batch, when we want the min and max per vector. \n",
        "\n",
        "We can also manually batch this by `jnp.stack` and a for loop, as follows:"
      ],
      "metadata": {
        "id": "6K0weiHOOb8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jit\n",
        "def manual_batch_min_max_loop(batched_x):\n",
        "  return jnp.stack([min_max(x) for x in batched_x])\n",
        "\n",
        "print(manual_batch_min_max_loop(batched_x))"
      ],
      "metadata": {
        "id": "q8RdAqr8N-Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or, just natively updating the `axis` in `jnp.min` and `jnp.max`. "
      ],
      "metadata": {
        "id": "jmu3VVtMR0GV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jit\n",
        "def manual_batch_min_max_axis(batched_x):\n",
        "  return jnp.array([jnp.min(batched_x,axis=1), jnp.max(batched_x,axis=1)]).T\n",
        "\n",
        "print(manual_batch_min_max_axis(batched_x))"
      ],
      "metadata": {
        "id": "lzxmORv-RcUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These approaches both work, but we need to change our function to work with batches. We can't just run the same code across a batch of data.\n",
        "\n",
        "There is where `vmap` becomes really useful! Using `vmap` we can write a function once, as if it is working on a single element, and then use `vmap` to automatically vectorize it! "
      ],
      "metadata": {
        "id": "CetKYASUSE4Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define our vmap function using our original single vector function\n",
        "@jit\n",
        "def min_max_vmap(batched_x):\n",
        "  return vmap(min_max)(batched_x)\n",
        "\n",
        "# Run it on a single vecor\n",
        "## We add extra dimention in a single vector, shape changes from (5,) to (1,5), which makes the vmapping possible\n",
        "x_with_leading_dim = jax.numpy.expand_dims(x,axis=0)\n",
        "print(f\"Single vector: {min_max_vmap(x_with_leading_dim)}\")\n",
        "\n",
        "# Run it on batch of vectors\n",
        "print(f\"Batch/list of vector:{min_max_vmap(batched_x)}\")"
      ],
      "metadata": {
        "id": "s2F8WUNQROkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So this is really conveniet, but what about performance? "
      ],
      "metadata": {
        "id": "-3bome92VRL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batched_x = np.arange(50000).reshape((500, 100))\n",
        "\n",
        "# Trace the functions with first call\n",
        "manual_batch_min_max_loop(batched_x).block_until_ready()\n",
        "manual_batch_min_max_axis(batched_x).block_until_ready()\n",
        "min_max_vmap(batched_x).block_until_ready()\n",
        "\n",
        "min_max_forloop_time = %timeit -o -n 10 manual_batch_min_max_loop(batched_x).block_until_ready()\n",
        "min_max_axis_time = %timeit -o -n 10 manual_batch_min_max_axis(batched_x).block_until_ready()\n",
        "min_max_vmap_time = %timeit -o -n 10 min_max_vmap(batched_x).block_until_ready()"
      ],
      "metadata": {
        "id": "O1Nb4uniUUor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So `vmap` should be similiar in performance to manually vectorized code (if everything is implemented well), and much better than naively vectorized code (i.e. for loops). "
      ],
      "metadata": {
        "id": "mYL758zCYsrR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### PMAP - <font color='green'>`Advanced`</font>\n",
        "\n",
        "**For this subsection, please ensure that colab is using a `TPU` runtime.**\n",
        "\n",
        "Another JAX transform is `pmap`. `pmap` transforms a function written for one device, to a function that can run in parallel, across many devices. \n",
        "\n",
        "**Difference between `vmap` and `pmap`**:\n",
        "\n",
        "So both `pmap` and `vmap` transform a function to work over an array, but they differ in implementation. `vmap` adds an extra batch dimension to all the operations in a function, while `pmap` replicates the function and executes each replica on their own XLA device in parallel."
      ],
      "metadata": {
        "id": "vAO9dOdrtiqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try and `pmap` a batch of dot products.\n",
        "\n",
        "Here is an illustration of how we would typically do this sequentially: \n",
        "\n",
        "[Source](https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2022/)"
      ],
      "metadata": {
        "id": "6qhlBnLs6AYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Illustration of Sequential Dot Product (Run me)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.assemblyai.com/blog/content/media/2022/02/not_parallel-2.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n",
        "\n"
      ],
      "metadata": {
        "id": "fz1i2AwA5_7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is code implementation of this:"
      ],
      "metadata": {
        "id": "MTmWNFZ08f8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's generate a batch of size 8, each with a matrix of size (5000, 6000)\n",
        "keys = jax.random.split(jax.random.PRNGKey(0), 8)\n",
        "mats = jnp.stack([jax.random.normal(key, (5000, 6000)) for key in keys])\n",
        "\n",
        "def dot_product_sequential():\n",
        "  @jit\n",
        "  def avg_dot_prod(mats):\n",
        "    result = []\n",
        "    # Loop through batch and compute dp\n",
        "    for mat in mats:\n",
        "        result.append(jnp.dot(mat, mat.T))\n",
        "    return jnp.stack(result)\n",
        "    \n",
        "\n",
        "  avg_dot_prod(mats).block_until_ready()\n",
        "\n",
        "run_sequential = %timeit -o -n 5 dot_product_sequential()"
      ],
      "metadata": {
        "id": "GqTuMldJ9Uv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an illustration of how we would do this in parallel \n",
        "\n",
        "[Source](https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2022/)"
      ],
      "metadata": {
        "id": "fBEtecJX-0AW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Illustration of Parallel Dot Product (Run me)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.assemblyai.com/blog/content/media/2022/02/parallelized.mp4\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')\n",
        "\n"
      ],
      "metadata": {
        "id": "Uswxurmn-5oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is code implementation of batched dot products:"
      ],
      "metadata": {
        "id": "sGsq8iTA_N9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will create `8` large random matrices (one for each available tpu devices ~ colab tpu's has 8 available [devices](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm)."
      ],
      "metadata": {
        "id": "0ygFWDfQIoeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keys = jax.random.split(jax.random.PRNGKey(0), 8)\n",
        "mats = pmap(lambda key: jax.random.normal(key, (5000, 6000)))(keys)"
      ],
      "metadata": {
        "id": "MZLMx06_K_qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The leading dimention here needs to equal the amount of available devices (since we are sending a batch to each device)."
      ],
      "metadata": {
        "id": "6BkMsaOtLISj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(mats.shape)"
      ],
      "metadata": {
        "id": "gWrdv_2wLG4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using `pmap` to generate the batches ensures these batches are of type `ShardedDeviceArray`. This is similiar to an ndarray, except each batch/shared is stored in the memory of multiple devices, so they can be used in subsequent `pmap` operations without moving data around between devices (gpu/tpu) and hosts (cpu). "
      ],
      "metadata": {
        "id": "HnqblcUsLaKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(mats))"
      ],
      "metadata": {
        "id": "JAeaBCvcLQWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dot_product_parallel():\n",
        "  \n",
        "  # Run a local matmul on each device in parallel (no data transfer)\n",
        "  result = pmap(lambda x: jnp.dot(x, x.T))(mats).block_until_ready()  # result.shape is (8, 5000, 5000)\n",
        "\n",
        "run_parallel = %timeit -o -n  5 dot_product_parallel()"
      ],
      "metadata": {
        "id": "PVz0gOWG9pkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is simple as that! Our dot product now runs in parallel across available devices (cpu, gpus or tpus). As we have more cores/devices, this code will automatically scale. "
      ],
      "metadata": {
        "id": "64gfyF3ENQzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's plot the performance difference\n",
        "jax_parallel_time=np.mean(run_parallel.all_runs)\n",
        "jax_seq_time=np.mean(run_sequential.all_runs)\n",
        "\n",
        "\n",
        "data = {'JAX (seq)':jax_seq_time, 'JAX (parallel)':jax_parallel_time}\n",
        "\n",
        "plot_performance(data,title=\"Average time taken for Seq vs Parallel Dot Product\")"
      ],
      "metadata": {
        "id": "5qcQXSbANP_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We showed an example of using `pmap` for *pure* parallelism, where there is no communication between devices. JAX also has various operations for communication across distributed devices ( more on this [here](https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html#communication-between-devices).)"
      ],
      "metadata": {
        "id": "-0j8iJRFUz6v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WILOYJH4gCnD"
      },
      "source": [
        "## Section Quiz \n",
        "\n",
        "Optional end of section quiz. Below is an example of an assessment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5iFeOKOgCnE"
      },
      "outputs": [],
      "source": [
        "#@title Generate Quiz Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "HTML(\n",
        "\"\"\"\n",
        "<iframe \n",
        "\tsrc=\"https://forms.gle/zbJoTSz3nfYq1VrY6\",\n",
        "  width=\"80%\" \n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB0503xgmSFh"
      },
      "source": [
        "# Part 2 - From Linear to Non-Linear Regression - `WIP`\n",
        "\n",
        "Now that we know some basics of JAX, we can build some simple models!\n",
        "\n",
        "Parts of this section are adapted from [Deepmind's Regression Tutorial](https://github.com/deepmind/educational/blob/master/colabs/summer_schools/intro_to_regression.ipynb). "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression\n",
        "\n",
        "In regression, we aim to find a function $f$ that maps inputs $x$ ($x \\in R^D$) to corresponding outputs - $f(x) \\in R$ [(source)](https://mml-book.github.io/). \n",
        "\n",
        "Put simply, we are trying to learn the relationship between our inputs and outputs.  "
      ],
      "metadata": {
        "id": "XrWSN-zaWAhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build a simple dataset, with 5 elements. Each element has a single input and single output.  "
      ],
      "metadata": {
        "id": "15_2U2klS1ER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_data_list = [1, 2, 3, 4, 5]\n",
        "y_data_list = [3, 2, 3, 1, 0]"
      ],
      "metadata": {
        "id": "gcqpxisITKYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can plot this dataset"
      ],
      "metadata": {
        "id": "fq1XmflLTLJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_basic_data(parameters_list=None, title=\"Observed data\"):\n",
        "  xlim = [-1, 7]\n",
        "  fig, ax = plt.subplots()\n",
        "  \n",
        "  if parameters_list is not None:\n",
        "    x_pred = np.linspace(xlim[0], xlim[1], 100)\n",
        "    for parameters in parameters_list:\n",
        "      y_pred = parameters[0] + parameters[1] * x_pred\n",
        "      ax.plot(x_pred, y_pred, ':', color=[1, 0.7, 0.6])\n",
        "\n",
        "    parameters = parameters_list[-1]\n",
        "    y_pred = parameters[0] + parameters[1] * x_pred\n",
        "    ax.plot(x_pred, y_pred, \"-\", color=[1, 0, 0], lw=2)\n",
        "\n",
        "  ax.plot(x_data_list, y_data_list, \"ob\")\n",
        "  ax.set(xlabel=\"Input x\", ylabel=\"Output y\",\n",
        "         title=title,\n",
        "         xlim=xlim, ylim=[-2, 5])\n",
        "  ax.grid()\n",
        "\n",
        "plot_basic_data()"
      ],
      "metadata": {
        "id": "NjEala0TSR-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's say we would to like to predict these $y$ (outputs) values given the $x$ (inputs). \n",
        "\n",
        "We can start modelling this by using a simple linear function: \n",
        "<center> \n",
        "$f(x) = \\color{red}{w} x + \\color{red}{b}$\n",
        "</center>,\n",
        "\n",
        "where $x$ is our input and  $\\color{red}{b}$ and $\\color{red}{w}$ are our model parameters.\n",
        "\n",
        "Usually, we learn the model parameters, but let's try to find these parameters by hand!\n",
        "\n"
      ],
      "metadata": {
        "id": "vnoEkgimTQ6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Task:** \n",
        "1. Move the two sliders below to set $\\color{red}{b}$ and $\\color{red}{w}$, and press \"Run cell\" on the code cell below. \n",
        "2. Is your $f(x)$ close to the blue data points? Can you find a better fit?\n",
        "3. Repeat 1-2 until convergence :D "
      ],
      "metadata": {
        "id": "FLvxEOBtWrSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters_list = [] # Used to track which parameters were tried. "
      ],
      "metadata": {
        "id": "A_8hyJrhdy6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b = 5.14 #@param {type:\"slider\", min:-1, max:8, step:0.01}\n",
        "w = -1.18 #@param {type:\"slider\", min:-3, max:3, step:0.01}\n",
        "print(\"Plotting line\", w, \"* x +\", b)\n",
        "parameters = [b, w]\n",
        "parameters_list.append(parameters)\n",
        "plot_basic_data(parameters_list,\n",
        "                title=\"Observed data and my first predictions\")"
      ],
      "metadata": {
        "id": "iYl7LM7kWYNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weights and Bias**\n",
        "\n",
        "What was the input of the function when you changed $\\color{red}{b}$ and $\\color{red}{w}$?\n",
        "\n",
        "- $\\color{red}{w}$ is our weights. This represents the slope of our function and determines the influence of the features $x$.\n",
        "- $\\color{red}{b}$ is our bias (also called the *intercept*). This shifts the line, without changing the slope."
      ],
      "metadata": {
        "id": "UCNWBHuBa9rj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You're a born optimizer!**\n",
        "\n",
        "Let's plot the optimizationt trajectory you took."
      ],
      "metadata": {
        "id": "XfUfPrRGeG2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots()\n",
        "opt = {\"head_width\": 0.2, \"head_length\": 0.2,\n",
        "       \"length_includes_head\": True, \"color\": \"r\"}\n",
        "if parameters_list is not None:\n",
        "  b_old = parameters_list[0][0]\n",
        "  w_old = parameters_list[0][1]\n",
        "  for i in range(1, len(parameters_list)):\n",
        "    b_next = parameters_list[i][0]\n",
        "    w_next = parameters_list[i][1]\n",
        "    ax.arrow(b_old, w_old, b_next - b_old, w_next - w_old, **opt)\n",
        "    b_old, w_old = b_next, w_next\n",
        "\n",
        "  ax.scatter(b_old, w_old, s=200, marker=\"o\", color=\"y\")\n",
        "  bs = [parameters[0] for parameters in parameters_list]\n",
        "  ws =  [parameters[1] for parameters in parameters_list]\n",
        "  ax.scatter(bs, ws, s=40, marker='o', color='k')\n",
        "\n",
        "ax.set(xlabel=\"Bias b\", ylabel=\"Weight w\",\n",
        "       title=\"My sequence of b\\'s and w\\'s\",\n",
        "       xlim=[-1, 8], ylim=[-3, 3])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ubqjOzjTXuRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Group Task**:\n",
        "\n",
        "*How did your neighbour do?*\n",
        "- Did they change $\\color{red}{b}$ and $\\color{red}{w}$ with big steps or small steps each time?\n",
        "- Did they start with small steps, and then progressed to bigger steps? Or the other way round? What about you?\n",
        "- Did the magnitude of your previous steps influence your next choice? Why? Or why not?\n",
        "- Did you all converge to roughly the same endpoint for $\\color{red}{b}$ and $\\color{red}{w}$, or did your sequences end up in different places?"
      ],
      "metadata": {
        "id": "Sqp1VK0KkLmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rK3RJPvAf4zm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import cm\n",
        "\n",
        "def l1_loss(b, w):\n",
        "  loss = 0 * b\n",
        "  for x, y in zip(x_data_list, y_data_list):\n",
        "    f = w * x + b\n",
        "    loss += np.abs(f - y)\n",
        "  return loss / len(x_data_list)\n",
        "\n",
        "bs, ws = np.linspace(-1, 8, num=25), np.linspace(-3, 3, num=25)\n",
        "b_grid, w_grid = np.meshgrid(bs, ws)\n",
        "loss_grid = l1_loss(b_grid, w_grid)\n",
        "\n",
        "def plot_loss(parameters_list, title, show_stops=False):\n",
        "  fig, ax = plt.subplots(1, 2, figsize=(18, 8),\n",
        "                         subplot_kw={\"projection\": \"3d\"})\n",
        "  ax[0].view_init(10, -30)\n",
        "  ax[1].view_init(30, -30)\n",
        "\n",
        "  if parameters_list is not None:\n",
        "    b_old = parameters_list[0][0]\n",
        "    w_old = parameters_list[0][1]\n",
        "    loss_old = l1_loss(b_old, w_old)\n",
        "    ls = [loss_old]\n",
        "\n",
        "    for i in range(1, len(parameters_list)):\n",
        "      b_next = parameters_list[i][0]\n",
        "      w_next = parameters_list[i][1]\n",
        "      loss_next = l1_loss(b_next, w_next)\n",
        "      ls.append(loss_next)\n",
        "\n",
        "      ax[0].plot([b_old, b_next], [w_old, w_next], [loss_old, loss_next],\n",
        "                color=\"red\", alpha=0.8, lw=2)\n",
        "      ax[1].plot([b_old, b_next], [w_old, w_next], [loss_old, loss_next],\n",
        "                color=\"red\", alpha=0.8, lw=2)\n",
        "      b_old, w_old, loss_old = b_next, w_next, loss_next\n",
        "\n",
        "    if show_stops:\n",
        "      ax[0].scatter(b_old, w_old, loss_old, s=100, marker=\"o\", color=\"y\")\n",
        "      ax[1].scatter(b_old, w_old, loss_old, s=100, marker=\"o\", color=\"y\")\n",
        "      bs = [parameters[0] for parameters in parameters_list]\n",
        "      ws = [parameters[1] for parameters in parameters_list]\n",
        "      ax[0].scatter(bs, ws, ls, s=40, marker=\"o\", color=\"k\")\n",
        "      ax[1].scatter(bs, ws, ls, s=40, marker=\"o\", color=\"k\")\n",
        "    else:\n",
        "      ax[0].scatter(b_old, w_old, loss_old, s=40, marker='o', color='k')\n",
        "      ax[1].scatter(b_old, w_old, loss_old, s=40, marker='o', color='k')\n",
        "\n",
        "  ax[0].plot_surface(b_grid, w_grid, loss_grid, cmap=cm.coolwarm,\n",
        "                     linewidth=0, alpha=0.4, antialiased=False)\n",
        "  ax[1].plot_surface(b_grid, w_grid, loss_grid, cmap=cm.coolwarm,\n",
        "                     linewidth=0, alpha=0.4, antialiased=False)\n",
        "  ax[0].set(xlabel=\"Bias b\", ylabel=\"Weight w\", zlabel=\"Loss\", title=title)\n",
        "  ax[1].set(xlabel=\"Bias b\", ylabel=\"Weight w\", zlabel=\"Loss\", title=title)\n",
        "  plt.show()\n",
        "\n",
        "plot_loss(parameters_list,\n",
        "          \"An example loss function and my sequence of b\\'s and w\\'s\",\n",
        "          show_stops=True)"
      ],
      "metadata": {
        "id": "mSIbd-xtfU2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "# Conclusion\n",
        "**Summary:**\n",
        "- JAX combines Autograd and XLA to perform accelerated numerical computations. These computations are achieved using transforms such as `jit`,`grad`,`vmap` and `pmap`. \n",
        "\n",
        "\n",
        "**Next Steps:** \n",
        "\n",
        "[Next steps for people who have completed the prac, like optional reading (e.g. blogs, papers, courses, youtube videos). This could also link to other pracs.]\n",
        "\n",
        "**Appendix:** \n",
        "\n",
        "[Anything (probably math heavy stuff) we don't have space for in the main practical sections.]\n",
        "\n",
        "**References:** \n",
        "1. Various JAX [docs](https://jax.readthedocs.io/en/latest/) - specifically [quickstart](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html), [common gotchas](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html), [jitting](\n",
        "https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html#), [random numbers](https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html) and [pmap](https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html?highlight=pmap#). \n",
        "\n",
        "2. http://matpalm.com/blog/ymxb_pod_slice/\n",
        "3. https://roberttlange.github.io/posts/2020/03/blog-post-10/\n",
        "4. [Machine Learning with JAX - From Zero to Hero | Tutorial #1](https://www.youtube.com/watch?v=SstuvS-tVc0). \n",
        "\n",
        "\n",
        "For other practicals from the Deep Learning Indaba, please visit [here](https://github.com/deep-learning-indaba/indaba-pracs-2022)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "## Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIZvkhfRz9Jz"
      },
      "outputs": [],
      "source": [
        "#@title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "HTML(\n",
        "\"\"\"\n",
        "<iframe \n",
        "\tsrc=\"https://forms.gle/bvLLPX74LMGrFefo9\",\n",
        "  width=\"80%\" \n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "-ZUp8i37dFbU",
        "CbOEYsWQ6tHv",
        "lg4__l4A7yqc",
        "dPbOnhE4ZSTi",
        "oAH4c_smdGQU",
        "Q2m376Ethf8m",
        "lGDU6ckKkzqL",
        "_bQ9QqT-yKbs",
        "cOGuGWtLmP7n",
        "QsJE_U-ZzVol",
        "dxq-z-xzs40s",
        "RCUB9YkCnCFb",
        "WILOYJH4gCnD",
        "aB0503xgmSFh",
        "XrWSN-zaWAhJ",
        "fV3YG7QOZD-B",
        "o1ndpYE50BpG"
      ],
      "name": "prac_0_intro_to_jax_for_ml.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}