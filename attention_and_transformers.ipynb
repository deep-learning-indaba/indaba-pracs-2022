{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# **Paying Attention to Transformers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0RWJRsNiFHX"
      },
      "source": [
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" width=\"40%\" />\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2022/blob/prac-transformers-and-attention/attention_and_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> [THIS SHOULD STILL CHANGE TO OUR PRAC]\n",
        "\n",
        "Â© Deep Learning Indaba 2022. Apache License 2.0.\n",
        "\n",
        "**Authors:** Ruan van der Merwe, Marianne Monteiro\n",
        "\n",
        "**Introduction:** \n",
        "\n",
        "The transformer architecture, introduced in Vaswani et al. 2017's paper [Attention is All You Need](https://arxiv.org/abs/1706.03762?amp=1), has significantly impacted the deep learning field. It has arguably become the de-facto architecture for complex Natural Language Processing (NLP) tasks; and can outperform benchmarks in various domains, including computer vision and reinforcement learning.\n",
        "\n",
        "Transformers, as the title of the original paper implies, are almost entirely based on a concept known as attention. Attention allows models to \"focus\" on different parts of an input; while considering the entire context of the input versus an RNN, that operates on the data sequentially.\n",
        "\n",
        "In this practical, we will introduce attention in greater detail and build the entire transformer architecture block by block to see why it is such a robust and powerful architecture.\n",
        "\n",
        "**Topics:** \n",
        "\n",
        "Content: <font color='blue'>`Attention mechanisms, Transformers, NLP`</font>  \n",
        "Level: <font color='grey'>`Advanced`</font>\n",
        "\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "- Learn how different attention mechanisms can be implemented.\n",
        "- Learn and create the basic building blocks from scratch for the most common transformer architectures.\n",
        "- Learn how to train a sequence-sequence model.\n",
        "- Create and train a small GPT inspired model.\n",
        "- Learn how to use the [Hugging Face](https://huggingface.co/) library for quicker development cycles.\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "- Basic understanding of Jax and Haiku\n",
        "- Basic understanding of linear algebra\n",
        "- RNN based sequence-sequence models\n",
        "- Word2Vec\n",
        "\n",
        "**Outline:** \n",
        "\n",
        ">[Installation and Imports](#scrollTo=6EqhIg1odqg0)\n",
        "\n",
        ">[Attention](#scrollTo=-ZUp8i37dFbU)\n",
        "\n",
        ">>[Sequence to sequence attenion mechanisms](#scrollTo=ii__Bc27epiJ)\n",
        "\n",
        ">>[Self-attention](#scrollTo=7RLO7ZHe3spv)\n",
        "\n",
        ">>[Multihead Attention](#scrollTo=7SrT6swYgCm9)\n",
        "\n",
        ">[Transformers](#scrollTo=e9NW58_3hAg2)\n",
        "\n",
        ">>[High level overview](#scrollTo=6HxXjYTFXaal)\n",
        "\n",
        ">>[Tokenisation](#scrollTo=sq0kPkVphbFG)\n",
        "\n",
        ">>[Positional encodings](#scrollTo=Ln7esAMHhdaz)\n",
        "\n",
        ">>[Feed Forwad block](#scrollTo=3l8mzIOeXbrA)\n",
        "\n",
        ">>[Add and Norm](#scrollTo=GQONPcKfXbfj)\n",
        "\n",
        ">>[Building the Encoder](#scrollTo=AIHHQTrcXb1b)\n",
        "\n",
        ">>[Building the Decoder](#scrollTo=RxSD1tLIX4CJ)\n",
        "\n",
        ">>[Putting it all together](#scrollTo=Ts-WLr0zY26X)\n",
        "\n",
        ">>[Training our model to invert sentence](#scrollTo=_bqOml7HYNmm)\n",
        "\n",
        ">[Hugging Face](#scrollTo=rectkTs9iFHg)\n",
        "\n",
        ">>[Datasets Package - Beginner](#scrollTo=Wq_pX3XTefu3)\n",
        "\n",
        ">>[Transformers Package](#scrollTo=iL4j1JZse8wi)\n",
        "\n",
        ">>[Training a chatbot intent model  - Advanced](#scrollTo=QSJ-Fh49fU-9)\n",
        "\n",
        ">[Conclusion](#scrollTo=fV3YG7QOZD-B)\n",
        "\n",
        ">[Feedback](#scrollTo=o1ndpYE50BpG)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "For this practical, you will need to use a TPU/GPU to speed up training. To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"TPU\"/\"GPU\" in the \"Hardware accelerator\" box."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4boGA9rYdt9l"
      },
      "outputs": [],
      "source": [
        "## Install and import anything required. Capture hides the output from the cell. \n",
        "#@title Install and import required packages. (Run Cell)\n",
        "\n",
        "!pip install git+https://github.com/deepmind/dm-haiku flax optax\n",
        "!pip install transformers==4.12.1 datasets \n",
        "!pip install seaborn umap-learn\n",
        "!pip install livelossplot \n",
        "\n",
        "import os\n",
        "import math\n",
        "import urllib.request\n",
        "\n",
        "# https://stackoverflow.com/questions/68340858/in-google-colab-is-there-a-programing-way-to-check-which-runtime-like-gpu-or-tpu\n",
        "if int(os.environ[\"COLAB_GPU\"]) > 0:\n",
        "  print(\"a GPU is connected.\")\n",
        "elif \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
        "  print(\"A TPU is connected.\")\n",
        "  import jax.tools.colab_tpu\n",
        "  jax.tools.colab_tpu.setup_tpu()\n",
        "else:\n",
        "  print(\"Only CPU accelerator is connected.\")\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "\n",
        "import haiku as hk\n",
        "import transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "import datasets\n",
        "\n",
        "import optax\n",
        "\n",
        "from PIL import Image\n",
        "from livelossplot import PlotLosses\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "# download images used in notebook\n",
        "urllib.request.urlretrieve(\n",
        "  'https://images.unsplash.com/photo-1529778873920-4da4926a72c2?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8Y3V0ZSUyMGNhdHxlbnwwfHwwfHw%3D&w=1000&q=80',\n",
        "   \"cat.png\")\n",
        "\n",
        "\n",
        "import gensim\n",
        "from nltk.data import find\n",
        "import nltk\n",
        "nltk.download('word2vec_sample')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9X10jhocGaS"
      },
      "outputs": [],
      "source": [
        "#@title Helper Functions. (Run Cell)\n",
        "\n",
        "def plot_position_encodings(P, max_tokens, d_model):\n",
        "  '''Function that takes in a position encoding matrix and plots it.'''\n",
        "\n",
        "  plt.figure(figsize=(20,np.min([8,max_tokens])))\n",
        "  im = plt.imshow(P, aspect=\"auto\", cmap='Blues_r')\n",
        "  plt.colorbar(im, cmap='blue')\n",
        "\n",
        "  if d_model<=64:\n",
        "    plt.xticks(range(d_model))\n",
        "  if max_tokens <=32:\n",
        "    plt.yticks(range(max_tokens))\n",
        "  plt.xlabel('Embedding index')\n",
        "  plt.ylabel('Position index')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_image_patches(patches):\n",
        "  '''Function that takes in a list of patches and plots them.'''\n",
        "  axes=[]\n",
        "  fig=plt.figure(figsize=(25,25))\n",
        "  for a in range(patches.shape[1]):\n",
        "      axes.append(fig.add_subplot(1, patches.shape[1], a+1) ) \n",
        "      plt.imshow(patches[0][a])\n",
        "  fig.tight_layout()    \n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_projected_embeddings(embeddings, labels):\n",
        "  '''Function that takes in a list of embeddings projects them onto a 2D space and plots them using UMAP.'''\n",
        "  import umap\n",
        "  import seaborn as sns\n",
        "\n",
        "  projected_embeddings = umap.UMAP().fit_transform(embeddings)\n",
        "\n",
        "  plt.figure(figsize=(15,8))\n",
        "  plt.title('Projected text embeddings')\n",
        "  sns.scatterplot(\n",
        "      x=projected_embeddings[:,0],\n",
        "      y=projected_embeddings[:,1],\n",
        "      hue=labels\n",
        "  )\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def get_word2vec_embedding(words):\n",
        "  '''\n",
        "  Function that takes in a list of words and returns a list of their embeddings,\n",
        "  based on a pretrained word2vec encoder.\n",
        "  '''\n",
        "  word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
        "  model = gensim.models.KeyedVectors.load_word2vec_format(\n",
        "      word2vec_sample, \n",
        "      binary=False\n",
        "  )\n",
        "  \n",
        "  output = []\n",
        "  words_pass = []\n",
        "  for word in words:\n",
        "    try:\n",
        "      output.append(jnp.array(model.word_vec(word)))\n",
        "      words_pass.append(word)\n",
        "    except:\n",
        "      pass\n",
        "\n",
        "  embeddings = jnp.array(output)\n",
        "  del model # free up space again\n",
        "  return embeddings,words_pass \n",
        "\n",
        "def plot_attention_weight_matrix(weight_matrix, x_ticks, y_ticks):\n",
        "  '''Function that takes in a weight matrix and plots it with custom axis ticks'''\n",
        "  plt.figure(figsize=(15,7))\n",
        "  ax = sns.heatmap(weight_matrix, cmap='Blues')\n",
        "  plt.xticks(np.arange(weight_matrix.shape[1])+0.5, x_ticks)\n",
        "  plt.yticks(np.arange(weight_matrix.shape[0])+0.5, y_ticks)\n",
        "  plt.title('Attention matrix')\n",
        "  plt.xlabel('Attention score')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "  '''Function that takes in a string and removes all punctuation.'''\n",
        "  import re\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "  return text\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfwoGkW3cLuk"
      },
      "outputs": [],
      "source": [
        "#@title Check what device you are using (Run Cell)\n",
        "print(f\"Num devices: {jax.device_count()}\")\n",
        "print(f\" Devices: {jax.devices()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZUp8i37dFbU"
      },
      "source": [
        "## **Attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5MqDkvKiFHb"
      },
      "source": [
        "In order to understand the transformer architecture, one must understand the concept of attention and how it is implemented in deep learning. The attention mechanism is inspired by how humans would look at an image or read a sentence. \n",
        "\n",
        "Let us take the image of the dog in human clothes below (image and example [source](https://lilianweng.github.io/posts/2018-06-24-attention/)). When paying *attention* to the red blocks of pixels, we will say that the yellow block of pointy ears is something we expected (correlated) but that the grey blocks of human clothes are unexpected for us (uncorrelated). This is *based on what we have seen in the past* when looking at pictures of dogs, specifically one of a Shiba Inu. \n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1iEU7Cph2D2PCXp3YEHj30-EndhHAeB5T\" alt=\"drawing\" width=\"450\"/>\n",
        "\n",
        "Assume we want to identify the dog breed in this image. When we look at the red pixels, we tend to pay more *attention* to relevant pixels that are more similar or relevant to them, which could be the ones in the yellow box. We almost completely remove the snow in the background and theÂ human clothing for this task. However, when we begin looking at the background in anÂ attemptÂ to identify what is in it, we will fade out the dog pixels because they are irrelevant to the current task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAnfB8-JsdYA"
      },
      "source": [
        "The same thing happens when we read. In order to understand the entire sentence, we will learn to correlate and *attend to* certain words based on the context of the entire sentence. For instance, in the first sentence below, when looking at the word \"coding\", we pay more attention to the word \"Apple\" and \"computer\" because we know that when we speak about coding, \"Apple\" is actually referring to the company. \n",
        "\n",
        "However, in the second sentence, we realise we should not consider \" apple \" when looking at \"code\" because given the context of the rest of the sentence, we know that this apple is referring to an actual apple and not a computer.\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1j23kcfu_c3wINU6DUvxzMYNmp4alhHc9\" alt=\"drawing\" width=\"350\"/>\n",
        "\n",
        "We can build better models by allowing by building mechanisms that mimic attention. It will enable our models to learn better representations of our input data by contextualizing what it knows about some parts of the input based on other parts of the It will enable our models to learn better representations of our input data by contextualizing what it knows about some parts of the input based on other parts of theÂ \n",
        "\n",
        "In the following sections, we will delve deeper into the mechanisms that enable us to train our deep learning models to attend to input data in the context of other input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii__Bc27epiJ"
      },
      "source": [
        "### Sequence to sequence attenion mechanisms - <font color='blue'>`Intermediate`</font>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWXE-pm0hhhX"
      },
      "source": [
        "The first attention mechanisms were used in sequence-to-sequence models. These models were usually RNN encoder and decoder structures. The input sequence was processed sequentially by an RNN, encoding the sequence in a single context vector, which is then fed into another RNN that generates a new sequence. Below is an example of this ([source](https://lilianweng.github.io/posts/2018-06-24-attention/)).\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1FKfaArN1rsLjzVWaJGpMLEcxEshSLXd6\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "Due to there only being one context vector, it was often found that for longer input sequences, information gets lost due to the inability of the encoders to remember longer sequences. The attention mechanism introduced in [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) was proposed to solve this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knytnRDG62Bb"
      },
      "source": [
        "Here, instead of relying on one static context vector, which is also only used once in the decoding process, let us provide information on the entire input sequence at every decoding step using a dynamic context vector. By doing this, the decoder can access a larger \"bank\" of memory and attend to the input's required information based on the output's current state. This is shown below.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1fB5KObXcKo5x35xlIDIcjHTq1q75ejIB\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "In deep learning, attention can be interpreted as a vector of \"importance.\" To predict or infer one element, such as a pixel in an image or a word in a sentence, we estimate how strongly it is correlated with, or \"attends to,\" other elements using the attention vector/weights. These attention weights is then used to generate a new weighted sum of the remaining elements, which represents the target. (PUT IN SOURCE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyBuSnVj4__6"
      },
      "source": [
        "This, usually, consists of two steps for each decoding step $t$: \n",
        "\n",
        "1. Calculate the score (importance) for each $h_n$, given $s_{t-1}$ and generate an attention vector, $w_{n}$. \n",
        "  - $\\text{score} = a(s_{tâ1}, h_{n})$, where $a$ can be any differentiable function\n",
        "  - $w_{n} = \\frac{\\exp \\left\\{a\\left(s_{t-1}, h_{n}\\right)\\right\\}}{\\sum_{j=1}^{N} \\exp \\left\\{a\\left(s_{t-1}, h_{j}\\right)\\right\\}}$, where we use the softmax function to generate relative attention wieghts\n",
        "2. Generate the final context vector, $c_t$\n",
        "  - $c_t=\\sum_{n=1}^{N} w_n h_{n}$ \n",
        "\n",
        "The final state fed into the RNN to generate $s_{t+1}$, is given below, where $f$ can again be any combination method. \n",
        "\n",
        "$s_{t+1} = f\\left ( c_t, s_t \\right)$ \n",
        "\n",
        "In Bahdanau et al., 2015, $f$ was a learned feedforward layer taking in the concatenated vector $[c_t; s_t]$, with $a(s_{tâ1}, h_{n})$ being the dot product. Next, let us build up this attention schema."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP5aFhzZQ00Y"
      },
      "source": [
        "In dot product attention, the score is given by\n",
        "\n",
        "$a(s_{t-1}, h_n)=s_{t-1}^\\top h_n$\n",
        "\n",
        "**Code task**: Complete the dot product attention function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1v3EhnW9THCW"
      },
      "outputs": [],
      "source": [
        "def dot_product_attention(hidden_states, previous_state):\n",
        "  scores = # FINISH ME \n",
        "  w_n = # FINSIH ME \n",
        "  c_t = jnp.matmul(w_n, hidden_states)\n",
        "\n",
        "  return w_n, c_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "APwzi2xmY8Qe"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!') \n",
        "def dot_product_attention(hidden_states, previous_state):\n",
        "\n",
        "  scores = jnp.dot(hidden_states, previous_state.T)\n",
        "  w_n = jax.nn.softmax(scores)\n",
        "  c_t = jnp.matmul(w_n, hidden_states)\n",
        "\n",
        "  return w_n, c_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_sdxOrgawet"
      },
      "source": [
        "In order to show how the dot product can produce attention weights that make sense, let us use pretrained [word2vec](https://jalammar.github.io/illustrated-word2vec/) embeddings. These word2vec embeddings are generated by an encoder network that was trained to generate similar embeddings for words with similar meanings. \n",
        "\n",
        "Even though we are not processing something sequentially now that needs context, the attention matrix should indicate which words are correlatedâand would thus attend to each other.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkGoImvQdX2H"
      },
      "outputs": [],
      "source": [
        "words = ['king', 'queen', 'royalty', 'food', 'apple', 'pear', 'computers']\n",
        "word_embeddings, words = get_word2vec_embedding(words)\n",
        "weights, _ = dot_product_attention(word_embeddings, word_embeddings)\n",
        "plot_attention_weight_matrix(weights, words, words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUKsSUr3ql7F"
      },
      "source": [
        "Looking at the matrix, assuming the function was implemented correctly, we can see which words have similar meanings. The \"royal\" group of words have higher attention scores with each other than the \"food\" words, which all attend to one another. We also see that \"computers\" have very low attention scores for all of them, which shows that they are neither very related to \"royal\" or \"food\" words.  \n",
        "\n",
        "**Group task:** \n",
        "  - Play with the word selections above. See if you can find word combinations whose attention values seem counter-intuitive. Think of possible explanations. Which sense of a word did the attention scores capture? \n",
        "  - Ask your friend if they found examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENocLBe1YJMN"
      },
      "source": [
        "For a more extensive range of attention mechanisms, you can visit this [link](https://lilianweng.github.io/posts/2018-06-24-attention/#summary)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x22cVV00c9ZK"
      },
      "source": [
        "### Self-attention to Multihead Attention- <font color='blue'>`Intermediate`</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnfU62CY4PmW"
      },
      "source": [
        "Self-attention and multi-head attention (MHA) are the core building blocks for the transformer architecture. We will build up the intuition and implementation here in detail. Then in the **Transformers** section, you will see how this mechanism is utilised to build an attention only sequence-to-sequence model.\n",
        "\n",
        "\n",
        "Going forward in this section, we will represent a sentence by splitting it up into a list of words, then using the word2vec model from above to encode each word. In the transformers section, we will dive deeper into how we transform an input into a sequence of vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo1zsFH969Tf"
      },
      "outputs": [],
      "source": [
        "def embedd_sentence(sentence):\n",
        "  # this is just for examples, later we will not do this\n",
        "  sentence = remove_punctuation(sentence)\n",
        "  words = sentence.split()\n",
        "  sentence_vector_sequence, words = get_word2vec_embedding(words)\n",
        "  return sentence_vector_sequence, words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RLO7ZHe3spv"
      },
      "source": [
        "#### Self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeDD9MdEho7D"
      },
      "source": [
        "Self-attention is an attention mechanism where we attend each vector of a given input sequence to the entire sequence. To gain an intuition for why self-attention is important, let us think about the sentence (example taken from [source](https://jalammar.github.io/illustrated-transformer/)):\n",
        "\n",
        "`\"The animal didn't cross the street because it was too tired.\"`\n",
        "\n",
        "A simple question about this sentence is what the word \"it\" refers to? Even though it might look simple, it can be tough for an algorithm to learn this. This is where self-attention comes in, as it can learn an attention matrix for the word \"it\" where a large weight is assigned to the word \"animal\".\n",
        "\n",
        "Self-attention also allows the model to learn how to interpret words with the same embeddings, such as apple, which can be a company or food, depending on the context. This is very similar to the hidden state found within an RNN, but this process, as you will see, allows the model to attend over the entire sequence in parrel, allowing longer sequences to be utilised. \n",
        "\n",
        "Self-attention consists of three concepts:\n",
        "\n",
        "- Queries, keys and values\n",
        "- Scaled dot product attention\n",
        "- Masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9SFONkT5Ds3"
      },
      "source": [
        "##### **Queries, keys and values**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVISm9pK5-FR"
      },
      "source": [
        "Typically all attention mechanisms can be written in terms of `key-value` pairs and `queries` to calculate the attention matrix and new context vector. \n",
        "\n",
        "To gain intuition, one can interpret the `query` vector as containing the information we are interested in, which is used to determine the `values` we should attend to, based on the similarity between the `keys` (which are paired with the `values`) and the `query`. Thus the similarity between the `queries` and `keys` gives us our attention score, where that score then determines the attention put in conjunction with the `values`. Or as [Lena-Voita](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html) put it:\n",
        "\n",
        "- Querie: asking for information\n",
        "- Key: saying that it has some information\n",
        "- Value: giving the information\n",
        "\n",
        "In transformer architectures, we use learnable weights matrices, reprsented as $Q,K,V$, to project each sequence vector to a unique $q,k,v$ vector. \n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1-96YjPxhcqW6FczUYwErGXHp6YpoLltq\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "You will notice that the vectors $q,k,v$ vectors are smaller in size than the input vectors. This will be covered at a later stage, but just know that it is a design choice for transformers and not required at all to work.\n",
        "\n",
        "This process can also be parrelised, as the input sequence can be represented as a matrix $X$, and then:\n",
        "\n",
        "$Q=W_QX \\\\ K=W_KX \\\\ V=W_VX$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqPxQdvjOQZN"
      },
      "source": [
        "**Code Task**: Code up a Haiku module that creates three linear layers such that an input can be projected down to $Q,K,V$. Hint: `hk.Linear`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeSPp3cmJhLd"
      },
      "outputs": [],
      "source": [
        "class SequenceToQKV(hk.Module):\n",
        "\n",
        "  def __init__(self, output_size):\n",
        "  \n",
        "    super().__init__()\n",
        "\n",
        "    self.output_size = output_size\n",
        "\n",
        "  def __call__(self, X):\n",
        "\n",
        "    initializer = hk.initializers.VarianceScaling(0.5)\n",
        "    \n",
        "    # this can also be one layer, how do you think you would do it?\n",
        "    q_layer = # FINISH ME\n",
        "    k_layer = # FINISH ME\n",
        "    v_layer = # FINISH ME\n",
        "\n",
        "    Q = q_layer(X)\n",
        "    K = k_layer(X)\n",
        "    V = v_layer(X)\n",
        "\n",
        "    return Q, K, V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qoII_QovZCkJ"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!') \n",
        "\n",
        "class SequenceToQKV(hk.Module):\n",
        "\n",
        "  def __init__(self, output_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.output_size = output_size\n",
        "\n",
        "  def __call__(self, X):\n",
        "    initializer = hk.initializers.VarianceScaling(0.5)\n",
        "    \n",
        "    # this can also be one layer, how do you think you would do it?\n",
        "    q_layer = hk.Linear(self.output_size, w_init=initializer)\n",
        "    k_layer = hk.Linear(self.output_size, w_init=initializer)\n",
        "    v_layer = hk.Linear(self.output_size, w_init=initializer)\n",
        "\n",
        "    Q = q_layer(X)\n",
        "    K = k_layer(X)\n",
        "    V = v_layer(X)\n",
        "\n",
        "    return Q, K, V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GRrkZUmWS15"
      },
      "outputs": [],
      "source": [
        "# run this to test if your code is running\n",
        "def x_to_qkv(sequences):\n",
        "  qkv_transforms = SequenceToQKV(64)\n",
        "  return qkv_transforms(sequences)\n",
        "\n",
        "x_to_qkv = hk.transform(x_to_qkv)\n",
        "\n",
        "# initialise model\n",
        "key = jax.random.PRNGKey(42)\n",
        "x = jax.random.normal(key, [6, 53, 768])\n",
        "params = x_to_qkv.init(key, x)\n",
        "Q,K,V = x_to_qkv.apply(params, key, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvbKqrRniVDa"
      },
      "source": [
        "##### **Scaled dot product attention**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpWykKCgGrdR"
      },
      "source": [
        "Now that we have our `query`, `key` and `value` matrices, it is time to calculate the attention matrix. Remember, in attention mechanisms; we must first find a score for each sequence vector and then use these scores to create a new context vector. We do this in self-attention using scaled dot product attention with the formula below. \n",
        "\n",
        "$\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V$\n",
        "\n",
        "What happens here is similar to what we did in the dot product attention in the previous section, just applying the mechanism to the sequence itself. For each element in the sequence, we calculate the attention weight matrix between $q_i$ and $K$. $V$ is multiplied by each weight, and all weighted vectors $v_{weighted}$ are summed together to form a new representation for $q_i$. By doing this, we are essentially drowning out irrelevant vectors and bringing up important vectors in the sequence when our focus is on $q_1$.\n",
        "\n",
        "$QK^\\top$ is scaled by the square root of the dimension of the vectors, $\\sqrt{d_k}$, to ensure more stable gradients during training. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRO-fhwmodb6"
      },
      "source": [
        "**Code Task:** Code up the scaled dot product attention function. This does not have to be a Haiku module as we are just doing matrix multiplications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNQ8Dgx2Wl8Q"
      },
      "outputs": [],
      "source": [
        "# we need to code up from scratch the function\n",
        "def scaled_dot_product_attention(query, key, value):\n",
        "  d_k = key.shape[-1]\n",
        "  logits = # FINSIH ME\n",
        "  scaled_logits = # FINISH ME\n",
        "  attention_weights = jax.nn.softmax(scaled_logits, axis=-1) \n",
        "  value = jnp.matmul(attention_weights, value)\n",
        "  return value, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lDq7CAcAd49i"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!') \n",
        "\n",
        "def scaled_dot_product_attention(query, key, value):\n",
        "  d_k = key.shape[-1]\n",
        "  logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
        "  scaled_logits = logits/jnp.sqrt(d_k)\n",
        "  attention_weights = jax.nn.softmax(scaled_logits, axis=-1) \n",
        "  value = jnp.matmul(attention_weights, value)\n",
        "  return value, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqVTaqqhir4e"
      },
      "source": [
        "Let's now see scaled dot product attention in action. We will take a sentence, embed each word using word2vec, and see what the final self-attention weights look like.\n",
        "\n",
        "We will not use the linear projection layers as they are not trained. Instead, we are going to make $X=Q=V=K$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVBq5ie0iz8G"
      },
      "outputs": [],
      "source": [
        "sentence = \"I drink coke, but eat steak\"\n",
        "word_embeddings, words = embedd_sentence(sentence)\n",
        "Q = K = V = word_embeddings\n",
        "\n",
        "# calculate weights and plot \n",
        "values, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
        "words = remove_punctuation(sentence).split()\n",
        "plot_attention_weight_matrix(attention_weights, words, words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tJJuTztnbdy"
      },
      "source": [
        "Keep in mind that we have not trained our attention matrix yet. However, we can see that by utilising the word2vec vectors as our sequence, we can see how scaled dot product attention already is capable of attending to \"eat\" when \"steak\" is our query and that the query \"drink\" attends more to \"coke\" and \"eat\". "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3ihUTrl_cxh"
      },
      "source": [
        "**Group task:** See if you can find any interesting results using the above, untrained, attention weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd7xAVznfBQU"
      },
      "source": [
        "##### **Masked attention** \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNC0zv5eB4sa"
      },
      "source": [
        "There exist cases where applying self-attention over the entire sequence is either not feasible or not allowed even. These can include:\n",
        "\n",
        "- Uneven length sequences batched together.\n",
        "  - When sending a batch of sequences through a network, the self-attention expects each sequence to be the same length. One handles this by padding the sequence. When calculating attention, ideally, these padding vectors should not be taken into consideration\n",
        "- Training a decoding model.\n",
        "  - When training decoder models, such as GPT-3, the decoder has access to the entire target sequence when training (as training is done in parallel), so we have to mask the future sequence data such that earlier data can not attend to it.\n",
        "\n",
        "By applying a mask to the final score calculated between queries and keys, we mitigate the influence of the unwanted sequence vectors. The vectors are masked by making the score between the query and their respective keys a VERY large negative value. The softmax function will push the attention weight down to zero, and the resulting value will be summed out and not influence the final representation.\n",
        "\n",
        "\n",
        "Putting everything together, masked scalled dot product attention visually looks like this:\n",
        "\n",
        "<img src=\"https://windmissing.github.io/NLP-important-papers/AIAYN/assets/5.png\" alt=\"drawing\" width=\"200\"/>. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qm1cMDA_hQIr"
      },
      "outputs": [],
      "source": [
        "# example of building a mask for tokens of size 32\n",
        "mask = jnp.tril(jnp.ones((32, 32)))\n",
        "sns.heatmap(mask, cmap='Blues')\n",
        "plt.title(\"Example of mask that can be applied\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQJtf6sCfBQY"
      },
      "source": [
        "**Code Task:** Try and implement the masking operation for your SCD function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LICrxTN5fBQZ"
      },
      "outputs": [],
      "source": [
        "# Code to be implemented during practical\n",
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "  d_k = key.shape[-1]\n",
        "  T_k = key.shape[1]\n",
        "  T_q = query.shape[1]\n",
        "  logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
        "  scaled_logits = logits/jnp.sqrt(d_k)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_logits = # FINISH ME\n",
        "\n",
        "  attention_weights = jax.nn.softmax(scaled_logits, axis=-1) \n",
        "  attention = jnp.matmul(attention_weights, value)\n",
        "  return attention, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iE1YNtLkfBQZ"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "  d_k = key.shape[-1]\n",
        "  T_k = key.shape[-2]\n",
        "  T_q = query.shape[-2]\n",
        "  logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
        "  scaled_logits = logits/jnp.sqrt(d_k)\n",
        "\n",
        "  if mask is not None:\n",
        "      scaled_logits = jnp.where(\n",
        "          mask[:T_q,:T_k], \n",
        "          scaled_logits, \n",
        "          -jnp.inf\n",
        "      )  \n",
        "\n",
        "  attention_weights = jax.nn.softmax(scaled_logits, axis=-1) \n",
        "  attention = jnp.matmul(attention_weights, value)\n",
        "  return attention, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAJnz3uOmHvC"
      },
      "source": [
        "In order to test if this works, lets apply a masked attention to our sequence, as one would do in a autoregressive setting, where the current output is only allowed to see the current and previous outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lq5Fnf2Xnd9D"
      },
      "outputs": [],
      "source": [
        "word_embeddings = jnp.expand_dims(word_embeddings,axis=0) if len(word_embeddings.shape)<3 else word_embeddings\n",
        "mask = jnp.tril(np.ones((word_embeddings.shape[1], word_embeddings.shape[1])))\n",
        "\n",
        "attention, attention_weights = scaled_dot_product_attention(\n",
        "    word_embeddings,\n",
        "    word_embeddings,\n",
        "    word_embeddings,\n",
        "    mask\n",
        ")\n",
        "\n",
        "plt.title('Attention weights with masked applied')\n",
        "sns.heatmap(attention_weights[0], cmap='Blues');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SrT6swYgCm9"
      },
      "source": [
        "#### Multihead Attention - <font color='blue'>`Advanced`</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eILi2mwrhtCD"
      },
      "source": [
        "Rather than only computing the attention once, the multi-head attention(MHA)mechanism runs through the scaled dot-product attention multiple times in parallel. According to the paper, Attention is all you need, \"multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this\". \n",
        "\n",
        "This can be viewed as a similar strategy to stacking convolution kernels in a CNN layer. This allows the kernels to focus on and learn different features and rules, which can be why multiple heads of attention also work. The process for MHA is given below.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1q0Oq6IVEkkMfVSpY4LkHBP866mcoIFsh\" alt=\"drawing\" width=\"1000\"/>\n",
        "\n",
        "As can be seen from the figure, the scaled dot product attention discussed earlier is just repeated $N$ times, whith $3N$ learnable matrices for each head. The outputs from the different heads are then concataneted, whereafter it is fed through a linear projection, which is then the final produced representations. \n",
        "\n",
        "Due to these large amount of computations and memory requirements, a design choice made that the $W_{Qn}, W_{Kn}, W_{Vn}$ matrices produce embedding of length $d_m/N$, where $d_m$ is the input sequence embedding size and $N$ is the number heads. By doing this, the MHA function is computational wise the same as one single head of attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXZPL9qVci0I"
      },
      "source": [
        "**Code Task:** Code up a Haiku module that implements the entire multi-head attention mechanism. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px2JQqOnYs17"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(hk.Module):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      num_heads,\n",
        "      d_m,\n",
        "      name = None,\n",
        "  ):\n",
        "    super().__init__(name=name)\n",
        "    self.num_heads = num_heads \n",
        "    self.sequence_to_qkv = SequenceToQKV(d_m)\n",
        "\n",
        "  def __call__(self, X=None, Q=None, K=None, V=None, mask = None, return_weights=False):\n",
        "    if None in [Q, K, V]:\n",
        "      assert not X is None, 'X has to be provided if either Q,K,V not provided'\n",
        "  \n",
        "      # project all data to Q, K, V\n",
        "      Q,K,V = # FINISH ME\n",
        "\n",
        "    # get the batch size, sequence lenght and embedding size\n",
        "    B, T, d_m = K.shape\n",
        "\n",
        "    # calculate heads embedding size (d_m/N)\n",
        "    head_size = d_m // self.num_heads\n",
        "\n",
        "    # B,T,d_m -> B, T, N, dm//N -> B, N, T, dm//N\n",
        "    q_heads = # FINSIH ME (REMEMBER, Q.shape is not always = K.shape)\n",
        "    k_heads = # FINISH ME\n",
        "    v_heads = # FINISH ME\n",
        "    \n",
        "    attention, attention_weights = scaled_dot_product_attention(q_heads,k_heads,v_heads,mask)\n",
        "\n",
        "    # (B, nh, T, hs) -> (B, T, nh, hs) -> (B, T, E) - re-assemble all head outputs\n",
        "    attention = # FINISH ME\n",
        "\n",
        "    # apply Wo\n",
        "    initializer = hk.initializers.VarianceScaling(0.5)\n",
        "    Wo = hk.Linear(d_m, w_init=initializer)\n",
        "    X_new = # FINISH ME\n",
        "\n",
        "    if return_weights:\n",
        "      return X_new, attention_weights \n",
        "    else:\n",
        "      return X_new\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ozNp_pfhc12I"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "class MultiHeadAttention(hk.Module):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      num_heads,\n",
        "      d_m,\n",
        "      name = None,\n",
        "  ):\n",
        "    super().__init__(name=name)\n",
        "    self.num_heads = num_heads \n",
        "    self.sequence_to_qkv = SequenceToQKV(d_m)\n",
        "\n",
        "  def __call__(self, X=None, Q=None, K=None, V=None, mask = None, return_weights=False):\n",
        "    if None in [Q, K, V]:\n",
        "      assert not X is None, 'X has to be provided if either Q,K,V not provided'\n",
        "  \n",
        "      # project all data to Q, K, V\n",
        "      Q,K,V = self.sequence_to_qkv(X)\n",
        "\n",
        "    # get the batch size, sequence lenght and embedding size\n",
        "    B, T, d_m = K.shape\n",
        "\n",
        "    # calculate heads embedding size (d_m/N)\n",
        "    head_size = d_m // self.num_heads\n",
        "\n",
        "    # B,T,d_m -> B, T, N, dm//N -> B, N, T, dm//N\n",
        "    q_heads = Q.reshape(B, -1, self.num_heads, head_size).swapaxes(1,2)\n",
        "    k_heads = K.reshape(B, -1, self.num_heads, head_size).swapaxes(1,2)\n",
        "    v_heads = V.reshape(B, -1, self.num_heads, head_size).swapaxes(1,2)\n",
        "    \n",
        "    attention, attention_weights = scaled_dot_product_attention(q_heads,k_heads,v_heads,mask)\n",
        "\n",
        "    # (B, nh, T, hs) -> (B, T, nh, hs) -> (B, T, E) - re-assemble all head outputs\n",
        "    attention = attention.swapaxes(1,2).reshape(B, -1, d_m)\n",
        "\n",
        "    # apply Wo\n",
        "    initializer = hk.initializers.VarianceScaling(0.5)\n",
        "    Wo = hk.Linear(d_m, w_init=initializer)\n",
        "    X_new = Wo(attention)\n",
        "\n",
        "    if return_weights:\n",
        "      return X_new, attention_weights \n",
        "    else:\n",
        "      return X_new\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w05LJDmGtXX3"
      },
      "source": [
        "Now that we have an understanding for how the MHA works, lets see how the transformer architecture uses them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WILOYJH4gCnD"
      },
      "source": [
        "### Section Quiz \n",
        "\n",
        "[Test knowledge on all the previous material of attention]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N5iFeOKOgCnE"
      },
      "outputs": [],
      "source": [
        "#@title Generate Quiz Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "HTML(\n",
        "\"\"\"\n",
        "<iframe \n",
        "\tsrc=\"https://forms.gle/zbJoTSz3nfYq1VrY6\",\n",
        "  width=\"80%\" \n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9NW58_3hAg2"
      },
      "source": [
        "## **Transformers**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3NUhE2jUo8O"
      },
      "source": [
        "The Transformer architecture was proposed in the [Attention is all you need](https://arxiv.org/abs/1706.03762) paper in 2017 as a SotA model for translation. This architecture is based only on attention mechanisms, while at the time, most models would also rely on recurrences (RNNs) or convolutions.\n",
        "\n",
        "Over time this became one of the most established and influential architectures used by the deep learning community; for example, Transformers are the base of the BERT and GPT-3 models. The architecture is also very versitale as it is capable of being used in multiple domains, including computer vision, audio processing and reinforcement learning and multimodal systems. \n",
        "\n",
        "In this section, we'll implement a Transformer based architecture in Haiku for a simpler task than translation: inverting a sentence. Each block will be built up from scratch and will be able to do more complex tasks. \n",
        "\n",
        "In this part of the tutorial we'll implement a Transformer based architecture in Haiku for a simpler task then translation: inverting a sentence.\n",
        "\n",
        "<img src=\"https://media.geeksforgeeks.org/wp-content/cdn-uploads/reverse-Words.jpg\" width=\"250\"/>\n",
        "\n",
        "We base a lot of code here from the [Deepmind Haiku example](https://github.com/deepmind/dm-haiku/tree/main/examples/transformer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HxXjYTFXaal"
      },
      "source": [
        "### High level overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxjJTlrhajcq"
      },
      "source": [
        "Originally the transformer was designed for machine translation, hence the encoder-decoder structure seen below. \n",
        "\n",
        "The encoder will receive an input sentence in one language and process it through multiple stacked `encoder blocks`. This creates a final representation where helpful information necessary for the decoding task has been extracted. This output is then fed into the stacked `decoder blocks` that produce new outputs in an autoregressive manner. \n",
        "\n",
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" width=\"350\" />\n",
        "\n",
        "\n",
        "The encoder consists of $N$ identical blocks, which process a sequence of token vectors sequentially. These blocks consist of 3 parts:\n",
        "\n",
        "1. Multihead attention blocks. These are the transformer architecture's backbone. They process the data to generate strong vectors for each token, ensuring that the necessary information for the task at hand is represented in the vectors. These are exactly the MHA we covered in the attention section previously. \n",
        "2. An MLP is applied to each input token separately and identically.\n",
        "3. Residual connection that adds the input tokens to the attended representations and a residual connection between the input to the MLP and its outputs. For both these connections, the result is normalized using layernorm. In certain implementations, these normalization steps are applied to the inputs rather than the outputs. Just like a Resnet, transformers are designed to be very deep models thus, these add and norm blocks are essential for a smooth gradient flow.  \n",
        "\n",
        "The decoder block consists of the same parts, except for one extra MHA block that appears after the initial MHA block. This block receives the output of the final encoder block, the transformed tokens, and uses that as the key-value pairs, while using the output of the first MHA block as the query. In doing this, the model attends over the input required to perform the sequence task. The initial MHA block is also masked in a decoder to block future inputs when making predictions. \n",
        "\n",
        "This entire architecture can then be trained end to end. These models expect data to be broken up into a sequence of vectors to be processed. Next, we will deep dive into how this is done.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq0kPkVphbFG"
      },
      "source": [
        "### Tokenisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd4hPASBiEGe"
      },
      "source": [
        "Before data can be fed into transformers, it has to be transformed into an acceptable format: a sequence of tokens and a vector representing each token. Below, we briefly discuss how we can transform text and vision data into tokens that transformers can process [source](https://huggingface.co/docs/transformers/preprocessing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8GylOMmBW4n"
      },
      "source": [
        "##### **Text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS_jLXJw0kdN"
      },
      "source": [
        "\n",
        "Transformers can not handle raw strings of text. So to process text, the text is first split up into tokens, where after these tokens are indexed and each token is assigned an embedding of size $d_{model}$. These embeddings can be learned during training or can come from a pretrained vocablay of embeddings. This new sequence of token embeddings is then fed into the transformer architecture. This idea is visualised below. \n",
        "\n",
        "\\\\\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=16euh4LADP_mcXywFwKKY3QQQkVplepiI\" alt=\"drawing\" width=\"450\"/>\n",
        "\n",
        "\n",
        "These token IDs are typically predicted when a model generates text, fills in missing words, etc.\n",
        "\n",
        "This process of splitting up text into tokens and assigning an ID to each token is called [tokenisation](https://huggingface.co/docs/transformers/tokenizer_summary). There are various ways to tokenise text, with some methods being trained directly from the data. When using pre-trained transformers, it is crucial to use the same tokeniser that was used to train the model. The previous link has in-depth descriptions of many widely known techniques.\n",
        "\n",
        "Below we show how the [Bert](https://arxiv.org/abs/1810.04805) models tokeniser tokenises a sentence. We use [Hugging Face](https://huggingface.co/) for this part, where we have a deep dive into Hugging Face later in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0C-3v0y0iZe"
      },
      "outputs": [],
      "source": [
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "encoded_input = bert_tokenizer(\"The practical is so much fun\")\n",
        "print(f\"Token IDs: {encoded_input['input_ids']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-408xpj794l"
      },
      "source": [
        "Here we can see that the tokeniser returns the IDs for each token, as shown in the figure. But counting the number of IDs, we see that it is larger than the number of words in the sentence. Let's print the tokens associated with each ID.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dYMGIgi8p-q"
      },
      "outputs": [],
      "source": [
        "print(f\"Tokens: {bert_tokenizer.decode(encoded_input['input_ids'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phjOR7ftAVt_"
      },
      "source": [
        "We can see the tokeniser attaches new tokens, `[CLS]` and `[SEP]`, to the start and end of the sequence. This is a Bert specific requirement for training and inference. Adding special tokens is a very common thing to do. \n",
        "\n",
        "For instance, to pretrain specific transformers, they perform what is known as masked prediction. For this, random tokens in a sequence are replaced by the `[MASK]` token, and the model is trained to predict the correct token ID for the token replaced with that token. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syL_uNAMVHQB"
      },
      "source": [
        "**Group task**:\n",
        "\n",
        "- Discuss with a friend what you think the current issue is when feeding these raw tokens into a transformer architecture? Think about the difference in meaning between a sentence and that same sentence where the word orders are random.\n",
        "- How would you fix that issue? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jWreoeTBUCD"
      },
      "source": [
        "##### **Images** (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcyHr8a_BdaT"
      },
      "source": [
        "As mentioned, transformers are versatile and can be applied to roughly any data which can be converted into a sequence of tokens.\n",
        "\n",
        "For example, to use the transformers encoder architecture with images, one can split an image into different patches, flatten these image patches and project each image patch into a fixed-sized embedding using any projection method. By doing this, the image has been converted into a sequence of image tokens, and the transformer will be able to process the data. \n",
        "\n",
        "This process is shown in the image below. \n",
        "\n",
        "\\\\\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ERF0f3Y_0wNb4kQ07xMYUysqNYIPkQMD\" alt=\"drawing\" width=\"650\"/>\n",
        "\n",
        "\n",
        "**Code task (OPTIONAL and ADVANCED):** Write a function that can take in a batch of images with shape (Batch, Height, Widht, Channels) and divide it into equal-sized patches. You can use the output of `image_to_patch` and `plot_image_patches` functions to visualise and test your function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htWcVGlDHcba"
      },
      "outputs": [],
      "source": [
        "def image_to_patch(image, patch_size):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        x - array of shape [B, H, W, C]\n",
        "        patch_size - Number of pixels per dimension of the patches (integer)\n",
        "    \"\"\"\n",
        "    B, H, W, C = image.shape # HINT: You will need these\n",
        " \n",
        "    image  # FINISH ME\n",
        "\n",
        "    return image_patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oqFn3peRTqm4"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "\n",
        "def image_to_patch(image, patch_size):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        x - array of shape [B, H, W, C]\n",
        "        patch_size - Number of pixels per dimension of the patches (integer)\n",
        "    \"\"\"\n",
        "    B, H, W, C = image.shape\n",
        "    image = image.reshape(B, H//patch_size, patch_size, W//patch_size, patch_size, C)\n",
        "    image = image.transpose(0, 1, 3, 2, 4, 5)    # [B, H', W', p_H, p_W, C]\n",
        "    image_patches = image.reshape(B, -1, *image.shape[3:])   # [B, H'*W', p_H, p_W, C]\n",
        "    return image_patches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBXMQInfTuJK"
      },
      "outputs": [],
      "source": [
        "# do not change these lines, only run them to test your function\n",
        "print('Original image:')\n",
        "image = np.array(Image.open(\"cat.png\"))\n",
        "plt.imshow(image)\n",
        "plt.show()\n",
        "\n",
        "print(\"Image broken into patches\")\n",
        "img = jnp.array(image)\n",
        "img = jax.image.resize(img, (512,512,3), \"nearest\")\n",
        "img = jnp.expand_dims(img, 0)\n",
        "patches = image_to_patch(img, 256)\n",
        "plot_image_patches(patches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln7esAMHhdaz"
      },
      "source": [
        "\n",
        "### Positional encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8MrN6YJXUM_"
      },
      "source": [
        "In most domains where a transformer can be utilised, there is an underlying order to the tokens produced, be it the order of words in a sentence, the location from which patches are taken in an image or even the steps taken in an RL environment. This order is very important in all cases; just imagine you interpret the sentence \"I have to read this book.\" as \"I have this book to read.\". Both sentences contain the exact same words, yet they have completely different meanings based on the order. \n",
        "\n",
        "As both the encoder and the decoder blocks process all tokens in parallel, the order of tokens is lost in these calculations. To cope with this, the sequence order has to be injected into the tokens directly. This can be done by adding *positional encodings* to the tokens at the start of the encoder and decoder blocks (though some of the latest techniques add positional information in the attention blocks). An example of how positional encodings alter the tokens is shown below.\n",
        "\n",
        "\\\\\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1eSgnVN2hnEsrjdHygDGwk1kxEi8-dcFo\" alt=\"drawing\" width=\"650\"/>\n",
        "\n",
        "Ideally, these encodings should have these characteristics ([source](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)):\n",
        "* Each time-step should have a unique value\n",
        "* The distance between time steps should stay constant.\n",
        "* The encoding should be able to generalise to longer sequences than seen during training.\n",
        "* The encoding must be deterministic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WHx-9m9h5JA"
      },
      "source": [
        "\n",
        "##### **Sine and cosine functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrkqm8gC90GU"
      },
      "source": [
        "\n",
        "In Attention is All you Need, the authors used a method that can satisfy all these requirements. This involves summing a combination of sine and cosine waves at different frequencies, with the formula for a position encoding at position $D$ shown below, where $i$ is the embedding index and $d_m$ is the token embedding size. \n",
        "\n",
        "\\\\\n",
        "\n",
        "$P_{D}= \\begin{cases}\\sin \\left(\\frac{D}{10000^{i/d_{m}}}\\right), & \\text { if } i \\bmod 2=0 \\\\ \\cos \\left(\\frac{D}{10000^{((i-1)/d_{m}}}\\right), & \\text { otherwise } \\end{cases}$\n",
        "\n",
        "\\\n",
        "\n",
        "Assuming our model as $d_m=8$, the position embedding will look like this:\n",
        "\n",
        "\\\n",
        "$P_{D}=\\left[\\begin{array}{c}\\sin \\left(\\frac{D}{10000^{0/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{0/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{2/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{2/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{4/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{4/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{8/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{8/8}}\\right)\\end{array}\\right]$\n",
        "\n",
        "\\\\\n",
        "\n",
        "Let's first create a function that can return these encodings to understand why this will work. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKobhuyj9-74"
      },
      "outputs": [],
      "source": [
        "def return_frequency_pe_matrix(token_sequence_length, token_embedding):\n",
        "\n",
        "  assert token_embedding%2==0, \"token_embedding should be divisible by two\"\n",
        "\n",
        "  P = jnp.zeros((token_sequence_length, token_embedding))\n",
        "  positions = jnp.arange(0, token_sequence_length)[:, jnp.newaxis]\n",
        "\n",
        "  i = jnp.arange(0, token_embedding, 2)\n",
        "  frequency_steps = jnp.exp(i * (-math.log(10000.0) / token_embedding))\n",
        "  frequencies = positions*frequency_steps\n",
        "\n",
        "  P = P.at[:, 0::2].set(jnp.sin(frequencies))\n",
        "  P = P.at[:, 1::2].set(jnp.cos(frequencies))\n",
        "\n",
        "  return P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4_9-Oh4yZuF"
      },
      "outputs": [],
      "source": [
        "token_sequence_length = 50 # Number of tokens the model will need to process\n",
        "token_embedding = 10000 # token embedding (and positional encoding) dimensions, ensure it is divisible by two\n",
        "P = return_frequency_pe_matrix(token_sequence_length, token_embedding)\n",
        "plot_position_encodings(P, token_sequence_length, token_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw8nOF2Ra93B"
      },
      "source": [
        "Looking at the graph above, we can see that for each position index, their is a unique pattern forming, where each position index will always have the same encoding. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHZXceUVNWy9"
      },
      "source": [
        "**Group task**:\n",
        "\n",
        "- Discuss with your friend why we are seeing that specific pattern when `token_sequence_length` is 1000, and `token_embedding` is 768.\n",
        "- You can try playing around with smaller values for `token_sequence_length` and  `token_embedding` to get a better intuition for the above discussion.\n",
        "- Ask your friend why they think the 10000 constant is used in the functions above. \n",
        "- Make `token_sequence_length` to be 50 and `token_embedding` something large, like 10000. What do you notice? Is a large token embedding always needed?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFiRpEaGd6TB"
      },
      "source": [
        "**Math task (optional):** Notice in our function that we do not directly implement the equation we describe above for numerical stability when calculating the frequency steps. See if you can derive by hand how we got to this new equation. Hint: Think about log rules.\n",
        "\n",
        "Original equation:\n",
        "\n",
        "$\\text{f} = \\frac{D}{x^{i/d_{m}}}$\n",
        "\n",
        "Code equation:\n",
        "\n",
        "$\\text{f} = \\text{exp} \\left( \\frac{-i\\log(x)}{d_m} \\right)D$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21sD_xKtd6TB"
      },
      "source": [
        "###### **Answer (but first try yourself)**\n",
        "\n",
        "Expand this section to see the answer. \n",
        "\n",
        "What we did here is an essential aspect of computer science, specifically in machine learning, when big numbers appear. We usually rewrite equations to utilise logs, such that multiplications become additions and large numbers get suppressed. This ensures better numerical stability.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQPI_Sqnd6TB"
      },
      "source": [
        "\\begin{align}\n",
        "    \\frac{ð·}{x^{ð/ðð}} &= D\\left( x^{-i/dm}\\right) \\\\\n",
        "    &= \\text{exp}\\left(\\log{x^{-i/dm}}\\right)D \\\\\n",
        "    &= \\text{exp}\\left(\\frac{-i}{dm}\\log{x}\\right)D\\\\\n",
        "    &= \\text{exp}\\left(\\frac{-i\\log{x}}{dm}\\right)D\\\\\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GeFt_xwh722"
      },
      "source": [
        "##### **Learned positional embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnTNW1kNyZ75"
      },
      "source": [
        "\n",
        "Another method which is commonly used is to allow the model to learn the positional information required. In this method, the model learns a lookup table, where each index in the table refers to a positional embedding. Whereas the previous method allowed for an infinite amount of tokens, this method caps the maximum token sequence length as the lookup table has to be set beforehand. \n",
        "\n",
        "Using Haiku, this method is relatively straightforward as we can use [Haiku's embed function](https://dm-haiku.readthedocs.io/en/latest/api.html#embed) or [Haiku's get parameter function](https://dm-haiku.readthedocs.io/en/latest/api.html#get-parameter)\n",
        "\n",
        "**Code Task:** Try and implement a lookup table, which can be learned, for using Haikus get parameter function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXbQ_-RnXaNY"
      },
      "outputs": [],
      "source": [
        "class PositionEmbeddingsLookup(hk.Module):\n",
        "    \"\"\"\n",
        "    A position embedding of shape [max_sequence_len, d_model]\n",
        "    \"\"\"\n",
        "    def __init__(self, max_sequence_len, d_model):\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_sequence_len = max_sequence_len\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def __call__(self, sequence):\n",
        "        \n",
        "        assert sequence.shape[0]<=self.max_sequence_len, f\"Sequence to long, max lenght={self.max_sequence_len}\"\n",
        "\n",
        "        lookup_table = hk.get_parameter( \n",
        "            name=\"position_embedding\",\n",
        "            shape=, # FILL ME IN\n",
        "            init=jnp.zeros\n",
        "        )\n",
        "\n",
        "        return lookup_table[:sequence.shape[0], :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "R1_WpVc0Y3Hk"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "\n",
        "class PositionEmbeddingsLookup(hk.Module):\n",
        "    \"\"\"\n",
        "    A position embedding of shape [max_sequence_len, d_model]\n",
        "    \"\"\"\n",
        "    def __init__(self, max_sequence_len, d_model):\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_sequence_len = max_sequence_len\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def __call__(self, sequence):\n",
        "        \n",
        "        assert sequence.shape[0]<=self.max_sequence_len, f\"Sequence to long, max lenght={self.max_sequence_len}\"\n",
        "\n",
        "        lookup_table = hk.get_parameter(\n",
        "            name=\"position_embedding\", \n",
        "            shape=(self.max_sequence_len, self.d_model),\n",
        "            init=jnp.zeros\n",
        "        )\n",
        "\n",
        "        return lookup_table[:sequence.shape[0], :]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l8mzIOeXbrA"
      },
      "source": [
        "### Feed Forwad block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn0EFwjXi1rI"
      },
      "source": [
        "These blocks are just a single 2-layer MLP that uses ReLU activation in the original model, but GeLU has also become very popular, and we will be using it throughout the practical.\n",
        "\n",
        "$$\n",
        "\\operatorname{FFN}(x)=\\max \\left(0, x W_{1}+b_{1}\\right) W_{2}+b_{2}\n",
        "$$\n",
        "\n",
        "One can interpret this block as processing what the MHA block has produced and then projecting these new token representations to a space that the next block can use more optimally. Usually, the first layer is very wide, in the range of 2-8 times the size of the token representations. They do this as it is easier to parallelize computations for a single wider layer during training than to parallelize a feedforward block with multiple layers. Thus they can add in more complexity but keep training and inference optimized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGEy7dapluco"
      },
      "source": [
        "**Code task:** Code up a Haiku Module that impliments the Feed forward block. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh6mpR6rl47u"
      },
      "outputs": [],
      "source": [
        "class FeedForwardBlock(hk.Module):\n",
        "  \"\"\"A 2-layer MLP which widens then narrows the input.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               widening_factor: int = 4, \n",
        "               name):\n",
        "    super().__init__(name=name)\n",
        "    self._init_scale = 0.25\n",
        "    self._widening_factor = widening_factor\n",
        "\n",
        "  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "    d_m = x.shape[-1]\n",
        "    layer1_size = self._widening_factor * d_m\n",
        "\n",
        "    initializer = hk.initializers.VarianceScaling(self._init_scale)\n",
        "    layer1 = # FINSIH ME\n",
        "    layer2 = # FINISH ME\n",
        "\n",
        "    x = jax.nn.gelu(layer1(x))\n",
        "    x = layer2(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5t7cgF8IYIUF"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!') \n",
        "\n",
        "class FeedForwardBlock(hk.Module):\n",
        "  \"\"\"A 2-layer MLP which widens then narrows the input.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               widening_factor: int = 4,\n",
        "               name=None):\n",
        "    super().__init__(name=name)\n",
        "    self._init_scale = .25\n",
        "    self._widening_factor = widening_factor\n",
        "\n",
        "  def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
        "    d_m = x.shape[-1]\n",
        "    layer1_size = self._widening_factor * d_m\n",
        "\n",
        "    initializer = hk.initializers.VarianceScaling(self._init_scale)\n",
        "    layer1 = hk.Linear(layer1_size, w_init=initializer)\n",
        "    layer2 = hk.Linear(d_m, w_init=initializer)\n",
        "\n",
        "    x = jax.nn.gelu(layer1(x))\n",
        "    x = layer2(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQONPcKfXbfj"
      },
      "source": [
        "### Add and Norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyyWfxPKsUm6"
      },
      "source": [
        "In order to get transformers to go deeper, the residual connections are very important to allow an easier flow of gradients through the network. For normalisation, `layer norm` is used. This normalises each token vector independently in the batch. It is found that normalising the vectors improves the convergence and stability of transformers. \n",
        "\n",
        "There are two learnable parameters in layernorm, `scale` and `bias`, which rescales the normalised value. Thus, for each input token in a batch, we calculate the mean, $\\mu_{i}$ and variance $\\sigma_i^2$. We then normalise the token with:\n",
        "\n",
        "$\\hat{x}_i = \\frac{x_i-$\\mu_{i}}{\\sigma_i^2 + Ïµ}$.\n",
        "\n",
        "Then $\\hat{x}$ is rescaled using the learned `scale`, $Î³$, and `bias` $Î²$, with:\n",
        "\n",
        "$y_i = Î³\\hat{x}_i + Î² = LN_{Î³,Î²}(x_i)$.\n",
        "\n",
        "So our add norm block can be represented as $LN(x+f(x))$, where $f(x)$ is either a MLP or MHA block."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9rPxLLD69nl"
      },
      "source": [
        "**Code task:** Code up a Haiku Module that impliments the add norm block. It should take as input the processed and unprocessed tokens. Hint: `hk.LayerNorm `"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIY5a7I969Fw"
      },
      "outputs": [],
      "source": [
        "class AddNorm(hk.Module):\n",
        "  \"\"\"A block that impliments the add and norm block\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               name=None):\n",
        "    super().__init__(name=name)\n",
        "\n",
        "  def __call__(self, x, processed_x):\n",
        "    \n",
        "    added = # FINISH ME\n",
        "    normalised = #FINISH ME\n",
        "    return normalised(added)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5OOLMaYXsR_O"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!') \n",
        "\n",
        "class AddNorm(hk.Module):\n",
        "  \"\"\"A block that impliments the add and norm block\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               name=None):\n",
        "    super().__init__(name=name)\n",
        "\n",
        "  def __call__(self, x, processed_x):\n",
        "    \n",
        "    added = x + processed_x\n",
        "    normalised = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)\n",
        "    return normalised(added)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIHHQTrcXb1b"
      },
      "source": [
        "### Building the Encoder "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDIHVioL9J7e"
      },
      "source": [
        "We now have all the building blocks that we need to build a single encoder block. \n",
        "\n",
        "**Code task:** Use your previous builded modules and create a new haiku module that is a transformer encoder block. One should also be able to retrieve the attention weights as an optional flag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1Y18G_OJGJu"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(hk.Module):\n",
        "  \"\"\"An encoder block for the Transformer stack.\"\"\"\n",
        "\n",
        "  def __init__(self, num_heads, d_m, widening_factor=4, name=None):\n",
        "    super().__init__(name=name)\n",
        "\n",
        "    self._num_heads = num_heads\n",
        "    self.d_m = d_m\n",
        "    self.mha = MultiHeadAttention(num_heads, d_m)\n",
        "    self.add_norm1 = AddNorm()\n",
        "    self.add_norm2 = AddNorm()\n",
        "    self.MLP = FeedForwardBlock(widening_factor=widening_factor)\n",
        "\n",
        "  def __call__(self, X, mask=None, return_att_weight=True):\n",
        "    sequence_len = X.shape[-2]\n",
        "    positions = # FINSIH ME\n",
        "    X = X + positions\n",
        "    attention, attention_weights = # FINISH ME\n",
        "    X = self.add_norm1(X, attention)\n",
        "    projection =  # FINISH ME\n",
        "    X = # FINISH ME\n",
        "    return (X, attention_weights) if return_att_weight else X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xrAGr3Td97i7"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!') \n",
        "\n",
        "class EncoderBlock(hk.Module):\n",
        "  \"\"\"An encoder block for the Transformer stack.\"\"\"\n",
        "\n",
        "  def __init__(self, num_heads, d_m, widening_factor=4, name=None):\n",
        "    super().__init__(name=name)\n",
        "\n",
        "    self._num_heads = num_heads\n",
        "    self.d_m = d_m\n",
        "    self.mha = MultiHeadAttention(num_heads, d_m)\n",
        "    self.add_norm1 = AddNorm()\n",
        "    self.add_norm2 = AddNorm()\n",
        "    self.MLP = FeedForwardBlock(widening_factor=widening_factor)\n",
        "\n",
        "  def __call__(self, X, mask=None, return_att_weight=True):\n",
        "    sequence_len = X.shape[-2]\n",
        "    positions = return_frequency_pe_matrix(sequence_len, self.d_m)\n",
        "    X = X + positions\n",
        "    attention, attention_weights = self.mha(X, mask, return_weights=True)\n",
        "    X = self.add_norm1(X, attention)\n",
        "    projection = self.MLP(X)\n",
        "    X = self.add_norm2(X, projection)\n",
        "    return (X, attention_weights) if return_att_weight else X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdaFnZ8P979f"
      },
      "source": [
        "Now that we have a single encoder block, lets create a new transformer encoder stack that stacks the encoder blocks into one single transformer encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tedqHar-YJfV"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(hk.Module):\n",
        "  \"\"\"Transformer encoder\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               num_heads: int,\n",
        "               num_layers: int,\n",
        "               d_m: int,\n",
        "               widening_factor=4,\n",
        "               name=None):\n",
        "    super().__init__(name=name)\n",
        "    self.num_layers = num_layers\n",
        "    self.num_heads = num_heads\n",
        "    self.widening_factor = widening_factor\n",
        "    self.blocks = [\n",
        "      EncoderBlock(self.num_heads, d_m, self.widening_factor) for _ in range(num_layers)\n",
        "    ]\n",
        "  def __call__(self, X, mask, return_att_weights=False):\n",
        "    if return_att_weights:\n",
        "      att_weights = []\n",
        "    \n",
        "    for block in self.blocks:\n",
        "      out = block(X, mask, return_att_weights)\n",
        "\n",
        "      if return_att_weights:\n",
        "        X = out[0]\n",
        "        att_weights.append(out[1])\n",
        "      else:\n",
        "        X = out\n",
        "    \n",
        "    return X if not return_att_weights else (X, jnp.array(att_weights).swapaxes(0,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDpwRd7CEKyi"
      },
      "outputs": [],
      "source": [
        "B, T, d_m, N = 4, 33, 16, 8\n",
        "\n",
        "# run this to test if your code is running\n",
        "def encode(sequences, num_heads=2, num_layers=2, widening_factor=4, d_m=d_m, return_weights=True):\n",
        "  encoder = TransformerEncoder(num_heads, num_layers, d_m, widening_factor)\n",
        "  return encoder(sequences,None,return_weights)\n",
        "\n",
        "encode = hk.transform(encode)\n",
        "\n",
        "# initialise module\n",
        "key = jax.random.PRNGKey(42)\n",
        "X = jax.random.normal(key, [B, T,d_m])\n",
        "print('BEFORE SHAPE:')\n",
        "print(X.shape)\n",
        "params = encode.init(key, X, N, d_m)\n",
        "new_x = encode.apply(params, key, X)\n",
        "print('AFTER SHAPE:')\n",
        "new_x[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxSD1tLIX4CJ"
      },
      "source": [
        "### Building the Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CouFQCMIK86f"
      },
      "source": [
        "Most of the ground work has now already happened. The only difference now is that we must pass the encoder outputs to each of the decoder blocks and apply the extra MHA block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4puW2jKJO6E5"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(hk.Module):\n",
        "  \"\"\"An decoder block for the Transformer stack.\"\"\"\n",
        "\n",
        "  def __init__(self, num_heads, d_m, widening_factor=4, name=None):\n",
        "    super().__init__(name=name)\n",
        "\n",
        "    self._num_heads = num_heads\n",
        "    self.d_m = d_m\n",
        "    self.mha = MultiHeadAttention(num_heads, d_m)\n",
        "    self.mha_combine = MultiHeadAttention(num_heads, d_m)\n",
        "    self.add_norm1 = AddNorm()\n",
        "    self.add_norm2 = AddNorm()\n",
        "    self.add_norm3 = AddNorm()\n",
        "    self.MLP = FeedForwardBlock(widening_factor=widening_factor)\n",
        "\n",
        "  def __call__(self, X, encoder_output , mask=None, return_att_weight=True):\n",
        "    sequence_len = X.shape[-2]\n",
        "    positions = return_frequency_pe_matrix(sequence_len, self.d_m)\n",
        "    X = X + positions\n",
        "    \n",
        "\n",
        "    attention, attention_weights_1 = self.mha(X, mask=mask, return_weights=True)\n",
        "    \n",
        "    X = self.add_norm1(X, attention)\n",
        "    \n",
        "    attention, attention_weights_2 = self.mha_combine(\n",
        "        Q=X,\n",
        "        K=encoder_output,\n",
        "        V=encoder_output,\n",
        "        mask=mask,\n",
        "        return_weights=True\n",
        "    )\n",
        "    \n",
        "    X = self.add_norm2(X, attention)\n",
        "    projection = self.MLP(X)\n",
        "    X = self.add_norm3(X, projection)\n",
        "\n",
        "    return (X, attention_weights_2) if return_att_weight else X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxeVVOxDT0hK"
      },
      "source": [
        "As you can see in the code, we now use the encoder outputs passed to the module when calculating the final attention weights before feeding everything through the MLP. \n",
        "\n",
        "Next, we just put everything together, just like we did for the encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZcdRdWKQETZ"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(hk.Module):\n",
        "  \"\"\"Transformer Decoder\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               num_heads: int,\n",
        "               num_layers: int,\n",
        "               d_m: int,\n",
        "               widening_factor=4,\n",
        "               name=None):\n",
        "    super().__init__(name=name)\n",
        "    self.num_layers = num_layers\n",
        "    self.num_heads = num_heads\n",
        "    self.widening_factor = widening_factor\n",
        "    self.blocks = [\n",
        "      DecoderBlock(self.num_heads, d_m, self.widening_factor) for _ in range(num_layers)\n",
        "    ]\n",
        "  def __call__(self, X, encoder_output, mask, return_att_weights=False):\n",
        "    if return_att_weights:\n",
        "      att_weights = []\n",
        "    for block in self.blocks:\n",
        "      out = block(X, encoder_output, mask, return_att_weights)\n",
        "      if return_att_weights:\n",
        "        X = out[0]\n",
        "        att_weights.append(out[1])\n",
        "      else:\n",
        "        X = out\n",
        "  \n",
        "    return X if not return_att_weights else (X, jnp.array(att_weights).swapaxes(0,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWGj3pjJ0C9l"
      },
      "outputs": [],
      "source": [
        "B, T, d_m, N = 18, 32, 16, 8\n",
        "\n",
        "# run this to test if your code is running\n",
        "def encode(sequences, encoder_output, num_heads=2, num_layers=2, widening_factor=4, d_m=d_m, return_weights=True):\n",
        "  print(encoder_output.shape)\n",
        "  decoder = TransformerDecoder(num_heads, num_layers, d_m, widening_factor)\n",
        "  return decoder(sequences,encoder_output,None,return_weights)\n",
        "\n",
        "encode = hk.transform(encode)\n",
        "\n",
        "# initialise module\n",
        "key = jax.random.PRNGKey(42)\n",
        "X = jax.random.normal(key, [B, T,d_m])\n",
        "encoder_output = jax.random.normal(key, [B, T,d_m])\n",
        "params = encode.init(key, X, encoder_output, N, d_m)\n",
        "new_y = encode.apply(params, key, X, encoder_output)\n",
        "print('AFTER SHAPE:')\n",
        "new_y[0].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts-WLr0zY26X"
      },
      "source": [
        "### Putting it all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXPxlqooYY3T"
      },
      "source": [
        "There is nothing more to it. We have built every single component required to build a transformer, now we just have to combine everything into one module. \n",
        "\n",
        "**Code task**: Build a encoder-decoder transformer haiku module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26tL91_bY5ZL"
      },
      "outputs": [],
      "source": [
        "class Transformer(hk.Module):\n",
        "  \"\"\"A full transformer encoder-decoder architecture\"\"\"\n",
        "\n",
        "  def __init__(self, num_heads, num_layers, d_m, widening_factor=4, name=None):\n",
        "    super().__init__(name=name)\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.encoder = # FINISH ME\n",
        "    self.decoder = # FINISH ME\n",
        "\n",
        "   def __call__(self, X_encoder, X_decoder, encoder_mask, decoder_mask, return_weights):\n",
        "    encoder_out = # FINISH ME \n",
        "    attention_input = encoder_out[0] if return_weights else encoder_out\n",
        "    decoder_out = # FINISH ME\n",
        "    logits = decoder_out[0] if return_weights else decoder_out\n",
        "\n",
        "    return logits if not return_weights else (logits, encoder_out[1], decoder_out[1])   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qW6MLaDGYXgv"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!') \n",
        "\n",
        "class Transformer(hk.Module):\n",
        "  \"\"\"A full transformer encoder-decoder architecture\"\"\"\n",
        "\n",
        "  def __init__(self, num_heads, num_layers, d_m, widening_factor=4, name=None):\n",
        "    super().__init__(name=name)\n",
        "    self.num_layers = num_layers\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.encoder = TransformerEncoder(\n",
        "        num_heads, \n",
        "        num_layers, \n",
        "        d_m, \n",
        "        widening_factor\n",
        "    )\n",
        "    self.decoder = TransformerDecoder(\n",
        "        num_heads, \n",
        "        num_layers, \n",
        "        d_m, \n",
        "        widening_factor\n",
        "    )\n",
        "\n",
        "  def __call__(self, X_encoder, X_decoder, encoder_mask, decoder_mask, return_weights):\n",
        "    encoder_out = self.encoder(X_encoder, encoder_mask, return_weights)\n",
        "    attention_input = encoder_out[0] if return_weights else encoder_out\n",
        "    decoder_out = self.decoder(X_decoder, attention_input, decoder_mask, return_weights)\n",
        "    logits = decoder_out[0] if return_weights else decoder_out\n",
        "\n",
        "    return logits if not return_weights else (logits, encoder_out[1], decoder_out[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3WZyV_s54_R"
      },
      "source": [
        "If everything is correct, then if we run the code below, everything should run without any issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbRxqe_lfRIq"
      },
      "outputs": [],
      "source": [
        "B, T, d_m, N = 18, 32, 16, 8\n",
        "\n",
        "# run this to test if your code is running\n",
        "def tranformer(sequences, num_heads=1, num_layers=1, widening_factor=4, d_m=d_m, return_weights=True):\n",
        "  transform = Transformer(num_heads, num_layers, d_m, widening_factor)\n",
        "  mask = jnp.tril(np.ones((sequences.shape[1], sequences.shape[1])))\n",
        "  return transform(sequences, sequences, None, mask, return_weights)\n",
        "\n",
        "tranformer = hk.transform(tranformer)\n",
        "\n",
        "# initialise module and get dummy output\n",
        "key = jax.random.PRNGKey(42)\n",
        "X = jax.random.normal(key, [B, T,d_m])\n",
        "params = tranformer.init(\n",
        "    key, X, num_heads=1, num_layers=1, widening_factor=4, d_m=d_m, return_weights=True\n",
        ")\n",
        "\n",
        "# extract output from decoder\n",
        "logits, encoder_att_weights, decoder_att_weights = tranformer.apply(\n",
        "    params, key, X, num_heads=1, num_layers=1, widening_factor=4, d_m=d_m, return_weights=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLeXyXNE5qq9"
      },
      "source": [
        "As a final sanity check, we can see that our attention weights behave as expected for now. The encoder weights can attend to all input sequences, and our decoder only attends to previous tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0PNKe1r3O9H"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1,2, figsize=(10,5))\n",
        "plt.suptitle('Decoder attention weights (left) vs Encoder attention weights(right')\n",
        "sns.heatmap(encoder_att_weights[0,0,0,...], ax=ax[0],cmap='Blues')\n",
        "sns.heatmap(decoder_att_weights[0,0,0,...], ax=ax[1],cmap='Blues')\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bqOml7HYNmm"
      },
      "source": [
        "### Training our model to invert sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-f_jnA2P_k5"
      },
      "source": [
        "To showcase the encoder-decoder transformer model we have just created, we will train it to reverse the order of an input string. For example:\n",
        "\n",
        "- *Input*: \"Look at me, I am reveresed.\"\n",
        "- *Output*: \"reversed. am I me, at Look\"\n",
        "\n",
        "Some might even say a little Yoda simulator ;) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3e4e_vSP8qM"
      },
      "source": [
        "#### Creating our dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKtkOftcQijr"
      },
      "source": [
        "We will use the tinyShakespear hosted by Andrej Karpathy [here](https://github.com/karpathy/char-rnn)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yw7DnRaUP8Hj"
      },
      "outputs": [],
      "source": [
        "# Download data set\n",
        "! wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RldPc6ZOTDnP"
      },
      "source": [
        "Next, we need to build a dataset class that will return a sample of text from tinyShakespear and the reversed version of the sentence. For now, do not worry too much about how this class works. It is just a way to iteratively run over the dataset and randomly sample snippets of text for an \"infinite\" number of steps.\n",
        "\n",
        "Another critical step to note is that we tokenize the data at a char level. In other words, all text is split and encoded to a unique ASCII value, with 128 unique values. The model will thus receive a sequence of integers, where we can then learn a lookup table of vectors for each ASCII character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3N12LcdTV7p"
      },
      "outputs": [],
      "source": [
        "BREAK_ORDS = (ord(' '), ord('\\n'), ord('\\t'), ord('.'))\n",
        "\n",
        "def ord_to_char(list_of_ords):\n",
        "  \"\"\"Format ascii np.array to char\"\"\"\n",
        "  return''.join([chr(o) for o in list_of_ords])\n",
        "\n",
        "class AsciiDatasetForInversionTask:\n",
        "  \"\"\"In-memory dataset of a single-file ASCII dataset for inversion task.\"\"\"\n",
        "\n",
        "  def __init__(self, path: str, batch_size: int, sequence_length: int):\n",
        "    \"\"\"Load a single-file ASCII dataset in memory.\"\"\"\n",
        "    self.vocab_size = 128\n",
        "    self._batch_size = batch_size\n",
        "\n",
        "    with open(path, 'r') as f:\n",
        "      corpus = f.read()\n",
        "\n",
        "    if not corpus.isascii():\n",
        "      raise ValueError('Loaded corpus is not ASCII.')\n",
        "\n",
        "    if '\\0' in corpus:\n",
        "      # Reserve 0 codepoint for pad token.\n",
        "      raise ValueError('Corpus must not contain null byte.')\n",
        "\n",
        "    # Tokenize by taking ASCII codepoints.\n",
        "    corpus = np.array([ord(c) for c in corpus if c!='\\n']).astype(np.int32)\n",
        "    assert np.min(corpus) > 0\n",
        "    assert np.max(corpus) < self.vocab_size  # Double-checking ASCII codepoints.\n",
        "\n",
        "    crop_len = sequence_length + 1\n",
        "    num_batches, ragged = divmod(corpus.size, batch_size * crop_len)\n",
        "    if ragged:\n",
        "      corpus = corpus[:-ragged]\n",
        "    corpus = corpus.reshape([-1, crop_len])\n",
        "\n",
        "    if num_batches < 10:\n",
        "      raise ValueError(f'Only {num_batches} batches; consider a shorter '\n",
        "                       'sequence or a smaller batch.')\n",
        "\n",
        "    self._ds = AsciiDatasetForInversionTask._infinite_shuffle(corpus, batch_size * 10)\n",
        "\n",
        "  def __next__(self):\n",
        "    \"\"\"Yield next mini-batch.\"\"\"\n",
        "    batch = [next(self._ds) for _ in range(self._batch_size)]\n",
        "    batch = np.stack(batch)\n",
        "    # Create the language modeling observation/target pairs.\n",
        "    return dict(input=batch[:, :], target=AsciiDatasetForInversionTask.invert_batch(batch))\n",
        "\n",
        "  def __iter__(self):\n",
        "    return self\n",
        "\n",
        "  @staticmethod\n",
        "  def invert_batch(batch):\n",
        "    inverted_batch = []\n",
        "    for instance in batch:\n",
        "      inverted_batch.append([])\n",
        "      last_seen_space_index = len(instance)\n",
        "      for i in range(len(instance)-1, -1, -1):\n",
        "        if instance[i] in BREAK_ORDS:\n",
        "          inverted_batch[-1].extend(instance[i+1:last_seen_space_index])\n",
        "          inverted_batch[-1].append(instance[i])\n",
        "          last_seen_space_index = i\n",
        "      inverted_batch[-1].extend(instance[i:last_seen_space_index])\n",
        "    return np.array(inverted_batch)\n",
        "\n",
        "  @staticmethod\n",
        "  def _infinite_shuffle(iterable, buffer_size):\n",
        "    \"\"\"Infinitely repeat and shuffle data from iterable.\"\"\"\n",
        "    ds = itertools.cycle(iterable)\n",
        "    buf = [next(ds) for _ in range(buffer_size)]\n",
        "    random.shuffle(buf)\n",
        "    while True:\n",
        "      item = next(ds)\n",
        "      idx = random.randint(0, buffer_size - 1)  # Inclusive.\n",
        "      result, buf[idx] = buf[idx], item\n",
        "      yield result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfJR9CMoUwzD"
      },
      "source": [
        "Now that we can load data and return infinite batches, lets see it in action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2pTRomeUtqB"
      },
      "outputs": [],
      "source": [
        "batch_size = 2\n",
        "seq_length = 32\n",
        "train_dataset = AsciiDatasetForInversionTask(\n",
        "    'input.txt', batch_size, seq_length)\n",
        "vocab_size = train_dataset.vocab_size\n",
        "\n",
        "batch = next(train_dataset)\n",
        "\n",
        "for obs, target in zip(batch['input'], batch['target']):\n",
        "  print(\"-\" * 10, 'Input', \"-\" * 11)\n",
        "  print('TEXT:', ord_to_char(obs))\n",
        "  print('ASCII:', obs)\n",
        "  print(\"-\" * 10, 'Target', \"-\" * 10)\n",
        "  print('TEXT:', ord_to_char(target))\n",
        "  print('ASCII:', target)\n",
        "\n",
        "\n",
        "print(f'\\n Total vocabulary size: {vocab_size}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1YoKTusQdJU"
      },
      "source": [
        "#### Training objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrKvWyk9AmpM"
      },
      "source": [
        "**Viewing it as a classification task**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4z7OBdEZAvx"
      },
      "source": [
        "Given an input sequence, which is a sequence of ASCII characters in this specific use case, our model should learn to take that sequence and reverse it. \n",
        "\n",
        "This can be framed as a sequence-2-sequence problem, where the decoder should predict the sequence, given the processed information from the encoder and the original sequence.\n",
        "\n",
        "The loss function will then be the sum of the loss over each token. The loos will be cross-entropy, where the ASCII token is the correct label to predict. Take note, that because the sequence can be padded during training, the loss is also masked to ignore the result on padded tokens. \n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=16-F7uCuJEvFn3e9PgpJpEQmsuMAUkCLT\" alt=\"drawing\" width=\"290\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCVb0hylgN3o"
      },
      "source": [
        "**Code task**: Implimement the cross entropy loss function below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBao6oI2ZDVp"
      },
      "outputs": [],
      "source": [
        "def sequence_loss_fn(logits, data):\n",
        "  targets = jax.nn.one_hot(data['target'], 128)\n",
        "  assert logits.shape == targets.shape\n",
        "\n",
        "  mask = jnp.greater(data['input'], 0)\n",
        "  loss = #FINSIH ME\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vX38mblNhV4k"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!') \n",
        "def sequence_loss_fn(logits, data):\n",
        "  \"\"\"Compute the loss on data wrt params.\"\"\"\n",
        "  targets = jax.nn.one_hot(data['target'], 128)\n",
        "  assert logits.shape == targets.shape\n",
        "  mask = jnp.greater(data['input'], 0)\n",
        "  loss = -jnp.sum(targets * jax.nn.log_softmax(logits), axis=-1)\n",
        "  loss = jnp.sum(loss * mask) / jnp.sum(mask)\n",
        "\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EKILfw9Au6X"
      },
      "source": [
        "**Viewing it as an generative task**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5R5y-JEA1ra"
      },
      "source": [
        "We can also solve this problem by iteratively generating a ASCII id, based on the previously generated ASCII's and input sequence. Notice here that the first token being fed into the decoder is a unique character, `<s>`, which is used to kickstart the process. This unique character we will represent with token ID $191$. \n",
        "\n",
        "\n",
        "\\\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1CF59zcS-odNrCZ6DV7TLU2PHNUaaJiwO\" alt=\"drawing\" width=\"290\"/>\n",
        "\n",
        "We are in essence, then trying to model the distribution: \n",
        "\n",
        "$$\n",
        "P\\left(y_{1}, y_{2}, \\ldots, y_{n}, \\mid x\\right)=\\prod_{t=1}^{n} p\\left(y_{t} \\mid y_{<t}, x\\right)\n",
        "$$\n",
        "\n",
        "The loss function will not change, but how we feed our data into the decoder will be different.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1eXwfOFZc4C"
      },
      "source": [
        "#### Training and inspecting models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YfDIMbnhph6"
      },
      "source": [
        "In the next section, we define all the processes required to train the model using the two objectives described above. A lot of this is now the work required to do training using Haiku. \n",
        "\n",
        "Below we define the forward functions that we can wrap in the transform function from Haiku. These functions take in our data and produce our logits as a full parallel prediction or generative iteratively.\n",
        "\n",
        "Please take note that in the below function, the ASCI characters are encoded using a lookup table and that there is an MLP at the end that predicts ASCI characters from our decoded token embeddings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHLFuYEMYOGH"
      },
      "outputs": [],
      "source": [
        "def build_forward_fn(vocab_size, d_model, num_heads, num_layers, widening_factor, generative=False):\n",
        "  def forward_fn(data, return_weights=False, vocab_size=vocab_size):\n",
        "    \"\"\"Forward pass.\"\"\"\n",
        "    tokens = data['input']\n",
        "    tokens_target = data['target']\n",
        "\n",
        "    # if generative, we need new token to say start generating\n",
        "    if generative:\n",
        "      vocab_size = vocab_size + 1\n",
        "\n",
        "    # Embed the input tokens and positions. \n",
        "    embed_init = hk.initializers.TruncatedNormal(stddev=0.02)\n",
        "    token_embedding_map = hk.Embed(vocab_size, d_model, w_init=embed_init)\n",
        "    token_embs = token_embedding_map(tokens)\n",
        "    \n",
        "    # Run the transformer over the inputs.\n",
        "    transformer = Transformer(num_heads, num_layers, d_model, widening_factor)\n",
        "\n",
        "    if generative:\n",
        "      # inject \"start\" token into target codes\n",
        "      start_token = jnp.ones((data['target'].shape[0], 1),dtype=jnp.int32)+jnp.array(vocab_size, dtype=jnp.int32)\n",
        "      tokens_target = jnp.concatenate((start_token, tokens_target), axis=1)\n",
        "      token_target_emb = token_embedding_map(tokens_target)\n",
        "      \n",
        "      max_mask_shape = max(tokens.shape[1], tokens_target.shape[1])\n",
        "      decoding_mask = jnp.tril(jnp.ones((tokens.shape[0], max_mask_shape)))\n",
        "      output_embeddings, weights1, weights2 = transformer(\n",
        "          token_embs, token_target_emb, None, decoding_mask, return_weights=True\n",
        "      )\n",
        "      # Predict the token IDs\n",
        "      logits = hk.Linear(vocab_size-1)(output_embeddings)[:,:-1,...] # we don't want predictions after sequence size\n",
        "      \n",
        "    else:\n",
        "      output_embeddings, weights1, weights2 = transformer(\n",
        "          token_embs, token_embs, None, None, return_weights=True\n",
        "      )\n",
        "      # Predict the token IDs\n",
        "      logits = hk.Linear(vocab_size)(output_embeddings)\n",
        "\n",
        "    return logits if not return_weights else (logits, weights1, weights2)\n",
        "  return forward_fn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wuhz4ummZTfK"
      },
      "source": [
        "##### Training parralel classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQTWhBV1l_j-"
      },
      "source": [
        "Next we need to initilalise our model and setup our optimiser using optax. Feel free to play with hyperparameters. We also set up the training data here again to easy hyperparameter changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV5e1HYenfak"
      },
      "outputs": [],
      "source": [
        "d_model = 128\n",
        "num_heads = 4\n",
        "num_layers = 1\n",
        "widening_factor = 4\n",
        "LR = 1e-3\n",
        "\n",
        "batch_size = 64\n",
        "seq_length =  32\n",
        "generative = False\n",
        "\n",
        "# set up the data\n",
        "train_dataset = AsciiDatasetForInversionTask(\n",
        "    'input.txt', batch_size, seq_length\n",
        ")\n",
        "vocab_size = train_dataset.vocab_size\n",
        "batch = next(train_dataset)\n",
        "\n",
        "# initialise model\n",
        "rng = jax.random.PRNGKey(42)\n",
        "forward_fn = hk.transform(build_forward_fn(vocab_size, d_model, num_heads, num_layers, widening_factor, generative))\n",
        "params = forward_fn.init(rng, batch)\n",
        "\n",
        "# set up the optimiser\n",
        "optimiser = optax.chain(\n",
        "    optax.clip_by_global_norm(1),\n",
        "    optax.adam(LR, b1=0.9, b2=0.99)\n",
        ")\n",
        "opt_state = optimiser.init(params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-q-73UWd-KW4"
      },
      "outputs": [],
      "source": [
        "#@title Haiku functions required for training - optional, but run this code block\n",
        "\n",
        "# set up function calculating loss\n",
        "def loss(params, batch):\n",
        "  vocab_size=128\n",
        "  key = jax.random.PRNGKey(42)\n",
        "  logits = forward_fn.apply(params, key, batch)\n",
        "  batch_loss = sequence_loss_fn(logits, batch)\n",
        "  return batch_loss\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def update(params, opt_state, batch):\n",
        "  # get data neded for training\n",
        "  grads = jax.grad(loss)(params, batch)\n",
        "  updates, opt_state = optimiser.update(grads, opt_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state\n",
        "\n",
        "# calculate loss per batch\n",
        "@jax.jit\n",
        "def calculate_batch_loss(params, batch, vocab_size):\n",
        "  key = jax.random.PRNGKey(42)\n",
        "  logits = forward_fn.apply(params, key, batch)\n",
        "  \n",
        "  return sequence_loss_fn(logits, batch)\n",
        "\n",
        "\n",
        "# get the model output\n",
        "def get_model_prediction(params, data, return_weights=False):\n",
        "  key = jax.random.PRNGKey(42)\n",
        "  logits, encoder_weights, decoder_weights = forward_fn.apply(params, key, data, True)\n",
        "  argmax_out = jnp.argmax(logits, axis=-1)\n",
        "  tokens = [ord_to_char(out) for out in argmax_out]\n",
        "  \n",
        "  if return_weights:    \n",
        "    return tokens, encoder_weights, decoder_weights\n",
        "  else:\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gn3y3AS2w-gb"
      },
      "outputs": [],
      "source": [
        "plotlosses = PlotLosses()\n",
        "\n",
        "MAX_STEPS = 1800\n",
        "LOG_EVERY = 8\n",
        "losses = []\n",
        "\n",
        "plotlosses = PlotLosses()\n",
        "\n",
        "key = jax.random.PRNGKey(42)\n",
        "\n",
        "# Training & evaluation loop.\n",
        "for step in range(MAX_STEPS):\n",
        "  data = next(train_dataset)\n",
        "  params, opt_state = update(params, opt_state, data)\n",
        "  losses.append(calculate_batch_loss(params, data, vocab_size))\n",
        "\n",
        "  if step % LOG_EVERY == 0:\n",
        "    loss_ = jnp.array(losses).mean()\n",
        "    plotlosses.update({\n",
        "      'loss': loss_,\n",
        "    })\n",
        "    plotlosses.send()\n",
        "    losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyfqX9AOF1fz"
      },
      "outputs": [],
      "source": [
        "data = next(train_dataset)\n",
        "\n",
        "predicted_string, encoder_weights, decoder_weights = get_model_prediction(params, data, True)\n",
        "target_string =[ord_to_char(t) for t in data['target']]\n",
        "input_string =[ord_to_char(t) for t in data['input']]\n",
        "\n",
        "for y_hat, y, input_ in zip(predicted_string[:2], target_string[:2], input_string[:2]):\n",
        "      print(\"-\" * 10, 'Input', \"-\" * 11)\n",
        "      print(input_)\n",
        "      print(\"-\" * 10, 'Target', \"-\" * 10)\n",
        "      print(y)\n",
        "      print(\"-\" * 10, 'Prediction', \"-\" * 10)\n",
        "      print(y_hat)\n",
        "      print()\n",
        "      print('*'*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjeoBTTmzMUr"
      },
      "source": [
        "Lets plot the average attention matrices over a batch of data, and see what our model has learned. This is a very typical process in transformer development and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMeoRKVPXvPm"
      },
      "outputs": [],
      "source": [
        "def plot_weights(mean_weights, num_layers, num_heads, block='Encoder'):\n",
        "  for layer in range(num_layers):\n",
        "    fig, ax = plt.subplots(1,num_heads, figsize=(45,5))\n",
        "    plt.suptitle(f'Encoder Layer {layer} attention heads')\n",
        "    for h in range(num_heads):\n",
        "      ax[h].set_title(f'Head {h}')\n",
        "      sns.heatmap(mean_weights[layer,h,...], ax=ax[h],cmap='Blues')\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHgMM2A6zLnq"
      },
      "outputs": [],
      "source": [
        "mean_encoder_weights = jnp.mean(encoder_weights,axis=0)\n",
        "plot_weights(mean_encoder_weights, num_layers, num_heads, block='Encoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMWj4j9AW1Mi"
      },
      "outputs": [],
      "source": [
        "mean_decoder_weights = jnp.mean(decoder_weights,axis=0)\n",
        "plot_weights(mean_decoder_weights, num_layers, num_heads, block='Decoder')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idM0U9G2146L"
      },
      "source": [
        "**Group task**: Discuss with your friend wheter these outputs make sense."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK4kUFOsZbkJ"
      },
      "source": [
        "##### Training generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGVkYlYRZi2g"
      },
      "source": [
        "Next, let us train our generator and see how it performs. The previous method could have been solved by just using the encoder part, but now we show it is possible by utilising the decoder-specific changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqpbVJwKZmNp"
      },
      "outputs": [],
      "source": [
        "d_model = 128\n",
        "num_heads = 2\n",
        "num_layers = 3\n",
        "widening_factor = 2\n",
        "LR = 1e-3\n",
        "\n",
        "batch_size = 64\n",
        "seq_length =  32\n",
        "generative = True\n",
        "\n",
        "# set up the data\n",
        "train_dataset = AsciiDatasetForInversionTask(\n",
        "    'input.txt', batch_size, seq_length\n",
        ")\n",
        "vocab_size = train_dataset.vocab_size\n",
        "batch = next(train_dataset)\n",
        "\n",
        "# initialise model\n",
        "rng = jax.random.PRNGKey(42)\n",
        "forward_fn = hk.transform(build_forward_fn(vocab_size, d_model, num_heads, num_layers, widening_factor, generative))\n",
        "params = forward_fn.init(rng, batch)\n",
        "\n",
        "# set up the optimiser\n",
        "optimiser = optax.chain(\n",
        "    optax.clip_by_global_norm(1),\n",
        "    optax.adam(LR, b1=0.9, b2=0.99)\n",
        ")\n",
        "opt_state = optimiser.init(params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_2ljKq8aRst"
      },
      "outputs": [],
      "source": [
        "plotlosses = PlotLosses()\n",
        "\n",
        "MAX_STEPS = 2500\n",
        "LOG_EVERY = 32\n",
        "losses = []\n",
        "\n",
        "key = jax.random.PRNGKey(42)\n",
        "# Training & evaluation loop.\n",
        "for step in range(MAX_STEPS):\n",
        "  data = next(train_dataset)\n",
        "  params, opt_state = update(params, opt_state, data)\n",
        "  losses.append(calculate_batch_loss(params, data, vocab_size))\n",
        "\n",
        "  if step % LOG_EVERY == 0:\n",
        "      loss_ = jnp.array(losses).mean()\n",
        "      plotlosses.update({\n",
        "        'loss': loss_,\n",
        "      })\n",
        "      plotlosses.send()\n",
        "      losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IP7crkAoaqZ-"
      },
      "outputs": [],
      "source": [
        "data = next(train_dataset)\n",
        "\n",
        "predicted_string, encoder_weights, decoder_weights = get_model_prediction(params, data, True)\n",
        "target_string =[ord_to_char(t) for t in data['target']]\n",
        "input_string =[ord_to_char(t) for t in data['input']]\n",
        "\n",
        "for y_hat, y, input_ in zip(predicted_string[:2], target_string[:2], input_string[:2]):\n",
        "      print(\"-\" * 10, 'Input', \"-\" * 11)\n",
        "      print(input_)\n",
        "      print(\"-\" * 10, 'Target', \"-\" * 10)\n",
        "      print(y)\n",
        "      print(\"-\" * 10, 'Prediction', \"-\" * 10)\n",
        "      print(y_hat)\n",
        "      print()\n",
        "      print('*'*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fzq6UbqaACQv"
      },
      "outputs": [],
      "source": [
        "mean_encoder_weights = jnp.mean(encoder_weights,axis=0)\n",
        "plot_weights(mean_encoder_weights, num_layers, num_heads, block='Encoder')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_YmMCTLYYDU"
      },
      "outputs": [],
      "source": [
        "mean_decoder_weights = jnp.mean(decoder_weights,axis=0)\n",
        "plot_weights(jnp.where(mean_decoder_weights==1, 0.5, mean_decoder_weights), num_layers, num_heads, block='Decoder')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35IPv-Ujjta4"
      },
      "source": [
        "### Decoder / Encoder only models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA6Kto9lCyRT"
      },
      "source": [
        "In this practical, we introduced what is known as a vanilla (original) transformer, where a decoder and encoder work together. However, there is no need to have this architecture, as both the encoder and decoder blocks can also operate separately on their own. Most of today's well-known transformers fall into decoder/encoder-only transformers. \n",
        "\n",
        "We are usually interested in training our model to extract semantics or representations from our data when we use only encoder blocks. We are interested in generative models when we use decoder-only blocks. These encoder blocks are, in many cases but not all cases, encoder blocks with masks, i.e. not two MHA blocks). Famous models for all three cases are listed below. \n",
        "\n",
        "**Vanilla transformers**:\n",
        "* [Original attention is all you need MLT](https://arxiv.org/abs/1706.03762?amp=1)\n",
        "* [T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)\n",
        "* [BART](https://arxiv.org/abs/1910.13461)\n",
        "\n",
        "**Encoder only**:\n",
        "* [Bert](https://arxiv.org/abs/1810.04805)\n",
        "* [RoBERTa](https://arxiv.org/abs/1907.11692)\n",
        "* [ELECTRA](https://openreview.net/pdf?id=r1xMH1BtvB)\n",
        "\n",
        "**Decoder only**:\n",
        "* [GPT-3](https://arxiv.org/abs/2005.14165)\n",
        "* [CTRL](https://arxiv.org/abs/1909.05858)\n",
        "* [Transformer-XL](https://arxiv.org/abs/1901.02860)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE6hEihPhAhK"
      },
      "source": [
        "### Section Quiz \n",
        "\n",
        "Optional end of section quiz. Below is an example of an assessment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "L03B3HKwhAhK"
      },
      "outputs": [],
      "source": [
        "#@title Generate Quiz Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "HTML(\n",
        "\"\"\"\n",
        "<iframe \n",
        "\tsrc=\"https://forms.gle/zbJoTSz3nfYq1VrY6\",\n",
        "  width=\"80%\" \n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rectkTs9iFHg"
      },
      "source": [
        "## **Hugging Face** - OPTIONAL (but recommended)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFBw8kRx-4Mk"
      },
      "source": [
        "\n",
        "\n",
        "<img src=\"data:image/png;base64,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\" width=\"10%\" />\n",
        "\n",
        "\n",
        "[Hugging Face](https://huggingface.co/) is a startup founded in 2016 and, in their own words: \"are on a mission to democratize good machine learning, one commit at a time.\"\n",
        "\n",
        "They have developed various open-source packages and allow users to easily interact with a large corpus of pretrained transformer models (across all modalities) and datasets to train or fine-tune pre-trained transformers.\n",
        "\n",
        "Their software is used widely in industry and research. The following sections show how one can interact with their various features, access SOTA models, and train your own model.\n",
        "\n",
        "This is an optional section for the practical. Still, working through it in the practical or afterwards is highly recommended. Understanding Hugging Face will allow you to build a very quick proof of concept system to test out various hypotheses, whereafter, the system you build can simply be used, or a new system can be developed with the knowledge that the original idea has merit. \n",
        "\n",
        "It should also be noted that various languages are still severely under-resourced, even in this ecosystem. See it as an opportunity also to see where the gaps are and how we as a community can reduce this gap. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq_pX3XTefu3"
      },
      "source": [
        "### Datasets Package - <font color='blue'>`Beginner`</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn-vfzXyehPu"
      },
      "source": [
        "Along with all the models being availible, datasets are also hosted on the hugginface hub. One can visit the [dataset hub](https://huggingface.co/datasets), and browse for interesting datasets and very easilly access it through the [datasets](https://github.com/huggingface/datasets) package.\n",
        "\n",
        "Lets say for instance we want to build an text intent classification model. What we do then is to go to the link above, and use the search tags to find a dataset that seems like a good fit for us. Doing this, we find the [`banking77`](https://huggingface.co/datasets/banking77) dataset. Below we then load in this dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4StYbstjeitO"
      },
      "outputs": [],
      "source": [
        "# just for notebook cleanliness\n",
        "from datasets.utils.logging import set_verbosity_error\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"banking77\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMh4aox5eh8h"
      },
      "source": [
        "Here we see that the API returns a variable of DatasetDict type, which containins our dataset that has split into two datasets, i.e. train and test splits. We also see that each of these datasets contain `text` and `label` features. \n",
        "\n",
        "Lets investigate how the data looks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qFr15bkemIu"
      },
      "outputs": [],
      "source": [
        "dataset['train']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzyH1lNWen3X"
      },
      "source": [
        "We see that our dataset splits are of type Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEQFsm0HepIm"
      },
      "outputs": [],
      "source": [
        "train_intents = dataset['train']\n",
        "test_intents = dataset['test']\n",
        "\n",
        "print('Text: ', train_intents['text'][0])\n",
        "print('Label: ', train_intents['label'][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7--kHR7eqej"
      },
      "source": [
        "Sometimes we want to work with only a subset of the dataset and we thus need to filter out the rest. Luckilly, the datasets have an easy to use filter functionality. Below we filter to only see text which relates to label 11. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehbEghPSetvI"
      },
      "outputs": [],
      "source": [
        "# we are only the first 5 text samples\n",
        "train_intents.filter(lambda data: data['label']==1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvcEMvEDevSn"
      },
      "source": [
        "We can also apply a function to each data item by mapping the function to each \"row\" in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-0tUb6sewij"
      },
      "outputs": [],
      "source": [
        "# creating small dataset as example of using select\n",
        "example_dataset = train_intents.select(range(10))\n",
        "\n",
        "# printing first character of a text example\n",
        "example_dataset.map(lambda example: print(example['text'][0]));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgt4MFR4ex14"
      },
      "source": [
        "Using the map function, we can also add a new collumn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gtdQeKVezT4"
      },
      "outputs": [],
      "source": [
        "def add_sentence_len(example):\n",
        "    example['lenght'] = len(example['text'])\n",
        "    return example\n",
        "\n",
        "# add new column\n",
        "example_dataset = example_dataset.map(lambda example: add_sentence_len(example))\n",
        "\n",
        "print(example_dataset)\n",
        "print('New data:', example_dataset['lenght'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ_qP_Jqe0r5"
      },
      "source": [
        "**Code Task**: \n",
        "- We want to build a classifier out of this. We need to investigate the distrubution of classes to see if our dataset is balanced or not. Write code that generates a dictionary containing the count of each class for both the train and test dataset.\n",
        "- Filter out classes with less than 150 classes in the training set, and ensure only the remaining classes are in the test test. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42qddlWve2Zy"
      },
      "outputs": [],
      "source": [
        "# task 1\n",
        "unique_labels = # FINISH ME\n",
        "total_unique_labels = # FINISH ME\n",
        "\n",
        "train_counts = # FINISH ME\n",
        "test_counts = # FINISH ME\n",
        "\n",
        "# task 2\n",
        "passed_labels = [label for label, count in train_counts.items() if count>=150]\n",
        "train_intents = # FINISH ME\n",
        "test_intents = # FINISH ME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ABrM5iAIe4YF"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "\n",
        "# # task 1\n",
        "unique_labels = np.unique(train_intents['label'])\n",
        "total_unique_labels = len(unique_labels)\n",
        "\n",
        "train_counts = {label:sum(label==train_intents['label']) for label in unique_labels}\n",
        "test_counts = {label:sum(label==train_intents['label']) for label in unique_labels}\n",
        "\n",
        "# task 2\n",
        "passed_labels = [label for label, count in train_counts.items() if count>=150]\n",
        "train_intents = train_intents.filter(lambda data: data['label'] in passed_labels)\n",
        "test_intents = test_intents.filter(lambda data: data['label'] in passed_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz4U0IZ8e34k"
      },
      "source": [
        "**Other modalities**\n",
        "\n",
        "Text is not the only data that can be accessed, audio and image data can be accessed just as easilly. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPCHECJQiFU7"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "dataset = load_dataset(\"cgarciae/cartoonset\")\n",
        "Image.open(BytesIO(dataset['train'][\"img_bytes\"][231]))\n",
        "\n",
        "# # free up a bit of space\n",
        "del dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL4j1JZse8wi"
      },
      "source": [
        "### Transformers Package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bKe1WDEe9eX"
      },
      "source": [
        "Now that we have a way in accessing data, lets shift our intention to accessin the hundreds of pretrained transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po9R6qiSe_HT"
      },
      "source": [
        "#### Pipeline - <font color='blue'>`Beginner`</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcK8-KpZfA7L"
      },
      "source": [
        "The easiest method to access a vast range of pre-trained models and use tasks is through the `pipeline` API.\n",
        "\n",
        "Pipelines group together a pretrained model found on their models hub with the preprocessing that was used during that model's training. To use the pipeline, one must import it from the [transformers](https://github.com/huggingface/transformers) library and specify the task and model you want.\n",
        "\n",
        "For a list of models, visit [this](https://huggingface.co/models) page that allows you to search through all models currently on the hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjTFOuylfDKH"
      },
      "outputs": [],
      "source": [
        "# When calling the function for the first time, the model, and its tokenizer, will be automatically downloaded\n",
        "sentiment_model = pipeline(task='sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "print(sentiment_model(\"I love this practical!\"))\n",
        "print(sentiment_model(\"I hate this practical!\"))\n",
        "\n",
        "# passing more than one sentence\n",
        "sentence_batch = [\n",
        "  'This is much quicker and easier to build a POC with than training everything from scratch',\n",
        "  'It really hurts when I stub my toe',\n",
        "  'I want to get ice cream'\n",
        "]\n",
        "print('\\nBatch output:')\n",
        "sentiment_model(sentence_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNYlFKWNfCL7"
      },
      "source": [
        "Notice that the first sentence we process in our batch of sentences is predicted to be Negative, with a relative low score of $0.51$, even though we feel this should be more neutral? The low score indicates this and we can interepet that when scores are low the actual label was meant to be Neutral, but this model was trained to do a binary prediction only.\n",
        "\n",
        "This model you just used is a Distellbert model, which was trained on *8 16GB V100s for 90 hours*, and you could use it as quickly as that.\n",
        "\n",
        "**Code Task:** Apply the zero shot model to all of our test intent chatbot examples, extracting the predicted senitment label into a new collumn, using the map function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00lgxVY8fKPw"
      },
      "outputs": [],
      "source": [
        "def get_sentiment(example):\n",
        "  # FINISH ME\n",
        "  return sentiment\n",
        "\n",
        "train_intents = # FINISH ME\n",
        "test_intents = # FINISH ME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KY_pOGR6fLYJ"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "def get_sentiment(example):\n",
        "  example['sentiment'] = sentiment_model(example['text'])[0]['label']\n",
        "  return example\n",
        "\n",
        "test_intents = test_intents.map(lambda example: get_sentiment(example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txcTUTbUfMnT"
      },
      "outputs": [],
      "source": [
        "plt.title('Count of positive versus negative text inputs')\n",
        "sns.countplot(test_intents['sentiment']);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg3ti5iEfGIv"
      },
      "source": [
        "**Group code task (optional depending on time)**: (Hint: use the tags when searching the [model hub](https://huggingface.co/models))\n",
        "- Search for other pipeline tasks available, and dicuss with your friend what you did and found. ([Hint](https://huggingface.co/docs/transformers/v4.20.1/en/main_classes/pipelines#pipelines))\n",
        "- Play with different language models and see how they perform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeMfN56EfS7c"
      },
      "outputs": [],
      "source": [
        "your_pipeline = pipeline(\n",
        "    task='text-generation', # CHANGE ME TO OTHER STUFF\n",
        "    model='gpt2' # CHANGE ME AS WELL\n",
        ")\n",
        "text = 'I like ice-cream and '\n",
        "your_pipeline(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSu5FY7CfRJ6"
      },
      "outputs": [],
      "source": [
        "# freeing up memory for future tasks\n",
        "del your_pipeline\n",
        "del sentiment_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSJ-Fh49fU-9"
      },
      "source": [
        "#### Training a chatbot intent model  - <font color='blue'>`Advanced`</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHzc2lpOfWVa"
      },
      "source": [
        "If you want more controll than the pipeline API provides you, you can also use the predefined model classes. \n",
        "\n",
        "To showcase this, we will be training a custom model on top of a large transformer.\n",
        "\n",
        "To set the scene, lets say for instance that we want to train an intent model that can be used along with a chatbot. This intent model will be responsible to predict the true underlying intent found within the text. \n",
        "\n",
        "Your imaganiry friend has built an intent model using TF-IDF techniques, but you think that you can use transformers for this task and that it will perform better. \n",
        "\n",
        "You have heard about the famous [DistilBERT](https://arxiv.org/pdf/1910.01108.pdf) model that was trained to extract text features, and yout think this model can be a perfect fit to extract features for your intent model, as it is an encoder only transformer architecture that produces strong token representations.\n",
        "\n",
        "To start your training process, you have two steps to follow:\n",
        "\n",
        "* Get the tokenizer\n",
        "* Get the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MBCwSG5i38v"
      },
      "source": [
        "###### **Getting the tokenizer**\n",
        "\n",
        "A tokenizer is responsible for preprocessing text into a format that is understandable to the model. It is very important to use the same tokenizer as the model you will be finetuning.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7ufSoUxi4uP"
      },
      "outputs": [],
      "source": [
        "# we want the Distilbert model, thus we import the correct tokenizer the model train\n",
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "# we specify a specif distilbert \n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "print('Tokenizer output:')\n",
        "output = tokenizer('This is example text')\n",
        "print(output)\n",
        "\n",
        "print('Tokens converted back to string')\n",
        "print(tokenizer.decode(output['input_ids']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW_VglCQfXfn"
      },
      "source": [
        "Looking at the output from the above, we that the special tokens are the `[CLS]` and `[SEP]` tokens. This is important to note, as we will be using the final output of the model for the `[CLS]` token when predicting the intent. \n",
        "\n",
        "This is a very common thing to do, where the token that indicates the start of the sentence is used when making predictions on the sentence. Seeing as our system is built in Jax, we will need to tell the tokenizer to return the data in the correct format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eR0yqg-mfZjF"
      },
      "outputs": [],
      "source": [
        "token_batch = tokenizer(\n",
        "    [\"We are very happy to show you the ð¤ Transformers library.\", \"We hope you don't hate it.\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"jax\",\n",
        ")\n",
        "print(token_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqBOZ4YLfa0H"
      },
      "source": [
        "Seeing as we want to use these tokens throughout our training process and for processing the embeddings, lets map the tokenizer output to new collumn for our train and test splits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyDoAqXvfc_K"
      },
      "source": [
        "##### **Getting the transformer and generating embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxSkOij_fexe"
      },
      "source": [
        "No that we can encode our text quickly into data that our transformer can processes, let gather and download the pretrained transformer model from the hub and generate representations for each text example.\n",
        "\n",
        "As was mentioned above, we are interested for now only in the `[CLS]` token embedding. One can ofcourse look at averaging over all token (being sure to only use tokens who are note masked) and see how it compares. We leave this as an excersise for the reader to compare how it changes the performance of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFhSfCOKfgA5"
      },
      "outputs": [],
      "source": [
        "from transformers import FlaxDistilBertModel\n",
        "\n",
        "# we use FlaxDistillBertModel because we are in the JAX world\n",
        "distell_bert_model = FlaxDistilBertModel.from_pretrained(model_name)\n",
        "\n",
        "tokens = tokenizer([train_intents[0]['text']])\n",
        "embeddings = distell_bert_model(**tokens)[0][:,0]\n",
        "embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKgHz1_tfh10"
      },
      "source": [
        "Applying one by one is extremely slow, so lets rather infer in batches, using the `batch` flag in the map function. It is important here that the function that is being mapped works with batches and return the data in the correct format, i.e a dictionary with the new column name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPkm_JdsfjIX"
      },
      "outputs": [],
      "source": [
        "def get_embedding(batch):\n",
        "\n",
        "  text = batch['text']\n",
        "  tokens = tokenizer(\n",
        "      text,\n",
        "      padding=True,\n",
        "      truncation=True,\n",
        "      max_length=50,\n",
        "      return_tensors=\"jax\",\n",
        "  )\n",
        "\n",
        "  cls_embeddings = np.array(distell_bert_model(**tokens)[0][:,0])\n",
        "  return {'embedding':cls_embeddings}\n",
        "\n",
        "train_intents = train_intents.map(lambda batch: get_embedding(batch), batched=True)\n",
        "test_intents = test_intents.map(lambda batch: get_embedding(batch), batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDcAZzm3fkfm"
      },
      "source": [
        "To see whether these embeddings are in anyway usefull, lets plot a few projected embeddings and their labels and see how it looks. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbCy-UK7flz9"
      },
      "outputs": [],
      "source": [
        "# Sampling to get a clear picture with less data.\n",
        "sample_labels = [5,11,20,28,34,45,51,76]\n",
        "plot_data = train_intents.filter(lambda data: data['label'] in sample_labels)\n",
        "plot_projected_embeddings(plot_data['embedding'], [str(l) for l in plot_data['label']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8agDZwufoFR"
      },
      "source": [
        "We see there are clear clusters forming, but there is defnitely still some work that can be done here.\n",
        "\n",
        "Lets thus train a non linear model on this data to try and find something that seperate this intents.\n",
        "\n",
        "We have a choice really of what we want to do, given data our new dataset can just be interpeted as a database. We can either train another neural netwok, or we can train anything ranging from a logistic regression model to a XGBoost model.\n",
        "\n",
        "Lets start by training a neural network, as we are at the deep learning indaba ;)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KohN7yfbfmOP"
      },
      "outputs": [],
      "source": [
        "# extracting data into more usable state for all methods\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder() # labels should be from 0 - N\n",
        "\n",
        "train_embeddings = train_intents['embedding']\n",
        "train_labels = jnp.array(le.fit_transform(train_intents['label']))\n",
        "test_embeddings = test_intents['embedding']\n",
        "test_labels = jnp.array(le.transform(test_intents['label']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LxKYDyYfr3c"
      },
      "source": [
        "To load batches of embedding, we feed our extracted embeddings and labels into tensorflow datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og_F4Hyrfquy"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "# creating tensorflow dataset loaders\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_embeddings,train_labels))\n",
        "train_ds = train_ds.\\\n",
        "  shuffle(buffer_size=len(train_embeddings),reshuffle_each_iteration=True).\\\n",
        "  batch(64)\n",
        "\n",
        "# we do not want to shuffle test data\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_embeddings,test_labels))\n",
        "test_ds = test_ds.batch(64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW5-9GbNf1nX"
      },
      "source": [
        "##### **Training intent model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-UfWogVf3zc"
      },
      "source": [
        "Even though the methods below only show us updating the weights or parameters of downstream models, there is now reason that one can not finetune the entire Distilbert model on the new dataset and task. This will just require much more compute and in many cases the extra costs are not linearly correlated with improved performance. This is why in this practical, why only train downstream models utilising the pretrained embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekey34T1f5jK"
      },
      "source": [
        "###### MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFetHB7tf641"
      },
      "source": [
        "Our intent model will be 2 layer MLP. \n",
        "\n",
        "**Code task:** Finish the 2 layer MLP Haiku Module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie5_Hf8Ff75r"
      },
      "outputs": [],
      "source": [
        "# build a training model\n",
        "\n",
        "class IntentClassifier(hk.Module):\n",
        "  \"\"\"A MLP which predicts intent from transformer embeddings\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self._init_scale = .5\n",
        "    self.number_classes = #FILL ME IN\n",
        "\n",
        "  def __call__(self, embeddings):\n",
        "    embedding_size = # FILL ME IN\n",
        "    initializer = hk.initializers.VarianceScaling(self._init_scale)\n",
        "    projection_layer = hk.Linear(embedding_size, w_init=initializer)\n",
        "    classification_layer = # FILL ME IN\n",
        "\n",
        "    projections = jax.nn.relu(projection_layer(embeddings))\n",
        "\n",
        "    logits = # FILL ME IN\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Kjq62LOFf9Ik"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "\n",
        "# build a training model\n",
        "\n",
        "class IntentClassifier(hk.Module):\n",
        "  \"\"\"A MLP which predicts intent from transformer embeddings\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self._init_scale = .5\n",
        "    self.number_classes = 38\n",
        "\n",
        "  def __call__(self, embeddings):\n",
        "    embedding_size = embeddings.shape[-1] \n",
        "    initializer = hk.initializers.VarianceScaling(\n",
        "        self._init_scale)\n",
        "    projection_layer = hk.Linear(\n",
        "        embedding_size, \n",
        "        w_init=initializer\n",
        "    )\n",
        "    classification_layer = hk.Linear(\n",
        "        self.number_classes,\n",
        "        w_init=initializer\n",
        "    )\n",
        "\n",
        "    projections = jax.nn.relu(\n",
        "        projection_layer(embeddings)\n",
        "    )\n",
        "    logits = classification_layer(projections)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS8bmMangAOh"
      },
      "source": [
        "Next we build the Haiku training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZp6hQMNf_Dv"
      },
      "outputs": [],
      "source": [
        "# initiliase model and optmiser\n",
        "\n",
        "def classify_intent(embeddings):\n",
        "  model = IntentClassifier()\n",
        "  return model(embeddings)\n",
        "\n",
        "classify_intent = hk.transform(classify_intent)\n",
        "\n",
        "# initialise model\n",
        "rng = jax.random.PRNGKey(42)\n",
        "x = np.zeros([1, 768])\n",
        "params = classify_intent.init(rng, x)\n",
        "\n",
        "optimiser = optax.adam(1e-3)\n",
        "opt_state = optimiser.init(params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwY8XsRMgEET"
      },
      "outputs": [],
      "source": [
        "# calculate loss \n",
        "key = jax.random.PRNGKey(42)\n",
        "\n",
        "def loss(params, batch):\n",
        "  \"\"\"Cross-entropy classification loss\"\"\"\n",
        "  batch_size = len(batch['labels'])\n",
        "  logits = classify_intent.apply(params,key, batch['embeddings'])\n",
        "  labels = jax.nn.one_hot(batch['labels'], num_classes=38)\n",
        "  log_likelihood = jnp.sum(labels * jax.nn.log_softmax(logits))\n",
        "  return -log_likelihood / batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvoc6MS1gM7-"
      },
      "outputs": [],
      "source": [
        "# update weights\n",
        "@jax.jit\n",
        "def update(params, opt_state, batch):\n",
        "  # get data neded for training\n",
        "  grads = jax.grad(loss)(params, batch)\n",
        "  updates, opt_state = optimiser.update(grads, opt_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2EtwmKTgSpk"
      },
      "outputs": [],
      "source": [
        "# calculate accuracy per batch\n",
        "@jax.jit\n",
        "def accuracy(params, batch):\n",
        "    predictions = classify_intent.apply(params, key, batch['embeddings'])\n",
        "    print(predictions)\n",
        "    return jnp.mean(jnp.argmax(predictions, axis=-1) == batch[\"labels\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UZ4x66OgUef"
      },
      "outputs": [],
      "source": [
        "# Training & evaluation loop.\n",
        "from tqdm import tqdm\n",
        "for epoch in range(20):\n",
        "\n",
        "  train_accs = 0\n",
        "  total_calcs = 0\n",
        "  for batch in tqdm(train_ds, desc='Train steps', leave=False):\n",
        "    batch = {\n",
        "        'embeddings':jnp.array(batch[0]),\n",
        "        'labels':jnp.array(batch[1])\n",
        "    }\n",
        "    params, opt_state = update(params, opt_state, batch)\n",
        "    train_accs+=accuracy(params, batch)\n",
        "    total_calcs+=1\n",
        "\n",
        "  if epoc%5==0:\n",
        "    print(f'At epoch: {epoc}') \n",
        "    train_accs /= round(total_calcs,2)\n",
        "\n",
        "    test_accs = 0\n",
        "    total_calcs = 0\n",
        "\n",
        "    for batch in tqdm(test_ds, desc='Test steps', leave=False):\n",
        "      batch = {\n",
        "          'embeddings':jnp.array(batch[0]),\n",
        "          'labels':jnp.array(batch[1])\n",
        "      }\n",
        "      test_accs+=accuracy(params, batch)\n",
        "      total_calcs+=1\n",
        "    \n",
        "    test_accs /= round(total_calcs,2)\n",
        "\n",
        "    print(f'\\nTrain accuracy:{train_accs}')\n",
        "    print(f'Test accuracy:{test_accs}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCAs9gLVgDnq"
      },
      "source": [
        "Now that we have a trained MLP that can predict the intent, we need to writ code that given new text, will classify an intent. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiCROb4lgZhb"
      },
      "outputs": [],
      "source": [
        "def predict_new_text_mlp(text):\n",
        "  embedding = get_embedding({'text':[text]})['embedding']\n",
        "  logits = classify_intent.apply(params, key, embedding)\n",
        "  predicted_intent = jnp.argmax(jax.nn.softmax(logits))\n",
        "  converted_back_intent = le.inverse_transform([predicted_intent])[0]\n",
        "  print(f'Predicted intent for input: \"{text}\" is {int(converted_back_intent)}')\n",
        "  \n",
        "  index = jnp.where(train_labels==predicted_intent)[0]\n",
        "  print(f'Example training data from this class: {train_intents[index][\"text\"][0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9o8sN2-gbIw"
      },
      "outputs": [],
      "source": [
        "predict_new_text_mlp('Can I get a refund please')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvyQtDmggcvn"
      },
      "source": [
        "###### Logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5TgD99zgeQO"
      },
      "source": [
        "As a final experiment, we will see how a logistic regression model performs against our MLP, and how quick it can be to get very quick results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxVGB2PGggQ4"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "model = LogisticRegression(max_iter=6000)\n",
        "model.fit(train_embeddings, train_labels)\n",
        "y_hat_train = model.predict(train_embeddings)\n",
        "y_hat_test = model.predict(test_embeddings)\n",
        "\n",
        "train_accuracy = jnp.sum(y_hat_train == train_labels)/len(y_hat_train)\n",
        "test_accuracy = jnp.sum(y_hat_test == test_labels)/len(y_hat_test)\n",
        "\n",
        "print(f'Train accuracy:{train_accuracy}')\n",
        "print(f'Test accuracy:{test_accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQvBBHhYgkv0"
      },
      "source": [
        "As you can see, in just those few lines of code we have coded a logistic regression model that can classify the intent of a user given "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovi4HNFfgiUc"
      },
      "outputs": [],
      "source": [
        "def predict_new_text_lr(text):\n",
        "  embedding = get_embedding({'text':[text]})['embedding']\n",
        "  predicted_intent = model.predict(embedding)\n",
        "  converted_back_intent = le.inverse_transform([predicted_intent])[0]\n",
        "  print(f'Predicted intent for input: \"{text}\" is {int(converted_back_intent)}')\n",
        "  \n",
        "  index = jnp.where(train_labels==predicted_intent)[0]\n",
        "  print(f'Example training data from this class: {train_intents[index][\"text\"][0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJMvLsDEgmj_"
      },
      "outputs": [],
      "source": [
        "predict_new_text_lr('Can I please get a refund')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "## Conclusion\n",
        "**Summary:**\n",
        "\n",
        "Welcome to the end of the practical! Let us list everything you have accomplished:\n",
        "\n",
        "* Learned about attention and how it improved sequence-to-sequence modelling.\n",
        "* Implemented your very own implementation of MHA\n",
        "* Learned about the inner working of a transformer, building each block on your own\n",
        "* Combined all your work into a single transformer, which you trained to reverse a sequence in general classification and generative manner.\n",
        "* Learned about Huggingface and used a pre-trained Roberta model to train your own banking chatbot intent model.\n",
        "\n",
        "**Next Steps:** \n",
        "\n",
        "[Next steps for people who have completed the prac, like optional reading (e.g. blogs, papers, courses, youtube videos). This could also link to other pracs.]\n",
        "\n",
        "**Appendix:** \n",
        "\n",
        "[Anything (probably math heavy stuff) we don't have space for in the main practical sections.]\n",
        "\n",
        "**References:** \n",
        "\n",
        "[References for any content used in the notebook.]\n",
        "\n",
        "For other practicals from the Deep Learning Indaba, please visit [here](https://github.com/deep-learning-indaba/indaba-pracs-2022)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "## Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OIZvkhfRz9Jz"
      },
      "outputs": [],
      "source": [
        "#@title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "HTML(\n",
        "\"\"\"\n",
        "<iframe \n",
        "\tsrc=\"https://forms.gle/bvLLPX74LMGrFefo9\",\n",
        "  width=\"80%\" \n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6EqhIg1odqg0",
        "21sD_xKtd6TB",
        "B3e4e_vSP8qM"
      ],
      "name": "prac_sampling_updates.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
