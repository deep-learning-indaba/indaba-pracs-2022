{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# **Paying Attention to Transformers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0RWJRsNiFHX"
      },
      "source": [
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" width=\"40%\" />\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2022/blob/prac-transformers-and-attention/attention_and_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> [THIS SHOULD STILL CHANGE TO OUR PRAC]\n",
        "\n",
        "© Deep Learning Indaba 2022. Apache License 2.0.\n",
        "\n",
        "**Authors:**\n",
        "\n",
        "**Introduction:** \n",
        "\n",
        "The transformer architecture, introduced in Vaswani et al. 2017 .'s paper Attention is All You Need, has significantly impacted the deep learning field. It has arguably become the de-facto architecture for complex Natural Language Processing (NLP) tasks; and can outperform benchmarks in various domains, including computer vision and reinforcement learning.\n",
        "\n",
        "Transformers, as the title of the original paper implies, are almost entirely based on a concept known as attention. Attention allows models to \"focus\" on different parts of an input; while considering the entire context of the input versus an RNN, that operates on the data sequentially.\n",
        "\n",
        "In this practical, we will introduce attention in greater detail and build the entire transformer architecture block by block to see why it is such a robust and powerful architecture.\n",
        "\n",
        "**Topics:** \n",
        "\n",
        "Content: <font color='blue'>`Attention mechanisms, Transformers, NLP`</font>  \n",
        "Level: <font color='grey'>`Advanced`</font>\n",
        "\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "- Learn how different attention mechanisms can be implemented.\n",
        "- Learn and create the basic building blocks from scratch for the most common transformer architectures.\n",
        "- Learn how to train a sequence-sequence model.\n",
        "- Create and train a small GPT inspired model.\n",
        "- Learn how to use the [Hugging Face](https://huggingface.co/) library for quicker development cycles.\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "- Basic understanding of Jax and Haiku\n",
        "- Basic understanding linear algebra\n",
        "- RNN based sequence-sequence models\n",
        "- Token/Word embedding techniques\n",
        "\n",
        "**Outline:** \n",
        "\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "For this practical, you will need to use a GPU to speed up training. To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"GPU\" in the \"Hardware accelerator\" box."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "4boGA9rYdt9l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "072760e8-9f6f-4a26-8cf4-2b59ce56e0b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/deepmind/dm-haiku\n",
            "  Cloning https://github.com/deepmind/dm-haiku to /tmp/pip-req-build-l6u_ah7k\n",
            "  Running command git clone -q https://github.com/deepmind/dm-haiku /tmp/pip-req-build-l6u_ah7k\n",
            "Requirement already satisfied: flax in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from dm-haiku==0.0.8.dev0) (1.2.0)\n",
            "Requirement already satisfied: jmp>=0.0.2 in /usr/local/lib/python3.7/dist-packages (from dm-haiku==0.0.8.dev0) (0.0.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from dm-haiku==0.0.8.dev0) (1.21.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from dm-haiku==0.0.8.dev0) (0.8.10)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from dm-haiku==0.0.8.dev0) (4.1.1)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from flax) (6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax) (3.2.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax) (1.0.4)\n",
            "Requirement already satisfied: jax>=0.3.2 in /usr/local/lib/python3.7/dist-packages (from flax) (0.3.14)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (from flax) (0.1.3)\n",
            "Requirement already satisfied: rich~=11.1 in /usr/local/lib/python3.7/dist-packages (from flax) (11.2.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.7/dist-packages (from jax>=0.3.2->flax) (1.7.3)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.3.2->flax) (3.3.0)\n",
            "Requirement already satisfied: etils[epath] in /usr/local/lib/python3.7/dist-packages (from jax>=0.3.2->flax) (0.6.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich~=11.1->flax) (2.6.1)\n",
            "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from rich~=11.1->flax) (0.9.1)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from rich~=11.1->flax) (0.4.5)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax>=0.3.2->flax) (5.8.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.7/dist-packages (from etils[epath]->jax>=0.3.2->flax) (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->flax) (1.15.0)\n",
            "Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax->flax) (0.1.3)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax->flax) (0.3.14+cuda11.cudnn805)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.12.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax->flax) (0.1.7)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.37->optax->flax) (2.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==4.12.1 in /usr/local/lib/python3.7/dist-packages (4.12.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.3.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.1) (1.21.6)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.1) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.1) (4.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.1) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.1) (3.7.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.1) (0.10.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.1) (0.8.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.1) (0.0.53)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.1) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.1) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.12.1) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers==4.12.1) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.12.1) (3.0.9)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.5.0)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.1) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.1) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.12.1) (3.0.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.12.1) (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.1) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.12.1) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.7/dist-packages (0.5.3)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.21.6)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.3.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.7.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.2->seaborn) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn) (1.15.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn) (4.64.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.5.7)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.51.2)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Only CPU accelerator is connected.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package word2vec_sample to /root/nltk_data...\n",
            "[nltk_data]   Package word2vec_sample is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "## Install and import anything required. Capture hides the output from the cell. \n",
        "#@title Install and import required packages. (Run Cell)\n",
        "\n",
        "!pip install git+https://github.com/deepmind/dm-haiku flax\n",
        "!pip install transformers==4.12.1 datasets \n",
        "!pip install seaborn umap-learn\n",
        "\n",
        "import os\n",
        "import math\n",
        "import urllib.request\n",
        "\n",
        "# https://stackoverflow.com/questions/68340858/in-google-colab-is-there-a-programing-way-to-check-which-runtime-like-gpu-or-tpu\n",
        "if int(os.environ[\"COLAB_GPU\"]) > 0:\n",
        "  print(\"a GPU is connected.\")\n",
        "elif \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
        "  print(\"A TPU is connected.\")\n",
        "  import jax.tools.colab_tpu\n",
        "  jax.tools.colab_tpu.setup_tpu()\n",
        "else:\n",
        "  print(\"Only CPU accelerator is connected.\")\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "\n",
        "import haiku as hk\n",
        "import transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "import datasets\n",
        "\n",
        "import optax\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# download images used in notebook\n",
        "urllib.request.urlretrieve(\n",
        "  'https://images.unsplash.com/photo-1529778873920-4da4926a72c2?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8Y3V0ZSUyMGNhdHxlbnwwfHwwfHw%3D&w=1000&q=80',\n",
        "   \"cat.png\")\n",
        "\n",
        "\n",
        "import gensim\n",
        "from nltk.data import find\n",
        "import nltk\n",
        "nltk.download('word2vec_sample')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "-9X10jhocGaS"
      },
      "outputs": [],
      "source": [
        "#@title Helper Functions. (Run Cell)\n",
        "\n",
        "def plot_position_encodings(P, max_tokens, d_model):\n",
        "  plt.figure(figsize=(20,np.min([8,max_tokens])))\n",
        "  im = plt.imshow(P, aspect=\"auto\", cmap='Blues_r')\n",
        "  plt.colorbar(im, cmap='blue')\n",
        "\n",
        "  if d_model<=64:\n",
        "    plt.xticks(range(d_model))\n",
        "  if max_tokens <=32:\n",
        "    plt.yticks(range(max_tokens))\n",
        "  plt.xlabel('Embedding index')\n",
        "  plt.ylabel('Position index')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_image_patches(patches):\n",
        "  axes=[]\n",
        "  fig=plt.figure(figsize=(25,25))\n",
        "  for a in range(patches.shape[1]):\n",
        "      axes.append(fig.add_subplot(1, patches.shape[1], a+1) ) \n",
        "      plt.imshow(patches[0][a])\n",
        "  fig.tight_layout()    \n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_projected_embeddings(embeddings, labels):\n",
        "  import umap\n",
        "  import seaborn as sns\n",
        "\n",
        "  projected_embeddings = umap.UMAP().fit_transform(embeddings)\n",
        "\n",
        "  plt.figure(figsize=(15,8))\n",
        "  plt.title('Projected text embeddings')\n",
        "  sns.scatterplot(\n",
        "      x=projected_embeddings[:,0],\n",
        "      y=projected_embeddings[:,1],\n",
        "      hue=labels\n",
        "  )\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def get_word2vec_embedding(words):\n",
        "  word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
        "  model = gensim.models.KeyedVectors.load_word2vec_format(\n",
        "      word2vec_sample, \n",
        "      binary=False\n",
        "  )\n",
        "  embeddings = jnp.array(\n",
        "      [jnp.array(model.word_vec(word)) for word in words]\n",
        "  )\n",
        "  del model # free up space again\n",
        "  return embeddings\n",
        "\n",
        "def plot_attention_weight_matrix(weight_matrix, x_ticks, y_ticks):\n",
        "  plt.figure(figsize=(15,7))\n",
        "  ax = sns.heatmap(weight_matrix, cmap='Blues')\n",
        "  plt.xticks(np.arange(weight_matrix.shape[1])+0.5, x_ticks)\n",
        "  plt.yticks(np.arange(weight_matrix.shape[0])+0.5, y_ticks)\n",
        "  plt.title('Attention matrix')\n",
        "  plt.xlabel('Attend score')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rfwoGkW3cLuk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ac0d63a-493a-4af7-fa80-34179f4061da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: \n",
            "INFO:absl:Unable to initialize backend 'cuda': FAILED_PRECONDITION: No visible GPU devices.\n",
            "INFO:absl:Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: CUDA Host Interpreter\n",
            "INFO:absl:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num devices: 1\n",
            " Devices: [CpuDevice(id=0)]\n"
          ]
        }
      ],
      "source": [
        "#@title Check what device you are using (Run Cell)\n",
        "print(f\"Num devices: {jax.device_count()}\")\n",
        "print(f\" Devices: {jax.devices()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZUp8i37dFbU"
      },
      "source": [
        "## **Attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5MqDkvKiFHb"
      },
      "source": [
        "In order to understand the transformer architecture, one must understand the concept of attention and how it is implemented from a deep learning sense. The attention mechanism is based on how humans would interpret an image or read a sentence. \n",
        "\n",
        "Let us take the image of the dog in human clothes below (image and example [source](https://lilianweng.github.io/posts/2018-06-24-attention/)). When paying *attention* to the red blocks of pixels, we will say that the yellow block of pointy ears is something we expected (correlated) but that the grey blocks of human clothes are unexpected for us (uncorrelated), *based on what we have seen in the past*. \n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1iEU7Cph2D2PCXp3YEHj30-EndhHAeB5T\" alt=\"drawing\" width=\"450\"/>\n",
        "\n",
        "When we look at the red pixels, we tend to start paying *attention* to relevant pixels or *attend* to these pixels, almost fading out the snow in the background and human clothes. However, when we start looking at the background, we will fade out the dog pixels, as it does not contribute to the current task at hand"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The same thing happens when we read. In order to understand the entire sentence, we will learn to correlate and *attend to* certain words based on the context of the entire sentence. For instance, in the first sentence below, when looking at the word \"coding\", we pay more attention to the word \"Apple\" and \"computer\" because we know that when we speak about coding, \"Apple\" is actually referring to the company. \n",
        "\n",
        "However, in the second sentence, we realise we should not consider \" apple \" when looking at \"code\" because given the context of the rest of the sentence, we know that this apple is referring to an actual apple and not a computer.\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1j23kcfu_c3wINU6DUvxzMYNmp4alhHc9\" alt=\"drawing\" width=\"350\"/>\n",
        "\n",
        "In the following sections, we will dive deeper into the mechanisms that allow us to train our deep learning models to attend to input data, given the context of other input data."
      ],
      "metadata": {
        "id": "SAnfB8-JsdYA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii__Bc27epiJ"
      },
      "source": [
        "### Initial attention mechanisms - <font color='blue'>`Intermediate`</font>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first attention mechanisms were born for sequence-2-sequence models. These models were usually RNN encoder and decoder structures. The input sequence was processed sequentially by an RNN, encoding the sequence to a single context vector, which is then fed into another RNN that generates a new sequence. Below is an example of this ([source](https://lilianweng.github.io/posts/2018-06-24-attention/)).\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1FKfaArN1rsLjzVWaJGpMLEcxEshSLXd6\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "Due to their only being one context vector, it was often found that for longer input sequences, information gets lost due to the inability of the encoders to remember longer sequences. The attention mechanism introduced in [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) was proposed to solve this.\n"
      ],
      "metadata": {
        "id": "MWXE-pm0hhhX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, instead of relying on one static context vector, which is also only used once in the decoding process, let us provide information on the entire input sequence at every decoding step using a dynamic context vector. By doing this, the decoder can access a larger \"bank\" of memory and attend to the input's required information based on the output's current state. This is shown below.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1fB5KObXcKo5x35xlIDIcjHTq1q75ejIB\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "knytnRDG62Bb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using an attention mechanism, we can then attend to each input hidden state. This mechanism, usually, consists of two steps for each decoding step $t$: \n",
        "\n",
        "1. Calculate the score (importance) for each $h_n$, given $s_{t-1}$ and generate and attention weight, $w_{n}$. \n",
        "  - $\\text{score} = a(s_{t−1}, h_{n})$, where $a$ can be any differentiable function\n",
        "  - $w_{n} = \\frac{\\exp \\left\\{a\\left(s_{t-1}, h_{n}\\right)\\right\\}}{\\sum_{j=1}^{N} \\exp \\left\\{a\\left(s_{t-1}, h_{j}\\right)\\right\\}}$\n",
        "2. Generate the final context vector, $c_t$\n",
        "  - $c_t=\\sum_{n=1}^{N} w_n h_{n}$  \n",
        "\n",
        "The final state fed into the RNN to generate $s_{t+1}$, is given below, where $f$ can again be any combination method. \n",
        "\n",
        "$s_{t+1} = f\\left ( c_t, s_t \\right)$ \n",
        "\n",
        "In Bahdanau et al., 2015, $f$ was a learned feedforward layer taking in the concatenated vector $[c_t; s_t]$, with $a(s_{t−1}, h_{n})$ being the dot product. Next, let us build up this attention schema."
      ],
      "metadata": {
        "id": "tyBuSnVj4__6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In dot product attention, the score is given by\n",
        "\n",
        "$a=s_{t_1}^\\top h_n$\n",
        "\n",
        "**Code task**: Complet the dot product atttention function below."
      ],
      "metadata": {
        "id": "bP5aFhzZQ00Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dot_product_attention(hidden_states, previous_state):\n",
        "  scores = # FINISH ME \n",
        "  w_n = # FINSIH ME \n",
        "  c_t = jnp.matmul(w_n, hidden_states)\n",
        "\n",
        "  return w_n, c_t"
      ],
      "metadata": {
        "id": "1v3EhnW9THCW"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!') \n",
        "def dot_product_attention(hidden_states, previous_state):\n",
        "\n",
        "  scores = jnp.dot(hidden_states, previous_state.T)\n",
        "  w_n = jax.nn.softmax(scores)\n",
        "  c_t = jnp.matmul(w_n, hidden_states)\n",
        "\n",
        "  return w_n, c_t"
      ],
      "metadata": {
        "cellView": "form",
        "id": "APwzi2xmY8Qe"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to show how the dot product can produce attention weights that make sense, let us use pretrained [word2vec](https://jalammar.github.io/illustrated-word2vec/) embeddings. These word2vec embeddings are generated by an encoder network that was trained to generate similar embeddings for similar meaning words. \n",
        "\n",
        "Even though we are not processing something sequentially now that needs context, the attention matrix should still give us something that makes sense.\n"
      ],
      "metadata": {
        "id": "k_sdxOrgawet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['king', 'queen', 'royalty', 'food', 'apple', 'pear', 'computers']\n",
        "word_embeddings = get_word2vec_embedding(words)\n",
        "weights, _ = dot_product_attention(word_embeddings, word_embeddings)\n",
        "plot_attention_weight_matrix(weights, words, words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "kkGoImvQdX2H",
        "outputId": "cd2211f9-e261-4c54-ccf4-a5964117149c"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1080x504 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the matrix, assuming the function was implemented correctly, one will see that everything makes sense. The \"royal\" group of words all attend more to each other than the \"food\" words, which all attend to one another. We also see that \"computers\" have very low attend scores for all of them. \n",
        "\n",
        "**Group task:** \n",
        "  - Play with the word selections above. See if you can find word combinations that do not make sense. \n",
        "  - Ask your friend if they found examples."
      ],
      "metadata": {
        "id": "fUKsSUr3ql7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "For a larger range of attenion mechanishs, you can visit this [link](https://lilianweng.github.io/posts/2018-06-24-attention/#summary)."
      ],
      "metadata": {
        "id": "ENocLBe1YJMN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x22cVV00c9ZK"
      },
      "source": [
        "### Self-attention - <font color='blue'>`Intermediate`</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Talk about how we progressed to self-attention mechanisms and deep dive into scaled dot product attention. Not important yet to know how it fits into MHA."
      ],
      "metadata": {
        "id": "NeDD9MdEho7D"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvbKqrRniVDa"
      },
      "source": [
        "#### **Scaled dot product attention**\n",
        "\n",
        "[Deep dive here, with intuition for what query, keys and values can be, why we scale it etc.]\n",
        "\n",
        "[Focus on query, value and key matrices code in MHA section when we build the MHA block of code to be used in transformer models]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRO-fhwmodb6"
      },
      "source": [
        "**Code Task:** Can you code up scaled dot product attention?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNQ8Dgx2Wl8Q"
      },
      "outputs": [],
      "source": [
        "# we need to code up from scratch the function\n",
        "def scd_attention(query, key, value):\n",
        "\n",
        "  # allow then to code up the formula on their own\n",
        "  raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVETJPtRyXDO"
      },
      "outputs": [],
      "source": [
        "# run to test your function\n",
        "\n",
        "def check_scd_attention_function(scd_attention_function):\n",
        "  # we still need to impliment this\n",
        "  raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lDq7CAcAd49i"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!') \n",
        "\n",
        "def scd_attention(query, key, value):\n",
        "  emb_dim = query.shape[-1]\n",
        "  logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
        "  scaled_logits = logits/jnp.sqrt(emb_dim)\n",
        "  attention_weights = jax.nn.softmax(scaled_logits, axis=-1) \n",
        "  values = jnp.matmul(attention_weights, value)\n",
        "  return values, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd7xAVznfBQU"
      },
      "source": [
        "#### **Masked scaled dot attention** \n",
        "\n",
        "[Talk about how in some cases we are not allowed to see into the future or other inputs, so now we add in the masked attention]\n",
        "\n",
        "[Build upon the function above and add in mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQJtf6sCfBQY"
      },
      "source": [
        "**Code Task:** Try and implement the masking operation for your SCD function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LICrxTN5fBQZ"
      },
      "outputs": [],
      "source": [
        "# Code to be implemented during practical\n",
        "def scd_with_mask_attention(query, key, value, mask=None):\n",
        "  # CHANGE ME \n",
        "  raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iE1YNtLkfBQZ"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "\n",
        "def scd_with_mask_attention(query, key, value, mask=None):\n",
        "  emb_dim = query.shape[-1]\n",
        "  logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
        "\n",
        "  if mask is not None:\n",
        "    logits = jnp.where(mask, logits, -1e30) # same big negative value used in Haiku\n",
        "\n",
        "  scaled_logits = logits/jnp.sqrt(emb_dim)\n",
        "  attention_weights = jax.nn.softmax(scaled_logits, axis=-1) \n",
        "  attention = jnp.matmul(attention_weights, value)\n",
        "  return attention, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZe6TDuNfBQa"
      },
      "source": [
        "**Group Task:**\n",
        "\n",
        "- Play with the mask you provide to your function and tell your friend what you see.\n",
        "- Ask your friend if they think it's fair that we don't allow specific models to look into the future using self-attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SrT6swYgCm9"
      },
      "source": [
        "### Multihead Attention - <font color='green'>`Advanced`</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Introduce the concept of MHA and why it can be useful in a models.]\n",
        "\n",
        "[Talk about projecting Q,K,V to smaller dimensions to make training more efficient etc.]\n",
        "\n",
        "[Code up haiku multi-head attention module that will be used in future transformer models]"
      ],
      "metadata": {
        "id": "eILi2mwrhtCD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXZPL9qVci0I"
      },
      "source": [
        "**Code Task:** Code up a Haiku module that implements the entire multi-head attention mechanism. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px2JQqOnYs17"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(hk.Module):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      num_heads,\n",
        "      key_size,\n",
        "      model_size = None,\n",
        "      name = None,\n",
        "  ):\n",
        "    super().__init__(name=name)\n",
        "    self.num_heads = num_heads\n",
        "    self.key_size = key_size\n",
        "    self.model_size = model_size or key_size * num_heads\n",
        "\n",
        "  def __call__(self, query, key, value, mask = None):\n",
        "    raise NotImplementedError\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ozNp_pfhc12I"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "\n",
        "# TODO: ADD IN CORRECT ANSWER\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1gJBDviYsHX"
      },
      "source": [
        "**MHA vs other sequence methods (optional)**\n",
        "\n",
        "[Talk about how it differs and the complexity differs and other differences. Like what was done in this [practical](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/JAX/tutorial6/Transformers_and_MHAttention.html)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WILOYJH4gCnD"
      },
      "source": [
        "### Section Quiz \n",
        "\n",
        "[Test knowledge on all the previous material of attention]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N5iFeOKOgCnE"
      },
      "outputs": [],
      "source": [
        "#@title Generate Quiz Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "HTML(\n",
        "\"\"\"\n",
        "<iframe \n",
        "\tsrc=\"https://forms.gle/zbJoTSz3nfYq1VrY6\",\n",
        "  width=\"80%\" \n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9NW58_3hAg2"
      },
      "source": [
        "## **Transformers**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjEsrhhVhZ2J"
      },
      "source": [
        "### Processing the input data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq0kPkVphbFG"
      },
      "source": [
        "#### Tokenisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd4hPASBiEGe"
      },
      "source": [
        "Before data can be fed into transformers, it has to be transformed into an acceptable format, in other words, a sequence of tokens. Below, we briefly discuss how we can transform text and vision data into tokens that transformers can process [source](https://huggingface.co/docs/transformers/preprocessing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8GylOMmBW4n"
      },
      "source": [
        "##### **Text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS_jLXJw0kdN"
      },
      "source": [
        "\n",
        "Transformers can not handle raw strings of text. So to process text, the text is first split up into tokens, where after these tokens are indexed and each token is assigned an embedding of size $d_{model}$. These embeddings can be learned during training or can come from a pretrained vocablay of embeddings. This new sequence of token embeddings is then fed into the transformer architecture. This idea is visualised below. \n",
        "\n",
        "\\\\\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=16euh4LADP_mcXywFwKKY3QQQkVplepiI\" alt=\"drawing\" width=\"350\"/>\n",
        "\n",
        "\n",
        "These token IDs are typically predicted when a model generates text, fills in missing words, etc.\n",
        "\n",
        "This process of splitting up text into tokens and assigning an ID to each token is called [tokenisation](https://huggingface.co/docs/transformers/tokenizer_summary). There are various ways to tokenise text, with some methods being trained directly from the data. When using pre-trained transformers, it is crucial to use the same tokeniser that was used to train the model. The previous link has in-depth descriptions of many widely known techniques.\n",
        "\n",
        "Below we show how the [Bert](https://arxiv.org/abs/1810.04805) models tokeniser tokenises a sentence. We use [Hugging Face](https://huggingface.co/) for this part, where we have a deep dive into Hugging Face later in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0C-3v0y0iZe"
      },
      "outputs": [],
      "source": [
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "encoded_input = bert_tokenizer(\"The practical is so much fun\")\n",
        "print(f\"Token IDs: {encoded_input['input_ids']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-408xpj794l"
      },
      "source": [
        "Here we can see that the tokeniser returns the IDs for each token, as shown in the figure. But counting the number of IDs, we see that it is larger than the number of words in the sentence. Let's print the tokens associated with each ID.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dYMGIgi8p-q"
      },
      "outputs": [],
      "source": [
        "print(f\"Tokens: {bert_tokenizer.decode(encoded_input['input_ids'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phjOR7ftAVt_"
      },
      "source": [
        "We can see the tokeniser attaches new tokens, `[CLS]` and `[SEP]`, to the start and end of the sequence. This is a Bert specific requirement for training and inference. Adding special tokens is a very common thing to do. \n",
        "\n",
        "For instance, to pretrain specific transformers, they perform what is known as masked prediction. For this, random tokens in a sequence are replaced by the `[MASK]` token, and the model is trained to predict the correct token ID for the token replaced with that token. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syL_uNAMVHQB"
      },
      "source": [
        "**Group task**:\n",
        "\n",
        "- Discuss with a friend what you think the current issue is when feeding these raw tokens into a transformer architecture? Think about the difference in meaning between a sentence and that same sentence where the word orders are random.\n",
        "- How would you fix that issue? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jWreoeTBUCD"
      },
      "source": [
        "##### **Images** (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcyHr8a_BdaT"
      },
      "source": [
        "As mentioned, transformers are versatile and can be applied to roughly any data which can be converted into a sequence of tokens.\n",
        "\n",
        "For example, to use the transformers encoder architecture with images, one can split an image into different patches, flatten these image patches and project each image patch into a fixed-sized embedding using any projection method. By doing this, the image has been converted into a sequence of image tokens, and the transformer will be able to process the data. \n",
        "\n",
        "This process is shown in the image below. \n",
        "\n",
        "\\\\\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ERF0f3Y_0wNb4kQ07xMYUysqNYIPkQMD\" alt=\"drawing\" width=\"550\"/>\n",
        "\n",
        "\n",
        "**Code task (OPTIONAL and ADVANCED):** Write a function that can take in a batch of images with shape (Batch, Height, Widht, Channels) and divide it into equal-sized patches. You can use the output of `image_to_patch` and `plot_image_patches` functions to visualise and test your function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htWcVGlDHcba"
      },
      "outputs": [],
      "source": [
        "def image_to_patch(image, patch_size):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        x - array of shape [B, H, W, C]\n",
        "        patch_size - Number of pixels per dimension of the patches (integer)\n",
        "    \"\"\"\n",
        "    B, H, W, C = image.shape # HINT: You will need these\n",
        " \n",
        "    image  # FINISH ME\n",
        "\n",
        "    return image_patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBXMQInfTuJK"
      },
      "outputs": [],
      "source": [
        "# do not change these lines, only run them to test your function\n",
        "print('Original image:')\n",
        "image = np.array(Image.open(\"cat.png\"))\n",
        "plt.imshow(image)\n",
        "plt.show()\n",
        "\n",
        "print(\"Image broken into patches\")\n",
        "img = jnp.array(image)\n",
        "img = jax.image.resize(img, (512,512,3), \"nearest\")\n",
        "img = jnp.expand_dims(img, 0)\n",
        "patches = image_to_patch(img, 256)\n",
        "plot_image_patches(patches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oqFn3peRTqm4"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "\n",
        "def image_to_patch(image, patch_size):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        x - array of shape [B, H, W, C]\n",
        "        patch_size - Number of pixels per dimension of the patches (integer)\n",
        "    \"\"\"\n",
        "    B, H, W, C = image.shape\n",
        "    image = image.reshape(B, H//patch_size, patch_size, W//patch_size, patch_size, C)\n",
        "    image = image.transpose(0, 1, 3, 2, 4, 5)    # [B, H', W', p_H, p_W, C]\n",
        "    image_patches = image.reshape(B, -1, *image.shape[3:])   # [B, H'*W', p_H, p_W, C]\n",
        "    return image_patches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln7esAMHhdaz"
      },
      "source": [
        "\n",
        "#### Positional encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8MrN6YJXUM_"
      },
      "source": [
        "In most domains where a transformer can be utilised, there is an underlying order to the tokens produced, be it the order of words in a sentence, the location from which patches are taken in an image or even the steps taken in an RL environment. This order is very important in all cases; just imagine you interpret the sentence \"I have to read this book.\" as \"I have this book to read.\". Both sentences contain the exact same words, yet they have completely different meanings based on the order. \n",
        "\n",
        "As both the encoder and the decoder blocks process all tokens in parrel, the order of tokens is lost in these calculations. To cope with this, the sequence order has to be injected into the tokens directly. This can be done by adding *positional encodings* to the tokens at the start of the encoder and decoder blocks (though some of the latest techniques add positional information in the attention blocks). An example of how positional encodings alter the tokens are shown below.\n",
        "\n",
        "\\\\\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1eSgnVN2hnEsrjdHygDGwk1kxEi8-dcFo\" alt=\"drawing\" width=\"450\"/>\n",
        "\n",
        "Ideally, these encodings should have these characteristics ([source](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)):\n",
        "* Each time-step should have a unique value\n",
        "* The distance between time steps should stay constant.\n",
        "* It should be able to generalise to longer sequences than seen during training.\n",
        "* It must be deterministic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WHx-9m9h5JA"
      },
      "source": [
        "\n",
        "##### **Sine and cosine functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrkqm8gC90GU"
      },
      "source": [
        "\n",
        "In Attention is All you Need, the authors used a method that can satisfy all these requirements. This involves summing a combination of sine and cosine waves at different frequencies, with the formula for a position encoding at position $D$ shown below, where $i$ is the embedding index and $d_m$ is the token embedding size. \n",
        "\n",
        "\\\\\n",
        "\n",
        "$P_{D}= \\begin{cases}\\sin \\left(\\frac{D}{10000^{i/d_{m}}}\\right), & \\text { if } i \\bmod 2=0 \\\\ \\cos \\left(\\frac{D}{10000^{((i-1)/d_{m}}}\\right), & \\text { otherwise } \\end{cases}$\n",
        "\n",
        "\\\\ \n",
        "\n",
        "Assuming our model as $d_m=8$, the position embedding will look like this:\n",
        "\n",
        "$P_{D}=\\left[\\begin{array}{c}\\sin \\left(\\frac{D}{10000^{0/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{0/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{2/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{2/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{4/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{4/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{8/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{8/8}}\\right)\\end{array}\\right]$\n",
        "\n",
        "\\\\\n",
        "\n",
        "Let's first create a function that can return these encodings to understand why this will work. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKobhuyj9-74"
      },
      "outputs": [],
      "source": [
        "def return_frequency_pe_matrix(token_sequence_length, token_embedding):\n",
        "\n",
        "  assert token_embedding%2==0, \"token_embedding should be divisible by two\"\n",
        "\n",
        "  P = jnp.zeros((token_sequence_length, token_embedding))\n",
        "  positions = jnp.arange(0, token_sequence_length)[:, jnp.newaxis]\n",
        "\n",
        "  i = jnp.arange(0, token_embedding, 2)\n",
        "  frequency_steps = jnp.exp(i * (-math.log(10000.0) / token_embedding))\n",
        "  frequencies = positions*frequency_steps\n",
        "\n",
        "  P = P.at[:, 0::2].set(jnp.sin(frequencies))\n",
        "  P = P.at[:, 1::2].set(jnp.cos(frequencies))\n",
        "\n",
        "  return P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4_9-Oh4yZuF"
      },
      "outputs": [],
      "source": [
        "token_sequence_length = 50 # Number of tokens the model will need to process\n",
        "token_embedding = 10000 # token embedding (and positional encoding) dimensions, ensure it is divisible by two\n",
        "P = return_frequency_pe_matrix(token_sequence_length, token_embedding)\n",
        "plot_position_encodings(P, token_sequence_length, token_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw8nOF2Ra93B"
      },
      "source": [
        "Looking at the graph above, we can see that for each position index, their is a unique pattern forming, where each position index will always have the same encoding. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHZXceUVNWy9"
      },
      "source": [
        "**Group task**:\n",
        "\n",
        "- Discuss with your friend why we are seeing that specific pattern when `token_sequence_length` is 1000, and `token_embedding` is 768.\n",
        "- You can try playing around with smaller values for `token_sequence_length` and  `token_embedding` to get a better intuition for the above discussion.\n",
        "- Ask your friend why they think the 10000 constant is used in the functions above. \n",
        "- Make `token_sequence_length` to be 50 and `token_embedding` something large, like 10000. What do you notice? Is a large token embedding always needed?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFiRpEaGd6TB"
      },
      "source": [
        "**Math task (optional):** Notice in our function that we do not directly implement the equation we describe above for numerical stability when calculating the frequency steps. See if you can derive by hand how we got to this new equation. Hint: Think about log rules.\n",
        "\n",
        "Original equation:\n",
        "\n",
        "$\\text{frequencies} = \\frac{D}{x^{i/d_{m}}}$\n",
        "\n",
        "Code equation:\n",
        "\n",
        "$\\text{frequencies} = D\\left(\\text{exp} \\left( \\frac{-i\\log(x)}{d_m} \\right)\\right)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21sD_xKtd6TB"
      },
      "source": [
        "###### **Answer (but first try yourself)**\n",
        "\n",
        "Expand this section to see the answer. \n",
        "\n",
        "What we did here is an essential aspect of computer science, specifically in machine learning, when big numbers appear. We usually rewrite equations to utilise logs, such that multiplications become additions and large numbers get suppressed. This ensures better numerical stability.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQPI_Sqnd6TB"
      },
      "source": [
        "\\begin{align}\n",
        "    \\frac{𝐷}{x^{𝑖/𝑑𝑚}} &= D\\left( x^{-i/dm}\\right) \\\\\n",
        "    &= D\\left( \\text{exp}\\left(\\log{x^{-i/dm}}\\right)\\right) \\\\\n",
        "    &= D\\left( \\text{exp}\\left(\\frac{-i}{dm}\\log{x}\\right)\\right) \\\\\n",
        "    &= D\\left( \\text{exp}\\left(\\frac{-i\\log{x}}{dm}\\right)\\right) \\\\\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GeFt_xwh722"
      },
      "source": [
        "##### **Learned positional embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnTNW1kNyZ75"
      },
      "source": [
        "\n",
        "Another method which is commonly used is to allow the model to learn the positional information required. In this method, the model learns a lookup table, where each index in the table refers to a positional embedding. Whereas the previous method allowed for an infinite amount of tokens, this method caps the maximum token sequence length as the lookup table has to be set beforehand. \n",
        "\n",
        "Using Haiku, this method is relatively straightforward as we can use [Haiku's embed function](https://dm-haiku.readthedocs.io/en/latest/api.html#embed) or [Haiku's get parameter function](https://dm-haiku.readthedocs.io/en/latest/api.html#get-parameter)\n",
        "\n",
        "**Code Task:** Try and implement a lookup table, which can be learned, for using Haikus get parameter function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXbQ_-RnXaNY"
      },
      "outputs": [],
      "source": [
        "class PositionEmbeddingsLookup(hk.Module):\n",
        "    \"\"\"\n",
        "    A position embedding of shape [max_sequence_len, d_model]\n",
        "    \"\"\"\n",
        "    def __init__(self, max_sequence_len, d_model):\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_sequence_len = max_sequence_len\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def __call__(self, sequence):\n",
        "        \n",
        "        assert sequence.shape[0]<=self.max_sequence_len, f\"Sequence to long, max lenght={self.max_sequence_len}\"\n",
        "\n",
        "        lookup_table = hk.get_parameter( \n",
        "            name=\"position_embedding\",\n",
        "            shape=, # FILL ME IN\n",
        "            init=jnp.zeros\n",
        "        )\n",
        "\n",
        "        return lookup_table[:sequence.shape[0], :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1_WpVc0Y3Hk"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "\n",
        "class PositionEmbeddingsLookup(hk.Module):\n",
        "    \"\"\"\n",
        "    A position embedding of shape [max_sequence_len, d_model]\n",
        "    \"\"\"\n",
        "    def __init__(self, max_sequence_len, d_model):\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_sequence_len = max_sequence_len\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def __call__(self, sequence):\n",
        "        \n",
        "        assert sequence.shape[0]<=self.max_sequence_len, f\"Sequence to long, max lenght={self.max_sequence_len}\"\n",
        "\n",
        "        lookup_table = hk.get_parameter(\n",
        "            name=\"position_embedding\", \n",
        "            shape=(self.max_sequence_len, self.d_model),\n",
        "            init=jnp.zeros\n",
        "        )\n",
        "\n",
        "        return lookup_table[:sequence.shape[0], :]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE6hEihPhAhK"
      },
      "source": [
        "### Section Quiz \n",
        "\n",
        "Optional end of section quiz. Below is an example of an assessment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L03B3HKwhAhK"
      },
      "outputs": [],
      "source": [
        "#@title Generate Quiz Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "HTML(\n",
        "\"\"\"\n",
        "<iframe \n",
        "\tsrc=\"https://forms.gle/zbJoTSz3nfYq1VrY6\",\n",
        "  width=\"80%\" \n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rectkTs9iFHg"
      },
      "source": [
        "## **Hugging Face**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFBw8kRx-4Mk"
      },
      "source": [
        "\n",
        "\n",
        "<img src=\"data:image/png;base64,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\" width=\"10%\" />\n",
        "\n",
        "\n",
        "[Hugging Face](https://huggingface.co/) is a startup founded in 2016 and, in their own words: \"are on a mission to democratize good machine learning, one commit at a time.\"\n",
        "\n",
        "They have developed various open-source packages and allow users to easily interact with a large corpus of pretrained transformer models (across all modalities) and datasets to train or fine-tune pre-trained transformers.\n",
        "\n",
        "Their software is used widely in industry and research. The following sections show how one can interact with their various features, access SOTA models, and train your own model.\n",
        "\n",
        "This is an optional section for the practical. Still, working through it in the practical or afterwards is highly recommended. Understanding Hugging Face will allow you to build a very quick proof of concept system to test out various hypotheses, whereafter, the system you build can simply be used, or a new system can be developed with the knowledge that the original idea has merit. \n",
        "\n",
        "It should also be noted that various languages are still severely under-resourced, even in this ecosystem. See it as an opportunity also to see where the gaps are and how we as a community can reduce this gap. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Datasets Package - <font color='blue'>`Beginner`</font>"
      ],
      "metadata": {
        "id": "Wq_pX3XTefu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Along with all the models being availible, datasets are also hosted on the hugginface hub. One can visit the [dataset hub](https://huggingface.co/datasets), and browse for interesting datasets and very easilly access it through the [datasets](https://github.com/huggingface/datasets) package.\n",
        "\n",
        "Lets say for instance we want to build an text intent classification model. What we do then is to go to the link above, and use the search tags to find a dataset that seems like a good fit for us. Doing this, we find the [`banking77`](https://huggingface.co/datasets/banking77) dataset. Below we then load in this dataset.\n"
      ],
      "metadata": {
        "id": "Tn-vfzXyehPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# just for notebook cleanliness\n",
        "from datasets.utils.logging import set_verbosity_error\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"banking77\")\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "4StYbstjeitO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see that the API returns a variable of DatasetDict type, which containins our dataset that has split into two datasets, i.e. train and test splits. We also see that each of these datasets contain `text` and `label` features. \n",
        "\n",
        "Lets investigate how the data looks."
      ],
      "metadata": {
        "id": "pMh4aox5eh8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train']"
      ],
      "metadata": {
        "id": "9qFr15bkemIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that our dataset splits are of type Dataset."
      ],
      "metadata": {
        "id": "EzyH1lNWen3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_intents = dataset['train']\n",
        "test_intents = dataset['test']\n",
        "\n",
        "print('Text: ', train_intents['text'][0])\n",
        "print('Label: ', train_intents['label'][0])"
      ],
      "metadata": {
        "id": "zEQFsm0HepIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes we want to work with only a subset of the dataset and we thus need to filter out the rest. Luckilly, the datasets have an easy to use filter functionality. Below we filter to only see text which relates to label 11. "
      ],
      "metadata": {
        "id": "N7--kHR7eqej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we are only the first 5 text samples\n",
        "train_intents.filter(lambda data: data['label']==1)"
      ],
      "metadata": {
        "id": "ehbEghPSetvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also apply a function to each data item by mapping the function to each \"row\" in the dataset."
      ],
      "metadata": {
        "id": "IvcEMvEDevSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating small dataset as example of using select\n",
        "example_dataset = train_intents.select(range(10))\n",
        "\n",
        "# printing first character of a text example\n",
        "example_dataset.map(lambda example: print(example['text'][0]));"
      ],
      "metadata": {
        "id": "q-0tUb6sewij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the map function, we can also add a new collumn."
      ],
      "metadata": {
        "id": "xgt4MFR4ex14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_sentence_len(example):\n",
        "    example['lenght'] = len(example['text'])\n",
        "    return example\n",
        "\n",
        "# add new column\n",
        "example_dataset = example_dataset.map(lambda example: add_sentence_len(example))\n",
        "\n",
        "print(example_dataset)\n",
        "print('New data:', example_dataset['lenght'])"
      ],
      "metadata": {
        "id": "6gtdQeKVezT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code Task**: \n",
        "- We want to build a classifier out of this. We need to investigate the distrubution of classes to see if our dataset is balanced or not. Write code that generates a dictionary containing the count of each class for both the train and test dataset.\n",
        "- Filter out classes with less than 150 classes in the training set, and ensure only the remaining classes are in the test test. "
      ],
      "metadata": {
        "id": "nZ_qP_Jqe0r5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# task 1\n",
        "unique_labels = # FINISH ME\n",
        "total_unique_labels = # FINISH ME\n",
        "\n",
        "train_counts = # FINISH ME\n",
        "test_counts = # FINISH ME\n",
        "\n",
        "# task 2\n",
        "passed_labels = [label for label, count in train_counts.items() if count>=150]\n",
        "train_intents = # FINISH ME\n",
        "test_intents = # FINISH ME"
      ],
      "metadata": {
        "id": "42qddlWve2Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "\n",
        "# # task 1\n",
        "unique_labels = np.unique(train_intents['label'])\n",
        "total_unique_labels = len(unique_labels)\n",
        "\n",
        "train_counts = {label:sum(label==train_intents['label']) for label in unique_labels}\n",
        "test_counts = {label:sum(label==train_intents['label']) for label in unique_labels}\n",
        "\n",
        "# task 2\n",
        "passed_labels = [label for label, count in train_counts.items() if count>=150]\n",
        "train_intents = train_intents.filter(lambda data: data['label'] in passed_labels)\n",
        "test_intents = test_intents.filter(lambda data: data['label'] in passed_labels)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ABrM5iAIe4YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Other modalities**\n",
        "\n",
        "Text is not the only data that can be accessed, audio and image data can be accessed just as easilly. "
      ],
      "metadata": {
        "id": "hz4U0IZ8e34k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "dataset = load_dataset(\"cgarciae/cartoonset\")\n",
        "Image.open(BytesIO(dataset['train'][\"img_bytes\"][231]))\n",
        "\n",
        "# # free up a bit of space\n",
        "del dataset"
      ],
      "metadata": {
        "id": "aPCHECJQiFU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformers Package"
      ],
      "metadata": {
        "id": "iL4j1JZse8wi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a way in accessing data, lets shift our intention to accessin the hundreds of pretrained transformers."
      ],
      "metadata": {
        "id": "4bKe1WDEe9eX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pipeline - <font color='blue'>`Beginner`</font>"
      ],
      "metadata": {
        "id": "Po9R6qiSe_HT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The easiest method to access a vast range of pre-trained models and use tasks is through the `pipeline` API.\n",
        "\n",
        "Pipelines group together a pretrained model found on their models hub with the preprocessing that was used during that model's training. To use the pipeline, one must import it from the [transformers](https://github.com/huggingface/transformers) library and specify the task and model you want.\n",
        "\n",
        "For a list of models, visit [this](https://huggingface.co/models) page that allows you to search through all models currently on the hub."
      ],
      "metadata": {
        "id": "tcK8-KpZfA7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# When calling the function for the first time, the model, and its tokenizer, will be automatically downloaded\n",
        "sentiment_model = pipeline(task='sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
        "print(sentiment_model(\"I love this practical!\"))\n",
        "print(sentiment_model(\"I hate this practical!\"))\n",
        "\n",
        "# passing more than one sentence\n",
        "sentence_batch = [\n",
        "  'This is much quicker and easier to build a POC with than training everything from scratch',\n",
        "  'It really hurts when I stub my toe',\n",
        "  'I want to get ice cream'\n",
        "]\n",
        "print('\\nBatch output:')\n",
        "sentiment_model(sentence_batch)"
      ],
      "metadata": {
        "id": "ZjTFOuylfDKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the first sentence we process in our batch of sentences is predicted to be Negative, with a relative low score of $0.51$, even though we feel this should be more neutral? The low score indicates this and we can interepet that when scores are low the actual label was meant to be Neutral, but this model was trained to do a binary prediction only.\n",
        "\n",
        "This model you just used is a Distellbert model, which was trained on *8 16GB V100s for 90 hours*, and you could use it as quickly as that.\n",
        "\n",
        "**Code Task:** Apply the zero shot model to all of our test intent chatbot examples, extracting the predicted senitment label into a new collumn, using the map function."
      ],
      "metadata": {
        "id": "qNYlFKWNfCL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentiment(example):\n",
        "  # FINISH ME\n",
        "  return sentiment\n",
        "\n",
        "train_intents = # FINISH ME\n",
        "test_intents = # FINISH ME"
      ],
      "metadata": {
        "id": "00lgxVY8fKPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "\n",
        "def get_sentiment(example):\n",
        "  example['sentiment'] = sentiment_model(example['text'])[0]['label']\n",
        "  return example\n",
        "\n",
        "test_intents = test_intents.map(lambda example: get_sentiment(example))"
      ],
      "metadata": {
        "id": "KY_pOGR6fLYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title('Count of positive versus negative text inputs')\n",
        "sns.countplot(test_intents['sentiment']);"
      ],
      "metadata": {
        "id": "txcTUTbUfMnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Group code task (optional depending on time)**: (Hint: use the tags when searching the [model hub](https://huggingface.co/models))\n",
        "- Search for other pipeline tasks available, and dicuss with your friend what you did and found. ([Hint](https://huggingface.co/docs/transformers/v4.20.1/en/main_classes/pipelines#pipelines))\n",
        "- Play with different language models and see how they perform"
      ],
      "metadata": {
        "id": "Tg3ti5iEfGIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "your_pipeline = pipeline(\n",
        "    task='text-generation', # CHANGE ME TO OTHER STUFF\n",
        "    model='gpt2' # CHANGE ME AS WELL\n",
        ")\n",
        "text = 'I like ice-cream and '\n",
        "your_pipeline(text)"
      ],
      "metadata": {
        "id": "jeMfN56EfS7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# freeing up memory for future tasks\n",
        "del your_pipeline\n",
        "del sentiment_model"
      ],
      "metadata": {
        "id": "DSu5FY7CfRJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training a chatbot intent model  - <font color='blue'>`Advanced`</font>"
      ],
      "metadata": {
        "id": "QSJ-Fh49fU-9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want more controll than the pipeline API provides you, you can also use the predefined model classes. \n",
        "\n",
        "To showcase this, we will be training a custom model on top of a large transformer.\n",
        "\n",
        "To set the scene, lets say for instance that we want to train an intent model that can be used along with a chatbot. This intent model will be responsible to predict the true underlying intent found within the text. \n",
        "\n",
        "Your imaganiry friend has built an intent model using TF-IDF techniques, but you think that you can use transformers for this task and that it will perform better. \n",
        "\n",
        "You have heard about the famous [DistilBERT](https://arxiv.org/pdf/1910.01108.pdf) model that was trained to extract text features, and yout think this model can be a perfect fit to extract features for your intent model, as it is an encoder only transformer architecture that produces strong token representations.\n",
        "\n",
        "To start your training process, you have two steps to follow:\n",
        "\n",
        "* Get the tokenizer\n",
        "* Get the model"
      ],
      "metadata": {
        "id": "LHzc2lpOfWVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### **Getting the tokenizer**\n",
        "\n",
        "A tokenizer is responsible for preprocessing text into a format that is understandable to the model. It is very important to use the same tokenizer as the model you will be finetuning.  "
      ],
      "metadata": {
        "id": "-MBCwSG5i38v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we want the Distilbert model, thus we import the correct tokenizer the model train\n",
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "# we specify a specif distilbert \n",
        "model_name = 'distilbert-base-uncased'\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "print('Tokenizer output:')\n",
        "output = tokenizer('This is example text')\n",
        "print(output)\n",
        "\n",
        "print('Tokens converted back to string')\n",
        "print(tokenizer.decode(output['input_ids']))"
      ],
      "metadata": {
        "id": "f7ufSoUxi4uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the output from the above, we that the special tokens are the `[CLS]` and `[SEP]` tokens. This is important to note, as we will be using the final output of the model for the `[CLS]` token when predicting the intent. \n",
        "\n",
        "This is a very common thing to do, where the token that indicates the start of the sentence is used when making predictions on the sentence. Seeing as our system is built in Jax, we will need to tell the tokenizer to return the data in the correct format.\n"
      ],
      "metadata": {
        "id": "zW_VglCQfXfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_batch = tokenizer(\n",
        "    [\"We are very happy to show you the 🤗 Transformers library.\", \"We hope you don't hate it.\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"jax\",\n",
        ")\n",
        "print(token_batch)"
      ],
      "metadata": {
        "id": "eR0yqg-mfZjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seeing as we want to use these tokens throughout our training process and for processing the embeddings, lets map the tokenizer output to new collumn for our train and test splits."
      ],
      "metadata": {
        "id": "pqBOZ4YLfa0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Getting the transformer and generating embeddings**"
      ],
      "metadata": {
        "id": "hyDoAqXvfc_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No that we can encode our text quickly into data that our transformer can processes, let gather and download the pretrained transformer model from the hub and generate representations for each text example.\n",
        "\n",
        "As was mentioned above, we are interested for now only in the `[CLS]` token embedding. One can ofcourse look at averaging over all token (being sure to only use tokens who are note masked) and see how it compares. We leave this as an excersise for the reader to compare how it changes the performance of the model.\n"
      ],
      "metadata": {
        "id": "sxSkOij_fexe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import FlaxDistilBertModel\n",
        "\n",
        "# we use FlaxDistillBertModel because we are in the JAX world\n",
        "distell_bert_model = FlaxDistilBertModel.from_pretrained(model_name)\n",
        "\n",
        "tokens = tokenizer([train_intents[0]['text']])\n",
        "embeddings = distell_bert_model(**tokens)[0][:,0]\n",
        "embeddings.shape"
      ],
      "metadata": {
        "id": "DFhSfCOKfgA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying one by one is extremely slow, so lets rather infer in batches, using the `batch` flag in the map function. It is important here that the function that is being mapped works with batches and return the data in the correct format, i.e a dictionary with the new column name."
      ],
      "metadata": {
        "id": "GKgHz1_tfh10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embedding(batch):\n",
        "\n",
        "  text = batch['text']\n",
        "  tokens = tokenizer(\n",
        "      text,\n",
        "      padding=True,\n",
        "      truncation=True,\n",
        "      max_length=50,\n",
        "      return_tensors=\"jax\",\n",
        "  )\n",
        "\n",
        "  cls_embeddings = np.array(distell_bert_model(**tokens)[0][:,0])\n",
        "  return {'embedding':cls_embeddings}\n",
        "\n",
        "train_intents = train_intents.map(lambda batch: get_embedding(batch), batched=True)\n",
        "test_intents = test_intents.map(lambda batch: get_embedding(batch), batched=True)"
      ],
      "metadata": {
        "id": "hPkm_JdsfjIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see whether these embeddings are in anyway usefull, lets plot a few projected embeddings and their labels and see how it looks. "
      ],
      "metadata": {
        "id": "dDcAZzm3fkfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sampling to get a clear picture with less data.\n",
        "sample_labels = [5,11,20,28,34,45,51,76]\n",
        "plot_data = train_intents.filter(lambda data: data['label'] in sample_labels)\n",
        "plot_projected_embeddings(plot_data['embedding'], [str(l) for l in plot_data['label']])"
      ],
      "metadata": {
        "id": "bbCy-UK7flz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see there are clear clusters forming, but there is defnitely still some work that can be done here.\n",
        "\n",
        "Lets thus train a non linear model on this data to try and find something that seperate this intents.\n",
        "\n",
        "We have a choice really of what we want to do, given data our new dataset can just be interpeted as a database. We can either train another neural netwok, or we can train anything ranging from a logistic regression model to a XGBoost model.\n",
        "\n",
        "Lets start by training a neural network, as we are at the deep learning indaba ;)"
      ],
      "metadata": {
        "id": "Z8agDZwufoFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting data into more usable state for all methods\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder() # labels should be from 0 - N\n",
        "\n",
        "train_embeddings = train_intents['embedding']\n",
        "train_labels = jnp.array(le.fit_transform(train_intents['label']))\n",
        "test_embeddings = test_intents['embedding']\n",
        "test_labels = jnp.array(le.transform(test_intents['label']))"
      ],
      "metadata": {
        "id": "KohN7yfbfmOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load batches of embedding, we feed our extracted embeddings and labels into tensorflow datasets."
      ],
      "metadata": {
        "id": "7LxKYDyYfr3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# creating tensorflow dataset loaders\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_embeddings,train_labels))\n",
        "train_ds = train_ds.\\\n",
        "  shuffle(buffer_size=len(train_embeddings),reshuffle_each_iteration=True).\\\n",
        "  batch(64)\n",
        "\n",
        "# we do not want to shuffle test data\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_embeddings,test_labels))\n",
        "test_ds = test_ds.batch(64)"
      ],
      "metadata": {
        "id": "og_F4Hyrfquy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Training intent model**"
      ],
      "metadata": {
        "id": "TW5-9GbNf1nX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Even though the methods below only show us updating the weights or parameters of downstream models, there is now reason that one can not finetune the entire Distilbert model on the new dataset and task. This will just require much more compute and in many cases the extra costs are not linearly correlated with improved performance. This is why in this practical, why only train downstream models utilising the pretrained embeddings."
      ],
      "metadata": {
        "id": "F-UfWogVf3zc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### MLP"
      ],
      "metadata": {
        "id": "ekey34T1f5jK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our intent model will be 2 layer MLP. \n",
        "\n",
        "**Code task:** Finish the 2 layer MLP Haiku Module."
      ],
      "metadata": {
        "id": "mFetHB7tf641"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build a training model\n",
        "\n",
        "class IntentClassifier(hk.Module):\n",
        "  \"\"\"A MLP which predicts intent from transformer embeddings\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self._init_scale = .5\n",
        "    self.number_classes = #FILL ME IN\n",
        "\n",
        "  def __call__(self, embeddings):\n",
        "    embedding_size = # FILL ME IN\n",
        "    initializer = hk.initializers.VarianceScaling(self._init_scale)\n",
        "    projection_layer = hk.Linear(embedding_size, w_init=initializer)\n",
        "    classification_layer = # FILL ME IN\n",
        "\n",
        "    projections = jax.nn.relu(projection_layer(embeddings))\n",
        "\n",
        "    logits = # FILL ME IN\n",
        "    return logits"
      ],
      "metadata": {
        "id": "Ie5_Hf8Ff75r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "\n",
        "# build a training model\n",
        "\n",
        "class IntentClassifier(hk.Module):\n",
        "  \"\"\"A MLP which predicts intent from transformer embeddings\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self._init_scale = .5\n",
        "    self.number_classes = 38\n",
        "\n",
        "  def __call__(self, embeddings):\n",
        "    embedding_size = embeddings.shape[-1] \n",
        "    initializer = hk.initializers.VarianceScaling(\n",
        "        self._init_scale)\n",
        "    projection_layer = hk.Linear(\n",
        "        embedding_size, \n",
        "        w_init=initializer\n",
        "    )\n",
        "    classification_layer = hk.Linear(\n",
        "        self.number_classes,\n",
        "        w_init=initializer\n",
        "    )\n",
        "\n",
        "    projections = jax.nn.relu(\n",
        "        projection_layer(embeddings)\n",
        "    )\n",
        "    logits = classification_layer(projections)\n",
        "    return logits"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Kjq62LOFf9Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we build the Haiku training loop."
      ],
      "metadata": {
        "id": "bS8bmMangAOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initiliase model and optmiser\n",
        "\n",
        "def classify_intent(embeddings):\n",
        "  model = IntentClassifier()\n",
        "  return model(embeddings)\n",
        "\n",
        "classify_intent = hk.transform(classify_intent)\n",
        "\n",
        "# initialise model\n",
        "rng = jax.random.PRNGKey(42)\n",
        "x = np.zeros([1, 768])\n",
        "params = classify_intent.init(rng, x)\n",
        "\n",
        "optimiser = optax.adam(1e-3)\n",
        "opt_state = optimiser.init(params)\n"
      ],
      "metadata": {
        "id": "FZp6hQMNf_Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate loss \n",
        "key = jax.random.PRNGKey(42)\n",
        "\n",
        "def loss(params, batch):\n",
        "  \"\"\"Cross-entropy classification loss\"\"\"\n",
        "  batch_size = len(batch['labels'])\n",
        "  logits = classify_intent.apply(params,key, batch['embeddings'])\n",
        "  labels = jax.nn.one_hot(batch['labels'], num_classes=38)\n",
        "  log_likelihood = jnp.sum(labels * jax.nn.log_softmax(logits))\n",
        "  return -log_likelihood / batch_size"
      ],
      "metadata": {
        "id": "UwY8XsRMgEET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update weights\n",
        "@jax.jit\n",
        "def update(params, opt_state, batch):\n",
        "  # get data neded for training\n",
        "  grads = jax.grad(loss)(params, batch)\n",
        "  updates, opt_state = optimiser.update(grads, opt_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "  return params, opt_state"
      ],
      "metadata": {
        "id": "pvoc6MS1gM7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate accuracy per batch\n",
        "@jax.jit\n",
        "def accuracy(params, batch):\n",
        "    predictions = classify_intent.apply(params, key, batch['embeddings'])\n",
        "    print(predictions)\n",
        "    return jnp.mean(jnp.argmax(predictions, axis=-1) == batch[\"labels\"])"
      ],
      "metadata": {
        "id": "o2EtwmKTgSpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Training & evaluation loop.\n",
        "for epoc in range(10):\n",
        "\n",
        "  train_accs = 0\n",
        "  total_calcs = 0\n",
        "  for batch in tqdm(train_ds, desc='Train steps', leave=False):\n",
        "    batch = {\n",
        "        'embeddings':jnp.array(batch[0]),\n",
        "        'labels':jnp.array(batch[1])\n",
        "    }\n",
        "    params, opt_state = update(params, opt_state, batch)\n",
        "    train_accs+=accuracy(params, batch)\n",
        "    total_calcs+=1\n",
        "\n",
        "  if epoc%5==0:\n",
        "    print(f'At epoch: {epoc}') \n",
        "    train_accs /= round(total_calcs,2)\n",
        "\n",
        "    test_accs = 0\n",
        "    total_calcs = 0\n",
        "\n",
        "    for batch in tqdm(test_ds, desc='Test steps', leave=False):\n",
        "      batch = {\n",
        "          'embeddings':jnp.array(batch[0]),\n",
        "          'labels':jnp.array(batch[1])\n",
        "      }\n",
        "      test_accs+=accuracy(params, batch)\n",
        "      total_calcs+=1\n",
        "    \n",
        "    test_accs /= round(total_calcs,2)\n",
        "\n",
        "    print(f'\\nTrain accuracy:{train_accs}')\n",
        "    print(f'Test accuracy:{test_accs}')"
      ],
      "metadata": {
        "id": "4UZ4x66OgUef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a trained MLP that can predict the intent, we need to writ code that given new text, will classify an intent. "
      ],
      "metadata": {
        "id": "PCAs9gLVgDnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_new_text_mlp(text):\n",
        "  embedding = get_embedding({'text':[text]})['embedding']\n",
        "  logits = classify_intent.apply(params, key, embedding)\n",
        "  predicted_intent = jnp.argmax(jax.nn.softmax(logits))\n",
        "  converted_back_intent = le.inverse_transform([predicted_intent])[0]\n",
        "  print(f'Predicted intent for inpy \"{text}\" is {int(converted_back_intent)}')\n",
        "  \n",
        "  index = jnp.where(train_labels==predicted_intent)[0]\n",
        "  print(f'Example training data from this class: {train_intents[index][\"text\"][0]}')"
      ],
      "metadata": {
        "id": "OiCROb4lgZhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_new_text_mlp('Can I get a refund please')"
      ],
      "metadata": {
        "id": "j9o8sN2-gbIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Logistic regression"
      ],
      "metadata": {
        "id": "XvyQtDmggcvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a final experiment, we will see how a logistic regression model performs against our MLP, and how quick it can be to get very quick results."
      ],
      "metadata": {
        "id": "g5TgD99zgeQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "model = LogisticRegression(max_iter=6000)\n",
        "model.fit(train_embeddings, train_labels)\n",
        "y_hat_train = model.predict(train_embeddings)\n",
        "y_hat_test = model.predict(test_embeddings)\n",
        "\n",
        "train_accuracy = jnp.sum(y_hat_train == train_labels)/len(y_hat_train)\n",
        "test_accuracy = jnp.sum(y_hat_test == test_labels)/len(y_hat_test)\n",
        "\n",
        "print(f'Train accuracy:{train_accuracy}')\n",
        "print(f'Test accuracy:{test_accuracy}')"
      ],
      "metadata": {
        "id": "HxVGB2PGggQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, in just those few lines of code we have coded a logistic regression model that can classify the intent of a user given "
      ],
      "metadata": {
        "id": "ZQvBBHhYgkv0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_new_text_lr(text):\n",
        "  embedding = get_embedding({'text':[text]})['embedding']\n",
        "  predicted_intent = model.predict(embedding)\n",
        "  converted_back_intent = le.inverse_transform([predicted_intent])[0]\n",
        "  print(f'Predicted intent for inpy \"{text}\" is {int(converted_back_intent)}')\n",
        "  \n",
        "  index = jnp.where(train_labels==predicted_intent)[0]\n",
        "  print(f'Example training data from this class: {train_intents[index][\"text\"][0]}')"
      ],
      "metadata": {
        "id": "ovi4HNFfgiUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_new_text_lr('Can I please get a refund')"
      ],
      "metadata": {
        "id": "cJMvLsDEgmj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "## Conclusion\n",
        "**Summary:**\n",
        "\n",
        "[Summary of the main points/takeaways from the prac.]\n",
        "\n",
        "**Next Steps:** \n",
        "\n",
        "[Next steps for people who have completed the prac, like optional reading (e.g. blogs, papers, courses, youtube videos). This could also link to other pracs.]\n",
        "\n",
        "**Appendix:** \n",
        "\n",
        "[Anything (probably math heavy stuff) we don't have space for in the main practical sections.]\n",
        "\n",
        "**References:** \n",
        "\n",
        "[References for any content used in the notebook.]\n",
        "\n",
        "For other practicals from the Deep Learning Indaba, please visit [here](https://github.com/deep-learning-indaba/indaba-pracs-2022)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "## Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIZvkhfRz9Jz"
      },
      "outputs": [],
      "source": [
        "#@title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "HTML(\n",
        "\"\"\"\n",
        "<iframe \n",
        "\tsrc=\"https://forms.gle/bvLLPX74LMGrFefo9\",\n",
        "  width=\"80%\" \n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "7SrT6swYgCm9",
        "WILOYJH4gCnD",
        "e9NW58_3hAg2",
        "hE6hEihPhAhK",
        "rectkTs9iFHg",
        "hyDoAqXvfc_K",
        "fV3YG7QOZD-B",
        "o1ndpYE50BpG"
      ],
      "name": "attention_section.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}