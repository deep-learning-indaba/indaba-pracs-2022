{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# Intro to Reinforcement Learning\n",
        "\n",
        "\n",
        "[Need to update this picture]\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2022/blob/prac_intro_to_rl/intro_to_rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "© Deep Learning Indaba 2022. Apache License 2.0.\n",
        "\n",
        "**Authors:**\n",
        "Claude Formanek\n",
        "\n",
        "**Introduction:** \n",
        "\n",
        "Reinforcement Learning\n",
        "\n",
        "**Topics:** \n",
        "\n",
        "Content: Reinforcement Learning\n",
        "\n",
        "Level: Beginner\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "[Points on the exact learning outcomes from the prac. This should probably be <=5 things.]\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "[Knowledge required for this prac. You can link a relevant parallel track session, blogs, papers, courses, topics etc.]\n",
        "\n",
        "**Outline:** \n",
        "\n",
        "[Points that link to each section.]\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "For this practical, you will need to use a GPU to speed up training. To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"GPU\" in the \"Hardware accelerator\" box.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Setup\n",
        "\n",
        "For this practical, it will help to use a GPU runtime to speed up training. To do this, go to the `Runtime` menu in Colab, select `Change runtime type` and then in the popup menu, choose `GPU` in the `Hardware accelerator` box."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "form",
        "id": "4boGA9rYdt9l"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jaxlib in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (0.3.10+cuda11.cudnn82)\n",
            "Requirement already satisfied: scipy in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from jaxlib) (1.8.1)\n",
            "Requirement already satisfied: absl-py in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from jaxlib) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from jaxlib) (2.0)\n",
            "Requirement already satisfied: numpy>=1.19 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from jaxlib) (1.23.0)\n",
            "Requirement already satisfied: jax in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (0.3.13)\n",
            "Requirement already satisfied: opt-einsum in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from jax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from jax) (1.8.1)\n",
            "Requirement already satisfied: numpy>=1.19 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from jax) (1.23.0)\n",
            "Requirement already satisfied: absl-py in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from jax) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from jax) (4.2.0)\n",
            "Collecting git+https://github.com/deepmind/dm-haiku\n",
            "  Cloning https://github.com/deepmind/dm-haiku to /tmp/pip-req-build-54x2wm0l\n",
            "  Running command git clone -q https://github.com/deepmind/dm-haiku /tmp/pip-req-build-54x2wm0l\n",
            "  Resolved https://github.com/deepmind/dm-haiku to commit d6e3c2085253735c3179018be495ebabf1e6b17c\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from dm-haiku==0.0.8.dev0) (1.1.0)\n",
            "Requirement already satisfied: jmp>=0.0.2 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from dm-haiku==0.0.8.dev0) (0.0.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from dm-haiku==0.0.8.dev0) (1.23.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from dm-haiku==0.0.8.dev0) (0.8.10)\n",
            "Requirement already satisfied: gym in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (0.24.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from gym) (2.1.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from gym) (0.0.7)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from gym) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from gym) (1.23.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym) (3.8.0)\n",
            "Requirement already satisfied: gym[box2d] in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (0.24.1)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from gym[box2d]) (1.23.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from gym[box2d]) (4.12.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from gym[box2d]) (0.0.7)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from gym[box2d]) (2.1.0)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from gym[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame==2.1.0 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from gym[box2d]) (2.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym[box2d]) (3.8.0)\n",
            "Requirement already satisfied: rlax in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (0.1.2)\n",
            "Requirement already satisfied: chex>=0.0.8 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from rlax) (0.1.3)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from rlax) (1.23.0)\n",
            "Requirement already satisfied: absl-py>=0.9.0 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from rlax) (1.1.0)\n",
            "Requirement already satisfied: jax>=0.1.55 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from rlax) (0.3.13)\n",
            "Requirement already satisfied: distrax>=0.0.2 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from rlax) (0.1.2)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from rlax) (0.3.10+cuda11.cudnn82)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from chex>=0.0.8->rlax) (0.11.2)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from chex>=0.0.8->rlax) (0.1.7)\n",
            "Requirement already satisfied: tensorflow-probability>=0.15.0 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from distrax>=0.0.2->rlax) (0.17.0)\n",
            "Requirement already satisfied: opt-einsum in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from jax>=0.1.55->rlax) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from jax>=0.1.55->rlax) (4.2.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from jax>=0.1.55->rlax) (1.8.1)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from jaxlib>=0.1.37->rlax) (2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from tensorflow-probability>=0.15.0->distrax>=0.0.2->rlax) (1.16.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from tensorflow-probability>=0.15.0->distrax>=0.0.2->rlax) (0.5.3)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from tensorflow-probability>=0.15.0->distrax>=0.0.2->rlax) (2.1.0)\n",
            "Requirement already satisfied: decorator in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from tensorflow-probability>=0.15.0->distrax>=0.0.2->rlax) (5.1.1)\n",
            "Requirement already satisfied: optax in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (0.1.2)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from optax) (0.3.10+cuda11.cudnn82)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from optax) (4.2.0)\n",
            "Requirement already satisfied: chex>=0.0.4 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from optax) (0.1.3)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from optax) (1.1.0)\n",
            "Requirement already satisfied: jax>=0.1.55 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from optax) (0.3.13)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from optax) (1.23.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from chex>=0.0.4->optax) (0.11.2)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from chex>=0.0.4->optax) (0.1.7)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from jax>=0.1.55->optax) (1.8.1)\n",
            "Requirement already satisfied: opt-einsum in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from jax>=0.1.55->optax) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from jaxlib>=0.1.37->optax) (2.0)\n",
            "Collecting matplotlib\n",
            "  Using cached matplotlib-3.5.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
            "Collecting pillow>=6.2.0\n",
            "  Downloading Pillow-9.2.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2 MB 7.9 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
            "  Downloading kiwisolver-1.4.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 10.1 MB/s eta 0:00:01\n",
            "\u001b[?25hCollecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.34.4-py3-none-any.whl (944 kB)\n",
            "\u001b[K     |████████████████████████████████| 944 kB 8.2 MB/s eta 0:00:01\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from matplotlib) (1.23.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from matplotlib) (21.3)\n",
            "Collecting cycler>=0.10\n",
            "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in /home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Installing collected packages: pillow, kiwisolver, fonttools, cycler, matplotlib\n",
            "Successfully installed cycler-0.11.0 fonttools-4.34.4 kiwisolver-1.4.3 matplotlib-3.5.2 pillow-9.2.0\n"
          ]
        }
      ],
      "source": [
        "# @title Install required packages (Run Cell)\n",
        "# %%capture\n",
        "!pip install jaxlib\n",
        "!pip install jax\n",
        "!pip install git+https://github.com/deepmind/dm-haiku\n",
        "!pip install gym\n",
        "!pip install gym[box2d]\n",
        "!pip install rlax\n",
        "!pip install optax\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gwbqggmcRjMy"
      },
      "outputs": [],
      "source": [
        "# @title Import required packages (Run Cell)\n",
        "# %%capture\n",
        "import random\n",
        "import collections # useful data structures\n",
        "import numpy as np\n",
        "import gym # reinforcement learning environments\n",
        "import jax\n",
        "import jax.numpy as jnp # jax numpy\n",
        "import haiku as hk # jax neural network library\n",
        "import optax # jax optimizer library\n",
        "import rlax # jax reinforcement learning library\n",
        "import matplotlib.pyplot as plt # graph plotting library\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZUp8i37dFbU"
      },
      "source": [
        "## Section 1: Key Concepts in Reinforcement Learning\n",
        "\n",
        "Reinforcement Learning (RL) is a subfield of Machine Learning (ML). RL algorithms try to learn the optimal actions to take in an environment in order to maximise some reward signal. More precicely, in RL we have an **agent** which percieves an observation $o_t$ of the current state $s_t$ of the **environment** and must choose an action $a_t$ to take. The environment then transitions to a new state $s_{t+1}$ in response to the agents action and also gives the the agent a scalar reward $r_t$ to indicate how good or bad the chosen action was given the environment's state. The goal in RL is for the agent to maximise the ammount of reward it receives from the environment over time. The subscipt $t$ is used to indicate the timestep number, i.e., $s_0$ is the state of the environment at the initial timestep and $a_{99}$ is the agent's action at the $99th$ timestep. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghgy69hFRjMz"
      },
      "source": [
        "### OpenAI Gym\n",
        "OpenAI has provided a Python package called Gym that includes implementations of popular environments and a simple interface for an RL agent to interact with. To create a gym environment, all you need to do is pass the name of the environment to the function `gym.make(<environment_name>)`. In this tutorial we will be using a simple environment called CartPole. In **CartPole** the task is for the agent to learn to balance the pole for as long as possible by moving the cart *left* or *right*.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/600/1*v8KcdjfVGf39yvTpXDTCGQ.gif\" width=\"30%\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WfxzajMYRjMz"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/claude/miniconda3/envs/indaba-prac/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "# Create the environment\n",
        "env = gym.make(\"CartPole-v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_BbftaJj3zu"
      },
      "source": [
        "### States and observations\n",
        "In RL the agent perceives an observation of environments state. In some settings the observation may include all the information underlying the environment's state. Such an envrionment is called **fully observed**. In other settings the the agent may only receive partial information about the environment's state in its observation. Such an environment is called **partially observed**. For the rest of this tutorial we will assume the environment is fully observed and so we will use state $s_t$ and observation $o_t$ interchangeably. In Gym we get the initial observation from the environment by calling the function `env.reset()`: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HdS8nqOgRjM0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial State:: [-0.03015566 -0.04560914  0.04607056  0.02550874]\n"
          ]
        }
      ],
      "source": [
        "# Reset the environment\n",
        "s_0 = env.reset()\n",
        "print(\"Initial State::\", s_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL1Nkgy7nUfn"
      },
      "source": [
        "In CartPole the state of the environment is represented by four numbers; *angular position of the pole, angular velocity of the pole, position of the cart, velocity of the cart*. \n",
        "\n",
        "### Actions\n",
        "In RL actions are usually either **discrete** or **continuous**. Continous actions are given by a vector of real numbers. Discrete actions are given by an integer value. In environments where we can count out the finite set of actions we usually use discrete actions. In CartPole there are only two actions; *left and right*. As such, the actions can be represented by integers $0$ and $1$. In gym we can easily get the list of possible actions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uOLZqU_LpIXh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment action space: Discrete(2)\n",
            "Number of actions: 2\n"
          ]
        }
      ],
      "source": [
        "# Get action space\n",
        "print(f\"Environment action space: {env.action_space}\")\n",
        "\n",
        "# Get num actions\n",
        "num_actions = env.action_space.n\n",
        "print(f\"Number of actions: {num_actions}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRsflxbDpoPm"
      },
      "source": [
        "### The agent's policy\n",
        "In RL the agent choses actions based on the observation it receives. We can think of the agent's action selection process as a function that takes an observation as input and returns an action as output. In RL we usually call this function the agent's **policy** and denote it $\\pi(s_t)=a_t$. \n",
        "\n",
        "In some cases we may want our policy to be stocastic, rather than deterministic. In such cases, actions are actiually randomly sampled from a probability distribution that is conditiond on the observation. We denote stocastic policies $a_t\\sim\\pi(\\cdot\\ |\\ s_t)$, where the symbol $\\cdot$ is simply a shorthand for *all actions* and $~$ means \"*sampled from*\".\n",
        "\n",
        "**Exercise 1:** If Bob has a deterministic policy $\\pi$ and the first time Bob uses his policy on some observation $o_t$ he choses action $0$. What will Bob's chosen action be if he uses his policy a second time on the exact same observation $o_t$. Chose from the options below and assume there are only two possible actions:\n",
        "1.   Bob will chose action 0 again.\n",
        "2.   Bob will chose action 1.\n",
        "3.   You can't say.\n",
        "\n",
        "**Exercise 2:**  If Alice has a stocastic policy $\\pi$ and the first time Alice uses her policy on some observation $o_t$ she choses action $0$. What will Alice's chosen action be if she uses her policy a second time on the exact same observation $o_t$. Chose from the options below and assume there are only two possible actions:\n",
        "1.   Alice will chose action 0 again.\n",
        "2.   Alice will chose action 1.\n",
        "3.   You can't say."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "id": "8peH76H7wBKN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\" \\nExercise 1: Since the policy is deterministic, Bob will always choose the same \\naction given the same observation.\\n\\nExercise 2: Since the policy is stocastic, the result from Alice's policy will be\\nrandom. So you can't say.\\n\""
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@title Exercise 1 & 2 solution\n",
        "# %%capture\n",
        "\n",
        "\"\"\" \n",
        "Exercise 1: Since the policy is deterministic, Bob will always choose the same \n",
        "action given the same observation.\n",
        "\n",
        "Exercise 2: Since the policy is stocastic, the result from Alice's policy will be\n",
        "random. So you can't say.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZdmz2Wqv_NN"
      },
      "source": [
        "As an exercise we will implement a stocastic policy as well as a deterministic policy for CartPole. \n",
        "\n",
        "**Exercise 3:** Complete the function below which should take an observation `obs` as input, compute the dot product between the `obs` and parameter vector `params=[1,-2,2,-1]` and then return a `0` if the result is less than or equal to zero and a `1` if the result is greater than zero. Is this a deterministic or stocastic policy?\n",
        "\n",
        "**Usefull methods:** \n",
        "*   [Numpy dot product](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)\n",
        "\n",
        "\n",
        "**Notes:**\n",
        "\n",
        "*   We already imported `numpy` as `np`.\n",
        "*   Assume `obs` is also a vector of four numbers like `params`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSGCd7XB1z8k"
      },
      "outputs": [],
      "source": [
        "def choose_action(obs):\n",
        "\n",
        "  # YOUR CODE\n",
        "\n",
        "\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "id": "OCzRmXbz2HfY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fixed observation: [1, 1, 2, 4]\n",
            "Result of policy call number 0: 1\n",
            "Result of policy call number 1: 1\n",
            "Result of policy call number 2: 1\n",
            "Result of policy call number 3: 1\n",
            "Result of policy call number 4: 1\n",
            "Result of policy call number 5: 1\n",
            "Result of policy call number 6: 1\n",
            "Result of policy call number 7: 1\n",
            "Result of policy call number 8: 1\n",
            "Result of policy call number 9: 1\n"
          ]
        }
      ],
      "source": [
        "#@title Exercise 3 solution\n",
        "\n",
        "def choose_action(obs):\n",
        "\n",
        "  weights = np.array([1,-2,2,-1])\n",
        "  dot_product = np.dot(obs, weights)\n",
        "\n",
        "  if dot_product >= 0:\n",
        "    action = 0\n",
        "  else:\n",
        "    action = 1\n",
        "\n",
        "  return action\n",
        "\n",
        "# TESTS\n",
        "fixed_obs = [1,1,2,4]\n",
        "print(f\"Fixed observation: {fixed_obs}\")\n",
        "for i in range(10):\n",
        "  print(f\"Result of policy call number {i}: {choose_action(fixed_obs)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI_-xUHL0qz-"
      },
      "source": [
        "**Exercise 4:** Complete the function below which should take an observation `obs` as input, compute the dot product between the `obs` and parameter vector `params=[1,-2,2,-1]` and then return a `0` 20% of the time and a `1` 80% of the time if the result is less than or equal to zero. If the result of the dot-product is greater than zero the function should return a `0` 100% of the time. Is this a deterministic or stocastic policy?\n",
        "\n",
        "**Usefull methods:** \n",
        "*   [Numpy random choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ypsMyq_S_jwv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fixed observation: [1, 1, 2, 4]\n",
            "Result of policy call number 0: None\n",
            "Result of policy call number 1: None\n",
            "Result of policy call number 2: None\n",
            "Result of policy call number 3: None\n",
            "Result of policy call number 4: None\n",
            "Result of policy call number 5: None\n",
            "Result of policy call number 6: None\n",
            "Result of policy call number 7: None\n",
            "Result of policy call number 8: None\n",
            "Result of policy call number 9: None\n"
          ]
        }
      ],
      "source": [
        "def choose_action(obs):\n",
        "\n",
        "  weights = np.array([1,-2,2,-1])\n",
        "  action = None # you will need to overwrite this\n",
        "\n",
        "  # YOUR CODE\n",
        "\n",
        "\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return action\n",
        "\n",
        "# TESTS\n",
        "fixed_obs = [1,1,2,4]\n",
        "print(f\"Fixed observation: {fixed_obs}\")\n",
        "for i in range(10):\n",
        "  print(f\"Result of policy call number {i}: {choose_action(fixed_obs)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "dmWivlcK4WlC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fixed observation: [1, 1, 2, 4]\n",
            "Result of policy call number 0: 0\n",
            "Result of policy call number 1: 1\n",
            "Result of policy call number 2: 1\n",
            "Result of policy call number 3: 1\n",
            "Result of policy call number 4: 1\n",
            "Result of policy call number 5: 1\n",
            "Result of policy call number 6: 1\n",
            "Result of policy call number 7: 1\n",
            "Result of policy call number 8: 1\n",
            "Result of policy call number 9: 1\n"
          ]
        }
      ],
      "source": [
        "#@title Exercise 4 solution\n",
        "\n",
        "def choose_action(obs):\n",
        "\n",
        "  weights = np.array([1,-2,2,-1])\n",
        "  dot_product = np.dot(obs, weights)\n",
        "\n",
        "  if dot_product <= 0:\n",
        "    action = np.random.choice([0,1], p=[0.2, 0.8])\n",
        "  else:\n",
        "    action = 0\n",
        "\n",
        "  return action\n",
        "\n",
        "# TESTS\n",
        "fixed_obs = [1,1,2,4]\n",
        "print(f\"Fixed observation: {fixed_obs}\")\n",
        "for i in range(10):\n",
        "  print(f\"Result of policy call number {i}: {choose_action(fixed_obs)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkuvT-jf6Ieh"
      },
      "source": [
        "### The environment transition function\n",
        "Now that we have a policy we can pass actions from the agent to the environment. The environment will the transition into a new state in response to the agent's action. In RL we model this process by a **state transition function** $P$ which takes the current state $s_t$ and an action $a_t$ as input and returns the next state $s_{t+1}$ as output. Like with policies, the state transiton function can either be deterministic $s_{t+1}=P(s_t, a_t)$ or it can be stocastic $s_{t+1}\\sim P(\\cdot\\ |\\ s_t, a_t)$. In gym we can pass actions to the environment by calling the `env.step(<action>)` function. The function will then return four values; the next observation, the reward for the action taken, a boolean flag to indicate if the game is done and finally some extra information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hh0j9-Tk7olb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Observation: [-0.04586864  0.0271866  -0.04945642  0.0103842 ]\n",
            "Action: 1\n",
            "Next observation: [-0.04532491  0.22298165 -0.04924874 -0.29748353]\n",
            "Reward: 1.0\n",
            "Game is done: False\n"
          ]
        }
      ],
      "source": [
        "# Get the initial obs by resetting the env\n",
        "initial_obs = env.reset()\n",
        "\n",
        "# Use the policy to chose an action\n",
        "action = choose_action(initial_obs)\n",
        "\n",
        "# Step the environment\n",
        "next_obs, reward, done, info = env.step(action)\n",
        "\n",
        "print(\"Observation:\", initial_obs)\n",
        "print(\"Action:\", action)\n",
        "print(\"Next observation:\", next_obs)\n",
        "print(\"Reward:\", reward)\n",
        "print(\"Game is done:\", done)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bX9iZtu48UYn"
      },
      "source": [
        "### Episode return\n",
        "In RL we usually break the agents interactions with the environment up into **episodes**. The sum of all reward collected during an episode is what we call the episode's **return**. The goal in RL is for the agent to chose actions which maximise the expected future return. In CartPole the agent receives a reward of `1` for every timestep the pole is still upright. If the pole falls over, the game is over and the agent receives a reward of `0`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6KZA1Nq9p47"
      },
      "source": [
        "### Agent-environment Loop\n",
        "Now that we know what a policy is and we know how to step the environment, lets close the agent-environment loop.\n",
        "\n",
        "**Exercise 5:** Write a function which runs one episode of CartPole by sequentially choosing actions and stepping the environment. You should use the stocastic policy we defined earlier to chose actions. The function should keep track of the reward received and output the return at the end of the episode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Buy0X7mi-gHP"
      },
      "outputs": [],
      "source": [
        "def run_episode(env):\n",
        "  episode_return = 0\n",
        "\n",
        "  ## YOUR CODE\n",
        "\n",
        "  # HINT: reset environment\n",
        "\n",
        "  # HINT: while loop until episode is done\n",
        "\n",
        "    # HINT: choose action\n",
        "\n",
        "    # HINT: step environment\n",
        "    \n",
        "    # HINT: add reward to episode_return\n",
        "\n",
        "  ## END CODE\n",
        "\n",
        "  return episode_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cellView": "form",
        "id": "JAfPqaeS_Krs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode return: 9.0\n"
          ]
        }
      ],
      "source": [
        "#@title Exercise 5 solution\n",
        "\n",
        "def run_episode(env):\n",
        "  episode_return = 0\n",
        "  \n",
        "  obs = env.reset()\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "    action = choose_action(obs)\n",
        "\n",
        "    next_obs, reward, done, info = env.step(action)\n",
        "\n",
        "    episode_return += reward\n",
        "\n",
        "    # Critical\n",
        "    obs = next_obs\n",
        "\n",
        "  return episode_return\n",
        "\n",
        "# TEST\n",
        "print(\"Episode return:\", run_episode(env))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUGdzHxJnZGl"
      },
      "source": [
        "In CartPole, the environment is considered solved when the agent can reliably achieve an episode return of 500. As you can see, our current policy is nowhere near optimal yet. Lets learn a way to find an optimal policy.\n",
        "\n",
        "One way we can find an optimal policy is by randomly searching for it. Obviously in a complex environment finding an optimal policy by randomly trying differnet strategies could take forever. But CartPole is a sufficiently simple environment that it might just work.\n",
        "\n",
        "Before we implement random policy search, lets take a look at the following environment loop function that we implemented for you. Its the environmentloop we will be using for the rest of the notebook. We will use a [NamedTuple](https://www.geeksforgeeks.org/namedtuple-in-python/) to bundle `obs`, `action`, `reward`, `next_obs` and the done flag into a **transition** object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vpxNZxbORjM0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transition obs: [1, 2, -1, 2]\n",
            "Transition action: 0\n",
            "Transition reward: 10\n",
            "Transition next obs: [1, 2, 2, 1]\n",
            "Transition done: True\n"
          ]
        }
      ],
      "source": [
        "# Named tuple to store transition\n",
        "Transition = collections.namedtuple(\"Transition\", [\"obs\", \"action\", \"reward\", \"next_obs\", \"done\"])\n",
        "\n",
        "# TEST\n",
        "transition = Transition(\n",
        "    obs=[1,2,-1,2],\n",
        "    action=0,\n",
        "    reward=10,\n",
        "    next_obs=[1,2,2,1],\n",
        "    done=True\n",
        ")\n",
        "\n",
        "print(\"Transition obs:\", transition.obs)\n",
        "print(\"Transition action:\", transition.action)\n",
        "print(\"Transition reward:\", transition.reward)\n",
        "print(\"Transition next obs:\", transition.next_obs)\n",
        "print(\"Transition done:\", transition.done)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSCMHsULvlky"
      },
      "source": [
        "Below is the environment loop function we have implemented for you. We reccomend reading through it and trying to understand it, but if anything is unclear, don't worry about it. It should all make more sense as we work through this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "ZWBwz3zMRjM0"
      },
      "outputs": [],
      "source": [
        "# Environment loop\n",
        "def run_environment_loop(rng, env, agent_params, agent_select_action_func, \n",
        "    agent_actor_state=None, agent_learn_func=None, agent_learner_state=None, \n",
        "    agent_memory=None, num_episodes=1000, evaluator_period=100, \n",
        "    evaluation_episodes=32, learn_steps_per_episode=1):\n",
        "    \"\"\"\n",
        "    This function runs several episodes in an environment and periodically does \n",
        "    some agent learning and evaluation.\n",
        "    \n",
        "    Args:\n",
        "        rng: a random number generator. This is for jax.\n",
        "        env: a gym environment.\n",
        "        agent_params: an object to store parameters that the agent uses.\n",
        "        agent_select_func: a function that does action selection for the agent.\n",
        "        agent_actor_state (optional): an object that stores the internal state \n",
        "            of the agents action selection function.\n",
        "        agent_learn_func (optional): a function that does some learning for the \n",
        "            agent by updating the agent parameters.\n",
        "        agent_learn_state (optional): an object that stores the internal state \n",
        "            of the agent learn function.\n",
        "        agent_memory (optional): an object for storing an retrieving historical \n",
        "            experience.\n",
        "        num_episodes: how many episodes to run.\n",
        "        evaluator_period: how often to run evaluation.\n",
        "        evaluation_episodes: how many evaluation episodes to run.\n",
        "\n",
        "    Returns:\n",
        "        episode_returns: list of all the episode returns.\n",
        "        evaluator_episode_returns: list of all the evaluator episode returns.\n",
        "    \"\"\"\n",
        "    episode_returns = [] # List to store history of episode returns.\n",
        "    evaluator_episode_returns = [] # List to store history of evaluator returns.\n",
        "    for episode in range(num_episodes):\n",
        "\n",
        "        # Reset environment.\n",
        "        obs = env.reset()\n",
        "        episode_return = 0\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "\n",
        "            # Agent select action.\n",
        "            action, agent_actor_state = agent_select_action_func(\n",
        "                                            next(rng), \n",
        "                                            agent_params, \n",
        "                                            agent_actor_state, \n",
        "                                            np.array(obs)\n",
        "                                        )\n",
        "\n",
        "            # Step environment.\n",
        "            next_obs, reward, done, _ = env.step(int(action))\n",
        "\n",
        "            # Pack into transition.\n",
        "            transition = Transition(obs, action, reward, next_obs, done)\n",
        "\n",
        "            # Add transition to memory.\n",
        "            if agent_memory: # check if agent has memory\n",
        "              agent_memory.push(transition)\n",
        "\n",
        "            # Add reward to episode return.\n",
        "            episode_return += reward\n",
        "\n",
        "            # Set obs to next obs before next environment step. CRITICAL!!!\n",
        "            obs = next_obs\n",
        "\n",
        "        episode_returns.append(episode_return)\n",
        "\n",
        "        # At the end of every episode we do a learn step.\n",
        "        if agent_memory and agent_memory.is_ready(): # Make sure memory is ready\n",
        "\n",
        "            for _ in range(learn_steps_per_episode):\n",
        "                # First sample memory and then pass the result to the learn function\n",
        "                memory = agent_memory.sample()\n",
        "                agent_params, agent_learner_state = agent_learn_func(\n",
        "                                                        next(rng), \n",
        "                                                        agent_params, \n",
        "                                                        agent_learner_state, \n",
        "                                                        memory\n",
        "                                                    )\n",
        "\n",
        "        if (episode % evaluator_period) == 0: # Do evaluation\n",
        "\n",
        "            evaluator_episode_return = 0\n",
        "            for eval_episode in range(evaluation_episodes):\n",
        "                obs = env.reset()\n",
        "                done = False\n",
        "                while not done:\n",
        "                    action, _ = agent_select_action_func(\n",
        "                                    next(rng), \n",
        "                                    agent_params, \n",
        "                                    agent_actor_state, \n",
        "                                    np.array(obs), \n",
        "                                    evaluation=True\n",
        "                                )\n",
        "\n",
        "                    obs, reward, done, _ = env.step(int(action))\n",
        "\n",
        "                    evaluator_episode_return += reward\n",
        "\n",
        "            evaluator_episode_return /= evaluation_episodes\n",
        "\n",
        "            evaluator_episode_returns.append(evaluator_episode_return)\n",
        "\n",
        "            logs = [\n",
        "                    f\"Episode: {episode}\",\n",
        "                    f\"Episode Return: {episode_return}\",\n",
        "                    f\"Average Episode Return: {np.mean(episode_returns[-20:])}\",\n",
        "                    f\"Evaluator Episode Return: {evaluator_episode_return}\"\n",
        "            ]\n",
        "\n",
        "            print(*logs, sep=\"\\t\") # Print the logs\n",
        "\n",
        "    return episode_returns, evaluator_episode_returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDhgij_UwWRn"
      },
      "source": [
        "Before we can test this environment loop function with our policy function we implemented earlier, we will need to modify it so that its interface matches the way our environment loop expects it. The `select_action` function should take a random seed in the first argument position, then parameters, then the actors internal state (more on this later), then the observation and finally a `evaluation` boolean flag that indicates if the function is being called during the evaluation loop or not (more on this later). The function should return the chosen action and the next state of the actor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "zujdaD_HwqZ3"
      },
      "outputs": [],
      "source": [
        "def select_action(key, params, actor_state, obs, evaluation=False):\n",
        "    \"\"\"\n",
        "    This function assums params is a vecotor of same size as obs.\n",
        "    It computes the dot product between params and obs. If the result is\n",
        "    less than zero it returns actions zero. If the result is greater than\n",
        "    or equal to zero, it returns action 1.\n",
        "    \"\"\"\n",
        "    dot_product = np.dot(params, obs)\n",
        "    \n",
        "    if dot_product >= 0:\n",
        "      action = 1\n",
        "    else:\n",
        "      action = 0\n",
        "\n",
        "    return action, actor_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v6VIqYozHDX"
      },
      "source": [
        "**Exercise 6:** Can you convert this into a function that only uses Jax methods so that we can jit the function?\n",
        "\n",
        "\n",
        "**Useful functions:** \n",
        "*   [Jax dot product](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.dot.html)\n",
        "*   [Jax select](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.select.html#jax.lax.select)\n",
        "\n",
        "**Note:**\n",
        "\n",
        "We already imported `jax.numpy` as `jnp`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hG3-qk4X0pyE"
      },
      "outputs": [],
      "source": [
        "def select_action(key, params, actor_state, obs, evaluation=False):\n",
        "\n",
        "  # YOUR CODE\n",
        "\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return action, actor_state\n",
        "\n",
        "# TEST: the result of your function should be action 1 in this test\n",
        "key = None # not used\n",
        "actor_state = None # not used\n",
        "select_action_jit = jax.jit(select_action) # jit the function\n",
        "params = np.array([1,1,-1,-1], \"float32\")\n",
        "obs = np.array([1,1,1,1], \"float32\")\n",
        "\n",
        "action, actor_state = select_action_jit(key, params, actor_state, obs)\n",
        "print(\"Action:\", action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "cellView": "form",
        "id": "cRAQiMPwzZFw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action: 1\n"
          ]
        }
      ],
      "source": [
        "#@title Exercise 6 solution\n",
        "\n",
        "def select_action(key, params, actor_state, obs, evaluation=False):\n",
        "    dot_product = jnp.dot(params, obs)\n",
        "\n",
        "    action = jax.lax.select(\n",
        "        dot_product >= 0.0,\n",
        "        1,\n",
        "        0,\n",
        "    )\n",
        "\n",
        "    return action, actor_state\n",
        "\n",
        "# TEST: the result of your function should be action 1 in this test\n",
        "key = None # not used\n",
        "actor_state = None # not used\n",
        "select_action_jit = jax.jit(select_action) # jit the function\n",
        "\n",
        "params = np.array([1,1,-1,-1], \"float32\")\n",
        "obs = np.array([1,1,1,1], \"float32\")\n",
        "\n",
        "action, actor_state = select_action_jit(key, params, actor_state, obs)\n",
        "print(\"Action:\", action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpAqVqVY_DcR"
      },
      "source": [
        "We can now test our `select_action` function in the `run_environment_loop` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "uQ2uN-kh_Qpm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 0\tEpisode Return: 8.0\tAverage Episode Return: 8.0\tEvaluator Episode Return: 9.0625\n",
            "Episode: 100\tEpisode Return: 8.0\tAverage Episode Return: 9.25\tEvaluator Episode Return: 8.90625\n",
            "Episode: 200\tEpisode Return: 9.0\tAverage Episode Return: 8.85\tEvaluator Episode Return: 9.15625\n",
            "Episode: 300\tEpisode Return: 8.0\tAverage Episode Return: 8.95\tEvaluator Episode Return: 8.96875\n",
            "Episode: 400\tEpisode Return: 10.0\tAverage Episode Return: 9.15\tEvaluator Episode Return: 9.03125\n",
            "Episode: 500\tEpisode Return: 9.0\tAverage Episode Return: 9.1\tEvaluator Episode Return: 8.9375\n",
            "Episode: 600\tEpisode Return: 8.0\tAverage Episode Return: 9.0\tEvaluator Episode Return: 8.84375\n",
            "Episode: 700\tEpisode Return: 9.0\tAverage Episode Return: 9.05\tEvaluator Episode Return: 9.0\n",
            "Episode: 800\tEpisode Return: 9.0\tAverage Episode Return: 9.4\tEvaluator Episode Return: 9.15625\n",
            "Episode: 900\tEpisode Return: 9.0\tAverage Episode Return: 9.0\tEvaluator Episode Return: 9.0\n",
            "Episode: 1000\tEpisode Return: 9.0\tAverage Episode Return: 9.0\tEvaluator Episode Return: 8.90625\n",
            "Average episode returns: 9.061938061938061\n"
          ]
        }
      ],
      "source": [
        "# Jax random number generator\n",
        "rng = hk.PRNGSequence(jax.random.PRNGKey(0)) # don't worry about this for now\n",
        "\n",
        "# Some arbitrary parameters\n",
        "params = np.array([1,1,-1,-1], \"float32\")\n",
        "\n",
        "episode_returns, evaluator_returns = run_environment_loop(\n",
        "                                        rng, \n",
        "                                        env, \n",
        "                                        params,\n",
        "                                        select_action_jit,\n",
        "                                        num_episodes=1001,\n",
        "                                      )\n",
        "\n",
        "print(\"Average episode returns:\", np.mean(episode_returns))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zu51ep7Sh0M"
      },
      "source": [
        "### Agent memory\n",
        "\n",
        "In many RL algorithms the agent uses a kind of memory to store some of the experiences it had in the environment. The interface we will use for the agent's memory is very simple. It will have a function `memory.push(<transition>)` that adds some information about the transition to the memory, a function `memory.is_ready()` to check if the memory is ready to do some learning, and fianlly a function `memory.sample()` that returns some information that the agent learn function can use to do learning.\n",
        "\n",
        "#### Average Episode Return Memory\n",
        "We have built a simple agent memory module for you below. It stores the `epsisode_returns` of the last 20 episodes. Read through our implementation below and see if you can understand it. The `memory.sample()` method returns the average episode return over the last 20 episodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "YVkTBIK5RjM1"
      },
      "outputs": [],
      "source": [
        "# A NamedTuple to store the average episode return of the last 20 runs\n",
        "AverageEpisodeReturnMemory = collections.namedtuple(\n",
        "                                \"AverageEpisodeReturnMemory\", \n",
        "                                [\"average_episode_return\"]\n",
        "                            )\n",
        "\n",
        "class AverageEpisodeReturnBuffer:\n",
        "\n",
        "    def __init__(self, num_episodes_to_store=20):\n",
        "        \"\"\"\n",
        "        This class implements an agent memory that stores the average episode \n",
        "        return over the last 20 episodes.\n",
        "        \"\"\"\n",
        "        self.num_episodes_to_store = num_episodes_to_store\n",
        "        self.episode_return_buffer = []\n",
        "        self.current_episode_return = 0\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.current_episode_return += transition.reward\n",
        "\n",
        "        if transition.done: # If the episode is done\n",
        "            # Add episode return to buffer\n",
        "            self.episode_return_buffer.append(self.current_episode_return)\n",
        "\n",
        "            # Reset episode return\n",
        "            self.current_episode_return = 0\n",
        "\n",
        "\n",
        "    def is_ready(self):\n",
        "        return len(self.episode_return_buffer) == self.num_episodes_to_store\n",
        "\n",
        "    def sample(self):\n",
        "        average_episode_return = np.mean(self.episode_return_buffer)\n",
        "\n",
        "        # Clear episode return buffer\n",
        "        self.episode_return_buffer = []\n",
        "\n",
        "        return AverageEpisodeReturnMemory(average_episode_return)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTPlIq4oDBPY"
      },
      "source": [
        "## Section 2: Random Policy Search (RPS)\n",
        "Now we are ready to implement the random policy search algorithm which will randomly try different policies and keep track of the best policy it has found so far. We will say that policy $A$ is better than policy $B$ if the average episode return policy $A$ achieved over the last 20 episodes is greater than that of policy $B$. We will need to modify the way we store the agents parameters so that we always have access to the latest parameters as well as the best parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1DcaC-PQRjM1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best params: [0 0 0 0]\n",
            "Current params: [ 1  1 -1 -1]\n"
          ]
        }
      ],
      "source": [
        "# Parameter container for random policy search\n",
        "RandomPolicySearchParams = collections.namedtuple(\"RandomPolicySearchParams\", [\"current\", \"best\"])\n",
        "\n",
        "# TEST: store two different sets of parameters\n",
        "current_params = np.array([1,1,-1,-1])\n",
        "best_params = np.array([0,0,0,0])\n",
        "rps_params = RandomPolicySearchParams(current_params, best_params)\n",
        "\n",
        "print(f\"Best params: {rps_params.best}\")\n",
        "print(f\"Current params: {rps_params.current}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tExceGeGNYH"
      },
      "source": [
        "### RPS select action function\n",
        "Now lets once again modify our `select_action` function such that it uses the best parameters when when `evaluation==True` and uses the current parameters when `evaluation==False`.\n",
        "\n",
        "**Exercise 7:** Implement the `random_policy_search_select_action` function as described above. Make sure you use Jax so that we can jit the function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1kmwT35JRjM1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current action: 1\n",
            "Best action: 1\n"
          ]
        }
      ],
      "source": [
        "def random_policy_search_select_action(\n",
        "    key, \n",
        "    params, \n",
        "    actor_state, \n",
        "    obs, \n",
        "    evaluation=False\n",
        "):\n",
        "\n",
        "  # YOUR CODE\n",
        "\n",
        "  # HINT: best_action = ... (two lines)\n",
        "\n",
        "  # HINT: current_action = ... (two lines)\n",
        "\n",
        "  # HINT: action = best_action if evaluation else current action (one line)\n",
        "\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return action, actor_state\n",
        "\n",
        "# TEST:\n",
        "key = None # not used\n",
        "actor_state = None # not used\n",
        "random_policy_search_select_action_jit = jax.jit(\n",
        "                                            random_policy_search_select_action\n",
        "                                            ) # jit the function\n",
        "# Parameters\n",
        "current_params = np.array([-1,-1,-1,-1])\n",
        "best_params = np.array([0,0,0,0])\n",
        "rps_params = RandomPolicySearchParams(current_params, best_params)\n",
        "\n",
        "# Observation\n",
        "obs = np.array([1,1,1,1], \"float32\")\n",
        "\n",
        "current_action, actor_state = random_policy_search_select_action_jit(\n",
        "    key, \n",
        "    rps_params, \n",
        "    actor_state, \n",
        "    obs, \n",
        "    evaluation=False\n",
        ")\n",
        "\n",
        "best_action, actor_state = random_policy_search_select_action_jit(\n",
        "    key, \n",
        "    rps_params, \n",
        "    actor_state, \n",
        "    obs, \n",
        "    evaluation=True\n",
        ")\n",
        "\n",
        "print(\"Current action:\", current_action)\n",
        "print(\"Best action:\", best_action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UH_9qJ08IFg6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current action: 0\n",
            "Best action: 1\n"
          ]
        }
      ],
      "source": [
        "#@title Exercise 7 solution\n",
        "\n",
        "def random_policy_search_select_action(\n",
        "    key, \n",
        "    params, \n",
        "    actor_state, \n",
        "    obs, \n",
        "    evaluation=False\n",
        "):\n",
        "\n",
        "  # YOUR CODE\n",
        "\n",
        "  dot_product = jnp.dot(params.best, obs)\n",
        "  best_action = jax.lax.select(\n",
        "      dot_product >= 0,\n",
        "      1,\n",
        "      0\n",
        "  )\n",
        "\n",
        "  dot_product = jnp.dot(params.current, obs)\n",
        "  current_action = jax.lax.select(\n",
        "      dot_product >= 0,\n",
        "      1,\n",
        "      0\n",
        "  )\n",
        "\n",
        "  action = jax.lax.select(\n",
        "      evaluation,\n",
        "      best_action,\n",
        "      current_action\n",
        "  )\n",
        "\n",
        "  # END YOUR CODE\n",
        "\n",
        "  return action, actor_state\n",
        "\n",
        "# TEST:\n",
        "key = None # not used\n",
        "actor_state = None # not used\n",
        "random_policy_search_select_action_jit = jax.jit(\n",
        "                                            random_policy_search_select_action\n",
        "                                            ) # jit the function\n",
        "# Parameters\n",
        "current_params = np.array([-1,-1,-1,-1])\n",
        "best_params = np.array([0,0,0,0])\n",
        "rps_params = RandomPolicySearchParams(current_params, best_params)\n",
        "\n",
        "# Observation\n",
        "obs = np.array([1,1,1,1], \"float32\")\n",
        "\n",
        "current_action, actor_state = random_policy_search_select_action_jit(\n",
        "    key, \n",
        "    rps_params, \n",
        "    actor_state, \n",
        "    obs, \n",
        "    evaluation=False\n",
        ")\n",
        "\n",
        "best_action, actor_state = random_policy_search_select_action_jit(\n",
        "    key, \n",
        "    rps_params, \n",
        "    actor_state, \n",
        "    obs, \n",
        "    evaluation=True\n",
        ")\n",
        "\n",
        "print(\"Current action:\", current_action)\n",
        "print(\"Best action:\", best_action)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oXBsSa2KjWE"
      },
      "source": [
        "### RPS learn function\n",
        "Now we need to implement a learn function for our random policy search agent. The learn function is quite simple. All we need to do is check if the current weights are better than the best weights. If they are better, then set the current weights to be the new best weights and randomly sample a new set of current weights. Lets assume our learn function receives a memory from the AverageEpisodeReturnMemory we implmented earlier. We can uses this to compare the current weights to the best weights. We will need to keep track of the best average episode return for the learn function. For that we can use the `learn_state` argument."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4cSH4uYmRjM2"
      },
      "outputs": [],
      "source": [
        "# A NamedTuple to store the best average episode return so far\n",
        "LearnerState = collections.namedtuple(\n",
        "                                      \"LearnerState\", \n",
        "                                      [\"count\", \"best_average_episode_return\"]\n",
        "                                      )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Djxc9j-LzkM"
      },
      "source": [
        "**Exercise 8:** Write a function to randomly sample new weights using jax. The weights should be sampled from the interval [-2,2].\n",
        "\n",
        "**Useful functions:** \n",
        "*   [Jax random uniform sample](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.uniform.html#jax.random.uniform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8V2yFM2XMjGW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New weights: None\n"
          ]
        }
      ],
      "source": [
        "def get_new_random_weights(random_key, old_weights):\n",
        "    new_weights_shape = old_weights.shape\n",
        "    new_weights_dtype = old_weights.dtype\n",
        "    new_weights = None # you should overwrite this\n",
        "\n",
        "    # YOUR CODE\n",
        "    \n",
        "    # new_weights = ...\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return new_weights\n",
        "\n",
        "# TEST \n",
        "old_weights = np.array([1,1,1,1], \"float32\")\n",
        "get_new_random_weights_jit = jax.jit(get_new_random_weights) # jit the function\n",
        "random_key = next(rng) # get net random key from the random number generator\n",
        "new_weights = get_new_random_weights_jit(random_key, old_weights)\n",
        "print(\"New weights:\", new_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "T05R_AWHLx_I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New weights: [-0.05722666 -0.61306715 -0.36567497  0.73983717]\n"
          ]
        }
      ],
      "source": [
        "#@title Exercise 8 solution\n",
        "\n",
        "def get_new_random_weights(random_key, old_weights):\n",
        "    new_weights_shape = old_weights.shape\n",
        "    new_weights_dtype = old_weights.dtype\n",
        "    new_weights = None # you should overwrite this\n",
        "\n",
        "    # YOUR CODE\n",
        "    \n",
        "    new_weights = jax.random.uniform(\n",
        "                      random_key, \n",
        "                      new_weights_shape, \n",
        "                      new_weights_dtype,\n",
        "                      minval=-2.0,\n",
        "                      maxval=2.0\n",
        "                  )\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return new_weights\n",
        "\n",
        "# TEST \n",
        "old_weights = np.array([1,1,1,1], \"float32\")\n",
        "get_new_random_weights_jit = jax.jit(get_new_random_weights) # jit the function\n",
        "random_key = next(rng) # get net random key from the random number generator\n",
        "new_weights = get_new_random_weights(random_key, old_weights)\n",
        "print(\"New weights:\", new_weights)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxZXdP8bOu6b"
      },
      "source": [
        "Now lets implement the random policy search function.\n",
        "\n",
        "**Exercise 9:** Use the description of the random policy search learn function at the top of this section to complete the function below. Try to use jax (remember `jax.lax.select()`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "-ElX1w1iQTrE"
      },
      "outputs": [],
      "source": [
        "def random_policy_search_learn(key, params, learner_state, memory):\n",
        "    best_weights = params.best \n",
        "    current_weights = params.current\n",
        "\n",
        "    average_episode_return = memory.average_episode_return\n",
        "    best_average_episode_return = learner_state.best_average_episode_return\n",
        "\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    # HINT: if current better than best then ...\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    # Generate new random weights\n",
        "    new_weights = get_new_random_weights_jit(key, best_weights)\n",
        "\n",
        "    # Bundle weights in RPS Params NamedTuple\n",
        "    params = RandomPolicySearchParams(current=new_weights, best=best_weights)\n",
        "\n",
        "    # Increment the learn counter by one\n",
        "    learn_count = learner_state.count + 1\n",
        "\n",
        "    return params, LearnerState(learn_count, best_average_episode_return)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "zkUcO2efRjM2"
      },
      "outputs": [],
      "source": [
        "#@title Exercise 9 solution\n",
        "def random_policy_search_learn(key, params, learner_state, memory):\n",
        "\n",
        "    best_weights = params.best \n",
        "    current_weights = params.current\n",
        "\n",
        "    current_episode_return = memory.average_episode_return\n",
        "    best_average_episode_return = learner_state.best_average_episode_return\n",
        "\n",
        "    best_weights = jax.lax.select(\n",
        "        current_episode_return > best_average_episode_return,\n",
        "        current_weights,\n",
        "        best_weights\n",
        "    )\n",
        "        \n",
        "    best_average_episode_return = jax.lax.select(\n",
        "        current_episode_return > best_average_episode_return,\n",
        "        current_episode_return,\n",
        "        best_average_episode_return\n",
        "    )\n",
        "\n",
        "    # Generate new random weights\n",
        "    new_weights = get_new_random_weights_jit(key, best_weights)\n",
        "\n",
        "    # Bundle weights in RPS Params NamedTuple\n",
        "    params = RandomPolicySearchParams(current=new_weights, best=best_weights)\n",
        "\n",
        "    # Increment the learn counter by one\n",
        "    learn_count = learner_state.count + 1\n",
        "\n",
        "    return params, LearnerState(learn_count, best_average_episode_return)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ol_AxnMBdgP"
      },
      "source": [
        "Now we can put everything together using the environment loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Qx57tf7vRjM3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 0\tEpisode Return: 181.0\tAverage Episode Return: 181.0\tEvaluator Episode Return: 140.8125\n",
            "Episode: 100\tEpisode Return: 9.0\tAverage Episode Return: 29.05\tEvaluator Episode Return: 500.0\n",
            "Episode: 200\tEpisode Return: 10.0\tAverage Episode Return: 36.65\tEvaluator Episode Return: 500.0\n",
            "Episode: 300\tEpisode Return: 9.0\tAverage Episode Return: 26.1\tEvaluator Episode Return: 500.0\n",
            "Episode: 400\tEpisode Return: 9.0\tAverage Episode Return: 398.0\tEvaluator Episode Return: 500.0\n",
            "Episode: 500\tEpisode Return: 91.0\tAverage Episode Return: 13.55\tEvaluator Episode Return: 500.0\n",
            "Episode: 600\tEpisode Return: 28.0\tAverage Episode Return: 139.0\tEvaluator Episode Return: 500.0\n",
            "Episode: 700\tEpisode Return: 50.0\tAverage Episode Return: 27.6\tEvaluator Episode Return: 500.0\n",
            "Episode: 800\tEpisode Return: 44.0\tAverage Episode Return: 29.05\tEvaluator Episode Return: 500.0\n",
            "Episode: 900\tEpisode Return: 10.0\tAverage Episode Return: 9.35\tEvaluator Episode Return: 500.0\n",
            "Episode: 1000\tEpisode Return: 24.0\tAverage Episode Return: 122.65\tEvaluator Episode Return: 500.0\n",
            "Episode: 1100\tEpisode Return: 71.0\tAverage Episode Return: 346.75\tEvaluator Episode Return: 500.0\n",
            "Episode: 1200\tEpisode Return: 10.0\tAverage Episode Return: 9.9\tEvaluator Episode Return: 500.0\n",
            "Episode: 1300\tEpisode Return: 157.0\tAverage Episode Return: 17.3\tEvaluator Episode Return: 500.0\n",
            "Episode: 1400\tEpisode Return: 8.0\tAverage Episode Return: 98.2\tEvaluator Episode Return: 500.0\n",
            "Episode: 1500\tEpisode Return: 54.0\tAverage Episode Return: 28.05\tEvaluator Episode Return: 500.0\n",
            "Episode: 1600\tEpisode Return: 8.0\tAverage Episode Return: 9.15\tEvaluator Episode Return: 500.0\n",
            "Episode: 1700\tEpisode Return: 187.0\tAverage Episode Return: 140.95\tEvaluator Episode Return: 500.0\n",
            "Episode: 1800\tEpisode Return: 97.0\tAverage Episode Return: 99.05\tEvaluator Episode Return: 500.0\n",
            "Episode: 1900\tEpisode Return: 68.0\tAverage Episode Return: 12.75\tEvaluator Episode Return: 500.0\n",
            "Episode: 2000\tEpisode Return: 10.0\tAverage Episode Return: 11.55\tEvaluator Episode Return: 500.0\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "initial_learner_state = LearnerState(0, -float(\"inf\"))\n",
        "\n",
        "# Jit the learn function for some extra speed\n",
        "random_policy_search_learn_jit = jax.jit(random_policy_search_learn)\n",
        "\n",
        "initial_weights = np.array([1,1,1,1], \"float32\")\n",
        "initial_params = RandomPolicySearchParams(initial_weights, initial_weights)\n",
        "\n",
        "memory = AverageEpisodeReturnBuffer(num_episodes_to_store=20)\n",
        "\n",
        "episode_return, evaluator_episode_returns = run_environment_loop(\n",
        "                                        rng, \n",
        "                                        env, \n",
        "                                        initial_params, \n",
        "                                        random_policy_search_select_action_jit, \n",
        "                                        None, # no actor state\n",
        "                                        random_policy_search_learn_jit, \n",
        "                                        initial_learner_state, \n",
        "                                        memory, \n",
        "                                        num_episodes=2001\n",
        "                                    )\n",
        "\n",
        "# Plot graph of evaluator episode returns\n",
        "plt.plot(evaluator_episode_returns)\n",
        "plt.title(\"Random Search Evaluator Episode Return\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TG10FG6uS05A"
      },
      "source": [
        "Hopefully you found a set of optimal parameters on CartPole. You will know you have, if the evaluator episode return eventually reaches 500. If you didn't find optimal parameters, try running the environment loop again, you were probably just a little unlucky. That is the big limitation with random policy search after all, if you are unlucky you might not (randomly) stumble on the optimal policy.\n",
        "\n",
        "So, in random policy search there is very little (if any) real learning going on. Next, lets look to implementing a simple RL algorithm instead, that can use its experience to guide the learning process, rather than just randomly sampling new weights. Hopefully this will help us more reliably find an optimal policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEnSjZVESrxc"
      },
      "source": [
        "## Section 3: Policy Gradients (PG)\n",
        "As discussed, the goal in RL is to find a policy which maximise the expected cummulative reward (return) the agent receives from the environment. We can write the expected return of a policy as:\n",
        "\n",
        "$J(\\pi_\\theta)=\\mathrm{E}_{\\tau\\sim\\pi_\\theta}\\ [R(\\tau)]$,\n",
        "\n",
        "where $\\pi_\\theta$ is a policy parametrised by $\\theta$, $\\mathrm{E}$ means *expectation*, $\\tau$ is shorthand for \"*episode*\", $\\tau\\sim\\pi_\\theta$ is shorthand for \"*episodes sampled using the policy* $\\pi_\\theta$\", and $R(\\tau)$ is the return of episode $\\tau$.\n",
        "\n",
        "Then, the goal in RL is to find the parameters $\\theta$ that maximise the function $J(\\pi_\\theta)$. One way to find these parameters is to perform gradient ascent on $J(\\pi_\\theta)$ with respect to the parameters $\\theta$: \n",
        "\n",
        "$\\theta_{k+1}=\\theta_k + \\alpha \\nabla J(\\pi_\\theta)|_{\\theta_{k}}$,\n",
        "\n",
        "where $\\nabla J(\\pi_\\theta)|_{\\theta_{k}}$ is the gradient of the expected return with respect to the policy parameters $\\theta_k$ and $\\alpha$ is the step size. This quantity, $\\nabla J(\\pi_\\theta)$, is also called the **policy gradient** and is very important in RL. If we can comput the policy gradient, theat we will have a means by which to directly optimise our policy.\n",
        "\n",
        "As it turns out, there is a way for us to compute the policy gradient and the mathematical derivation can be found [here](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html). But for this tutorial we will ommit the derivation and just give you the result:\n",
        "\n",
        "\n",
        "$\\nabla_{\\theta} J(\\pi_{\\theta})=\\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}[\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t} \\mid s_{t}) R(\\tau)]$\n",
        "\n",
        "Informaly, the policy gradient is equal to the gradient of the log of the probability of the action chosen multiplied by the return of the episode in which the action was taken.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTnTzgtSuy-y"
      },
      "source": [
        "### REINFORCE\n",
        "REINFORCE is a simple RL algorithm that uses the policy gradient to find the optimal policy by increasing the probability of choosing actions that tend to lead to high return episodes.\n",
        "\n",
        "**Exercise 10:** Implement a function that takes the probability of an action and the return of the episode the action was taken in and returns the log of the probability multiplied by the return. Make sure you use jax so that we can jit the function.\n",
        "\n",
        "**Usefull functions:**\n",
        "*   [Jax numpy log](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.log.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "bJObUsoUrOyV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weighted log prob: None\n"
          ]
        }
      ],
      "source": [
        "def compute_weighted_log_prob(action_prob, episode_return):\n",
        "    weighted_log_prob = None # you will need to overwrite this\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return weighted_log_prob\n",
        "\n",
        "# TEST: the result should be -22.314354\n",
        "action_prob = 0.8\n",
        "episode_return = 100\n",
        "print(\"Weighted log prob:\", compute_weighted_log_prob(action_prob, episode_return))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "cellView": "form",
        "id": "x7dTAlCdrJkf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weighted log prob: -22.314354\n"
          ]
        }
      ],
      "source": [
        "#@title Exercise 10 solution\n",
        "\n",
        "def compute_weighted_log_prob(action_prob, episode_return):\n",
        "    weighted_log_prob = None # you will need to overwrite this\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    weighted_log_prob = jnp.log(action_prob) * episode_return\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return weighted_log_prob\n",
        "\n",
        "# TEST\n",
        "action_prob = 0.8\n",
        "episode_return = 100\n",
        "print(\"Weighted log prob:\", compute_weighted_log_prob(action_prob, episode_return))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmgW9UJ3tIpl"
      },
      "source": [
        "### Rewards-to-go\n",
        "The gradient of the log of the actions probability, weighted by the return of the episode will tend to push up the probability of actions that were in episodes whith high return, regardless of where in the episode the action was taken. This does not really make much sense because an action near the end of an episode may be reinforced because lots of reward was collected earlier on in the episode, before the action was taken. RL agents should really only reinforce actions on the basis of their *consequences*. Rewards obtained before taking an action have no bearing on how good that action was: only rewards that come after. The cummulative rewards received after an action was taken is called the **rewards-to-go** and can be computed as:\n",
        "\n",
        "$\\hat{R}_t=\\sum_t^Tr_t$\n",
        "\n",
        "Compare the rewards-to-go with the episode return:\n",
        "\n",
        "$R(\\tau)=\\sum_{t=0}^Tr_t$\n",
        "\n",
        "Thus, the policy gradient with rewards-to-go is given by:\n",
        "\n",
        "$\\nabla_{\\theta} J(\\pi_{\\theta})=\\underset{\\tau \\sim \\pi_{\\theta}}{\\mathrm{E}}[\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_{t} \\mid s_{t}) \\hat{R}_t]$\n",
        "\n",
        "**Exercise 11:** Implement a function that takes a list of all the rewards obtained in an episode and computes the rewards-to-go. Don't worry about using jax in this function. You can use regular Python operations like `for-loops`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "nV1Hww8E3dUJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rewards-to-go: []\n"
          ]
        }
      ],
      "source": [
        "# Implement reward to go\n",
        "def rewards_to_go(rewards):\n",
        "    \"\"\"\n",
        "    This function should take a list of rewards as input and \n",
        "    compute the rewards-to-go for each timestep.\n",
        "    \n",
        "    Arguments:\n",
        "        rewards[t] is the reward at time step t.\n",
        "\n",
        "    Returns:\n",
        "        rewards_to_go[t] should be the reward-to-go at timestep t.\n",
        "    \"\"\"\n",
        "\n",
        "    rewards_to_go = []\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return rewards_to_go\n",
        "\n",
        "# TEST: The result should be [10, 9, 7, 4]\n",
        "rewards = np.array([1,2,3,4])\n",
        "print(\"Rewards-to-go:\", rewards_to_go(rewards))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "C2qy2F3xRjM3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rewards-to-go: [10, 9, 7, 4]\n"
          ]
        }
      ],
      "source": [
        "#@title Exercise 11 solution\n",
        "\n",
        "def rewards_to_go(rewards):\n",
        "    rewards_to_go = []\n",
        "    for i in range(len(rewards)):\n",
        "        r2g = 0\n",
        "        for j in range(i, len(rewards)):\n",
        "            r2g += rewards[j]\n",
        "        rewards_to_go.append(r2g)\n",
        "    return rewards_to_go\n",
        "\n",
        "# TEST: The result should be [10, 9, 7, 4]\n",
        "rewards = np.array([1,2,3,4])\n",
        "print(\"Rewards-to-go:\", rewards_to_go(rewards))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "7IsJg3PxVV2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rewards-to-go: [10  9  7  4]\n"
          ]
        }
      ],
      "source": [
        "# Faster rewards to go calculation using numpy\n",
        "def rewards_to_go(rewards):\n",
        "    return np.flip(np.cumsum(np.flip(rewards)))\n",
        "\n",
        "# TEST: The result should be [10, 9, 7, 4]\n",
        "rewards = np.array([1,2,3,4])\n",
        "print(\"Rewards-to-go:\", rewards_to_go(rewards))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IboxN9MS65i5"
      },
      "source": [
        "Next we will need to make a new agent memory to store the rewards-to-go $R_t$ along with the observation $o_t$ and action $a_t$ at every timestep."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "xhS4V6auRjM3"
      },
      "outputs": [],
      "source": [
        "# Now we need a new episode memory buffer\n",
        "\n",
        "EpisodeRewardsToGoMemory = collections.namedtuple(\"AverageEpisodeReturnMemory\", [\"obs\", \"action\", \"reward_to_go\"])\n",
        "\n",
        "class EpisodeRewardsToGoBuffer:\n",
        "\n",
        "    def __init__(self, num_transitions_to_store=500, batch_size=500):\n",
        "        self.batch_size = batch_size\n",
        "        self.memory_buffer = collections.deque(maxlen=num_transitions_to_store)\n",
        "        self.current_episode_transition_buffer = []\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.current_episode_transition_buffer.append(transition)\n",
        "\n",
        "        if transition.done:\n",
        "\n",
        "            episode_rewards = []\n",
        "            for t in self.current_episode_transition_buffer:\n",
        "                episode_rewards.append(t.reward)\n",
        "\n",
        "            r2g = rewards_to_go(episode_rewards)\n",
        "\n",
        "            for i, t in enumerate(self.current_episode_transition_buffer):\n",
        "                memory = EpisodeRewardsToGoMemory(t.obs, t.action, r2g[i])\n",
        "                self.memory_buffer.append(memory)\n",
        "\n",
        "            # Reset episode buffer\n",
        "            self.current_episode_transition_buffer = []\n",
        "\n",
        "\n",
        "    def is_ready(self):\n",
        "        return len(self.memory_buffer) >= self.batch_size\n",
        "\n",
        "    def sample(self):\n",
        "        random_memory_sample = random.sample(self.memory_buffer, self.batch_size)\n",
        "\n",
        "        obs_batch, action_batch, reward_to_go_batch = zip(*random_memory_sample)\n",
        "\n",
        "        return EpisodeRewardsToGoMemory(\n",
        "            np.stack(obs_batch).astype(\"float32\"), \n",
        "            np.asarray(action_batch).astype(\"int32\"), \n",
        "            np.asarray(reward_to_go_batch).astype(\"int32\")\n",
        "        )\n",
        "\n",
        "\n",
        "# Instantiate Memory\n",
        "REINFORCE_memory = EpisodeRewardsToGoBuffer(num_transitions_to_store=512, batch_size=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idkav_aSYXvz"
      },
      "source": [
        "### Policy neural network\n",
        "Next, we need to aproximate the policy using a simple neural network. Our policy neural network will have an input layer that takes the observation as input and passes it through two hidden layers and then outputs one scalar value for each of the possible actions. So, in CartPole the output layer will have size `2`.\n",
        "\n",
        "[Haiku](https://github.com/deepmind/dm-haiku) is a library for implementing neural networks is Jax. Below we have implemented a simple function to make the policy network for you. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "j2XO7VkORjM4"
      },
      "outputs": [],
      "source": [
        "def make_policy_network(num_actions: int, layers=[10, 10]) -> hk.Transformed:\n",
        "  \"\"\"Factory for a simple MLP network for the policy.\"\"\"\n",
        "\n",
        "  def policy_network(obs):\n",
        "    network = hk.Sequential(\n",
        "        [\n",
        "            hk.Flatten(),\n",
        "            hk.nets.MLP(layers + [num_actions])\n",
        "        ]\n",
        "    )\n",
        "    return network(obs)\n",
        "\n",
        "  return hk.without_apply_rng(hk.transform(policy_network))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GR2y8FjaG-G"
      },
      "source": [
        "Haiku networks have two important functions you need to know about. The first is the `<network>.init(<rng>, <input>)`, which returns a set of random initial parameters. The second method is the `<network>.apply(<params>, <input>)` which passes an input through the network using the set of parameters provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "fJrn9o-Vatkw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial params: dict_keys(['mlp/~/linear_0', 'mlp/~/linear_1', 'mlp/~/linear_2'])\n",
            "Policy network output: [0.08166866 1.0197883 ]\n"
          ]
        }
      ],
      "source": [
        "# Example\n",
        "POLICY_NETWORK = make_policy_network(num_actions=2, layers=[20,20])\n",
        "random_key = next(rng) # get next random key\n",
        "dummy_obs = np.array([1,1,1,1], \"float32\")\n",
        "\n",
        "REINFORCE_params = POLICY_NETWORK.init(random_key, dummy_obs)\n",
        "print(\"Initial params:\", REINFORCE_params.keys())\n",
        "\n",
        "output = POLICY_NETWORK.apply(REINFORCE_params, dummy_obs)\n",
        "print(\"Policy network output:\", output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlouUBvoeunz"
      },
      "source": [
        "The outputs of our policy network are [logits](https://qr.ae/pv4YTe). To convert this into a probability distribution over actions we pass the logits to the [softmax](https://en.wikipedia.org/wiki/Softmax_function) function (more on this later).\n",
        "\n",
        "### Action selection\n",
        "\n",
        "**Exercise 12:** Complete the function below which takes a vector of logits and randomly samples an action. \n",
        "\n",
        "**Useful functions:**\n",
        "*   [Jax random categorical](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.categorical.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "m3Z8DxUmeOGJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action: None\n",
            "Action: None\n",
            "Action: None\n",
            "Action: None\n",
            "Action: None\n",
            "Action: None\n",
            "Action: None\n",
            "Action: None\n",
            "Action: None\n",
            "Action: None\n"
          ]
        }
      ],
      "source": [
        "def sample_action(random_key, logits):\n",
        "    action = None # you will need to overwrite this\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "\n",
        "\n",
        "    # END YOUR code\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "for i in range(10):\n",
        "  print(\"Action:\", sample_action(next(rng), np.array([0, 1], \"float32\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "cellView": "form",
        "id": "2huSgyukg1N4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action: 0\n",
            "Action: 0\n",
            "Action: 1\n",
            "Action: 0\n",
            "Action: 1\n",
            "Action: 1\n",
            "Action: 1\n",
            "Action: 1\n",
            "Action: 1\n",
            "Action: 0\n"
          ]
        }
      ],
      "source": [
        "#@title Exercise 12 solution\n",
        "\n",
        "def sample_action(random_key, logits):\n",
        "    action = None # you will need to overwrite this\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "    action = jax.random.categorical(random_key, logits)\n",
        "    # END YOUR code\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST: \n",
        "for i in range(10):\n",
        "  print(\"Action:\", sample_action(next(rng), np.array([0, 1], \"float32\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn2yzwDrhmUI"
      },
      "source": [
        "Notice in the tests that the actions are randomly sampled. Ofcourse the action with the higher probability will be chose more often because, but there is always a chance the action with the lower probability will be chosen. This is actually desirable in RL because it mean the agent will always try new things in the environment. We call this **exploring**. Exploring is important because it helps the agent discover new, possibly better, strategies in the environment. When an agent chooses the best possible action (given its current knowledge) we say the agent is being **greedy**.\n",
        "\n",
        "**Exercise 13:** Complete the function below which takes a vector of logits and returns the greedy action. \n",
        "\n",
        "**Useful functions:**\n",
        "*   [Jax numpy argmax](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.argmax.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "XG_4qq6RjLCg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action: None\n",
            "Action: None\n",
            "Action: None\n",
            "Action: None\n",
            "Action: None\n",
            "Action: None\n",
            "Action: None\n",
            "Action: None\n",
            "Action: None\n",
            "Action: None\n"
          ]
        }
      ],
      "source": [
        "def greedy_action(logits):\n",
        "    action = None # you will need to overwrite this\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "for i in range(10):\n",
        "    print(\"Action:\", greedy_action(np.array([0, 1], \"float32\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "cellView": "form",
        "id": "r2sa7wqFjHb-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action: 1\n",
            "Action: 1\n",
            "Action: 1\n",
            "Action: 1\n",
            "Action: 1\n",
            "Action: 1\n",
            "Action: 1\n",
            "Action: 1\n",
            "Action: 1\n",
            "Action: 1\n"
          ]
        }
      ],
      "source": [
        "#@title Exercise 13 solution\n",
        "\n",
        "def greedy_action(logits):\n",
        "    action = None # you will need to overwrite this\n",
        "\n",
        "    # YOUR CODE\n",
        "    action = jnp.argmax(logits)\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "for i in range(10):\n",
        "    print(\"Action:\", greedy_action(np.array([0, 1], \"float32\")))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MffEyZUjyaF"
      },
      "source": [
        "Notice that the greed action selector always chooses the same action, namely the one with the highest probability (or equivalently the largets logit). Next, we will implement the REINFORCE `select_action` function. The function passes the observation through the policy neural network to get loggits and then uses thos to chose an action. If `evaluation` is `True`, the greedy action will be chosen. Otherwise, the action is sampled from the action probability distribution given by the logits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "jP5UH87VRjM4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action: 1\n"
          ]
        }
      ],
      "source": [
        "def REINFORCE_select_action(key, params, actor_state, obs, evaluation=False):\n",
        "    obs = jnp.expand_dims(obs, axis=0) # add dummy batch dim\n",
        "    logits = POLICY_NETWORK.apply(params, obs)[0] # remove batch dim\n",
        "\n",
        "    sampled_action = sample_action(key, logits)\n",
        "\n",
        "    best_action = greedy_action(logits)\n",
        "\n",
        "    action = jax.lax.select(\n",
        "        evaluation,\n",
        "        best_action,\n",
        "        sampled_action\n",
        "    )\n",
        "    \n",
        "    return action, actor_state\n",
        "\n",
        "# TEST\n",
        "action, actor_state = REINFORCE_select_action(\n",
        "    key=next(rng),\n",
        "    params=REINFORCE_params, # we instantiated this earlier\n",
        "    actor_state=None, # not used\n",
        "    obs=np.array([1,1,1,1], \"float32\") # dummy obs\n",
        ")\n",
        "\n",
        "print(\"Action:\", action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDhTH3culwqo"
      },
      "source": [
        "Now that we finished the REINFORCE action selection function, all we have left to do is make a REINFORCE learn function. The learn function should use the `weighted_log_prob` function we made earlier to compute the policy gradient and apply the updates to our neural network.\n",
        "\n",
        "### Network Optimiser\n",
        "\n",
        "To apply updates to our neural network we will use a Jax library called [Optax](https://github.com/deepmind/optax). Optax has an implementation of the [Adam optimizer](https://www.geeksforgeeks.org/intuition-of-adam-optimizer/) which we can use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "pxXINlMHP5Ic"
      },
      "outputs": [],
      "source": [
        "REINFORCE_OPTIMIZER = optax.adam(1e-3)\n",
        "REINFORCE_optim_state = REINFORCE_OPTIMIZER.init(REINFORCE_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36ALCJESQJ8e"
      },
      "source": [
        "### Policy gradient loss\n",
        "\n",
        "**Exercise 14:** Complete the `pg_loss` function below.\n",
        "\n",
        "**Useful methods:**\n",
        "*   [Jax softmax](https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.softmax.html)\n",
        "*   [Jax one-hot vector](https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.one_hot.html)\n",
        "*   [Jax dot product](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.dot.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "9sUKkqx0RjM4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Policy gradient loss: inf\n"
          ]
        }
      ],
      "source": [
        "def pg_loss(action, logits, reward_to_go):\n",
        "    chosen_action_prob = 0.0 # you will need to overwrite this\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    # HINT all_action_probs = ... (convert logits into probs)\n",
        "\n",
        "    # HINT extract the prob of the desired action.\n",
        "    # One way to achieve this is to use a one-hot vector and a dot product...?\n",
        "\n",
        "    # END YOUR CODE\n",
        "    weighted_log_prob = compute_weighted_log_prob(\n",
        "                            chosen_action_prob, \n",
        "                            reward_to_go\n",
        "                        )\n",
        "    \n",
        "    loss = - weighted_log_prob # negative because we want gradient `ascent`\n",
        "    \n",
        "    return loss\n",
        "\n",
        "# TEST \n",
        "print(\"Policy gradient loss:\", pg_loss(0, np.array([0,1], \"float32\"), 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "cellView": "form",
        "id": "1HAxSFU0qBVo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Policy gradient loss: 13.132616\n"
          ]
        }
      ],
      "source": [
        "#@title Exercise 14 solution\n",
        "\n",
        "def pg_loss(action, logits, reward_to_go):\n",
        "\n",
        "    # YOUR CODE\n",
        "    \n",
        "    all_action_probs = jax.nn.softmax(logits)\n",
        "    action_mask = jax.nn.one_hot(action, logits.shape[0])\n",
        "    chosen_action_prob = jnp.dot(all_action_probs, action_mask)\n",
        "\n",
        "    # END YOUR CODE\n",
        "    \n",
        "    weighted_log_prob = compute_weighted_log_prob(\n",
        "                            chosen_action_prob, \n",
        "                            reward_to_go\n",
        "                        )\n",
        "    \n",
        "    loss = - weighted_log_prob \n",
        "    \n",
        "    return loss\n",
        "\n",
        "# TEST \n",
        "print(\"Policy gradient loss:\", pg_loss(0, np.array([0,1], \"float32\"), 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzuqx1jJrwVx"
      },
      "source": [
        "Now, when we do a policy gradient update step we are going to want to do it using a batch of experience, rather than just a single experience like above. We can use Jax's [vmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.vmap.html#jax.vmap) function to easily make our `pg_loss` function work on a batch of experience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "3yq4naLURjM4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PG loss on batch: 2.4175563\n"
          ]
        }
      ],
      "source": [
        "def batched_pg_loss(params, obs_batch, action_batch, reward_to_go_batch):\n",
        "    logits_batch = POLICY_NETWORK.apply(params, obs_batch) # network we made earlier\n",
        "    pg_loss_batch = jax.vmap(pg_loss)(action_batch, logits_batch, reward_to_go_batch) # add batch\n",
        "    mean_pg_loss = jnp.mean(pg_loss_batch)\n",
        "    return mean_pg_loss\n",
        "\n",
        "# TEST\n",
        "obs_batch = np.array([[1,0,0,1],[1,0,0,1],[1,0,0,1]])\n",
        "actions_batch = np.array([1,0,0])\n",
        "rew2go_batch = np.array([2.3, 4.3, 2.1])\n",
        "\n",
        "loss = batched_pg_loss(REINFORCE_params, obs_batch, actions_batch, rew2go_batch)\n",
        "\n",
        "print(\"PG loss on batch:\", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViENrHOALbCw"
      },
      "source": [
        "Now we can make the REINFORCE learn function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "CQr2Uz5ORjM5"
      },
      "outputs": [],
      "source": [
        "REINFORCELearnerState = collections.namedtuple(\"LearnerState\", [\"optim_state\"])\n",
        "\n",
        "def REINFORCE_learn(key, params, learner_state, memory):\n",
        "    \n",
        "    #Get Policy gradient by using `jax.grad()` on `batched_pg_loss`\n",
        "    grad_loss = jax.grad(batched_pg_loss)(params, memory.obs, memory.action, memory.reward_to_go)\n",
        "\n",
        "    # Get param updates using gradient and optimizer\n",
        "    updates, new_optim_state = REINFORCE_OPTIMIZER.update(grad_loss, learner_state.optim_state)\n",
        "\n",
        "    # Apply updates to params\n",
        "    params = optax.apply_updates(params, updates)\n",
        "\n",
        "    return params, REINFORCELearnerState(new_optim_state) # update learner state\n",
        "\n",
        "# Lets jit the learn function and the select action function for some extra speed\n",
        "REINFORCE_learn_jit = jax.jit(REINFORCE_learn)\n",
        "REINFORCE_select_action_jit = jax.jit(REINFORCE_select_action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5an3U2NhRKgG"
      },
      "source": [
        "Now we can train our REINFORCE agent by putting everything together using the environment loop. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "vioIcVGsRjM5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 0\tEpisode Return: 10.0\tAverage Episode Return: 10.0\tEvaluator Episode Return: 11.21875\n",
            "Episode: 100\tEpisode Return: 14.0\tAverage Episode Return: 25.4\tEvaluator Episode Return: 176.8125\n",
            "Episode: 200\tEpisode Return: 30.0\tAverage Episode Return: 107.65\tEvaluator Episode Return: 328.09375\n",
            "Episode: 300\tEpisode Return: 319.0\tAverage Episode Return: 238.75\tEvaluator Episode Return: 368.40625\n",
            "Episode: 400\tEpisode Return: 225.0\tAverage Episode Return: 291.45\tEvaluator Episode Return: 312.5625\n",
            "Episode: 500\tEpisode Return: 216.0\tAverage Episode Return: 297.05\tEvaluator Episode Return: 339.28125\n",
            "Episode: 600\tEpisode Return: 333.0\tAverage Episode Return: 289.15\tEvaluator Episode Return: 395.96875\n",
            "Episode: 700\tEpisode Return: 500.0\tAverage Episode Return: 452.95\tEvaluator Episode Return: 500.0\n",
            "Episode: 800\tEpisode Return: 364.0\tAverage Episode Return: 389.1\tEvaluator Episode Return: 444.21875\n",
            "Episode: 900\tEpisode Return: 500.0\tAverage Episode Return: 497.55\tEvaluator Episode Return: 500.0\n",
            "Episode: 1000\tEpisode Return: 353.0\tAverage Episode Return: 487.05\tEvaluator Episode Return: 500.0\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "learner_state = REINFORCELearnerState(REINFORCE_optim_state)\n",
        "actor_state = None # not used\n",
        "\n",
        "episode_returns, evaluator_returns = run_environment_loop(rng, env, REINFORCE_params, REINFORCE_select_action_jit , actor_state, \n",
        "    REINFORCE_learn_jit, learner_state, REINFORCE_memory, num_episodes=1001)\n",
        "\n",
        "# Plot the episode returns over time\n",
        "plt.plot(episode_returns)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_HHzFTOc-Qr"
      },
      "source": [
        "## Section 4: Q-Learning\n",
        "Another common aproach to finding an optimal policy in an environment in RL is via Q-learning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF1nOZrUSzhE"
      },
      "source": [
        "### Value functions\n",
        "In Q-learnig the agent learns a function that aproximates the **value** of states and actions. By *value* we mean the return you expect to receive if you start in a particular state $s_t$, take a particular action $a_t$, and then act according to a particular policy $\\pi$ forever after. There are two types of **value functions**:\n",
        "1. The **state value function** which returns the expected value starting from a particular state.\n",
        "\n",
        "    $V_\\pi(s)=\\mathrm{E}_{\\tau\\sim\\pi}\\left[R(\\tau) \\mid s_0=s\\right]$\n",
        "\n",
        "2. The **state-action value function** which returns the expected value of starting from a particular state and choosing a particular action.\n",
        "\n",
        "    $Q_\\pi(s,a)=\\mathrm{E}_{\\tau\\sim\\pi}\\left[R(\\tau) \\mid s_0=s,\\ a_t=a\\right]$\n",
        "\n",
        "We say that the value function $V_\\pi(s)$ or $Q_\\pi(s,a)$ is the **optimal** value function if the policy $\\pi$ is an optimal policy. We denote the optimal value functions as follows:\n",
        "\n",
        "1.   $V_\\ast(s)=max_\\pi \\ \\mathrm{E}_{\\tau\\sim\\pi}\\left[R(\\tau) \\mid s_0=s\\right]$\n",
        "2.   $Q_\\ast(s,a)=max_\\pi \\  \\mathrm{E}_{\\tau\\sim\\pi}\\left[R(\\tau) \\mid s_0=s,\\ a_0=a\\right]$\n",
        "\n",
        "There is an important relationship between the optimal action $a_\\ast$ in a state $s$ and the optimal state-action value function $Q_\\ast$. Namely, the optimal action $a_\\ast$ in state $s$ is equal to the action that maximises the optimal state-action value function. This relationship naturally induces an optimal policy:\n",
        "\n",
        "$\\pi_\\ast(s)=\\mathop{argmax}_a\\ Q_\\ast(s, a)$\n",
        "\n",
        "Thus, if we can learn to aproximate the optimal Q-function, then we naturally have an optimal policy. This is the arpoach taken in RL algorithms that use Q-learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2x2tqvZSihz"
      },
      "source": [
        "### Greedy action selection\n",
        "\n",
        "**Exercise 15:** Lets implement a function that, given a vector of Q-values, returns the action with the largets Q-value (i.e. the greedy action).\n",
        "\n",
        "**Useful methods:**\n",
        "*   [Jax argmax](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.argmax.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Nn9P1YdzTIDU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy action: None\n"
          ]
        }
      ],
      "source": [
        "# Implement a function takes q-values as input and returns the greedy_action\n",
        "def select_greedy_action(q_values):\n",
        "    action = None # you will need to overwrite this\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "q_values = np.array([1, 3])\n",
        "print(\"Greedy action:\", select_greedy_action(q_values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "9GsPv35lRjM6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy action: 1\n"
          ]
        }
      ],
      "source": [
        "#@title Exercise 15 solution\n",
        "\n",
        "\n",
        "def select_greedy_action(q_values):\n",
        "    action = None # you will need to overwrite this\n",
        "    \n",
        "    # YOUR CODE\n",
        "\n",
        "    action = jnp.argmax(q_values)\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "q_values = np.array([1, 3])\n",
        "print(\"Greedy action:\", select_greedy_action(q_values))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q-Network\n",
        "We can use haiku to make a neural network to aproximate the Q-function. The network will take an observation as input and then output a Q-value for each of the avaliable actions. So in the case of CartPole, the output of the network will have size $2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q-Learning params: dict_keys(['mlp/~/linear_0', 'mlp/~/linear_1', 'mlp/~/linear_2'])\n"
          ]
        }
      ],
      "source": [
        "def build_network(num_actions: int, layers=[10, 10]) -> hk.Transformed:\n",
        "  \"\"\"Factory for a simple MLP network for approximating Q-values.\"\"\"\n",
        "\n",
        "  def q_network(obs):\n",
        "    network = hk.Sequential(\n",
        "        [hk.Flatten(),\n",
        "         hk.nets.MLP(layers + [num_actions])])\n",
        "    return network(obs)\n",
        "\n",
        "  return hk.without_apply_rng(hk.transform(q_network))\n",
        "\n",
        "# Initialise Q-network\n",
        "Q_NETWORK = build_network(num_actions=2)\n",
        "dummy_obs = jnp.zeros((1,4), jnp.float32)\n",
        "Q_NETWORK_WEIGHTS = Q_NETWORK.init(next(rng), dummy_obs)\n",
        "print(\"Q-Learning params:\", Q_NETWORK_WEIGHTS.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Bellman Equations\n",
        "The value functions can be written recursivly as:\n",
        "*   $V_{\\pi}(s) = \\underset{\\substack{a \\sim \\pi \\\\ s^{\\prime} \\sim P}}{\\mathrm{E}}\\left[r(s, a)+\\gamma V_{\\pi}\\left(s^{\\prime}\\right)\\right]$\n",
        "*   $Q_{\\pi}(s, a) =\\underset{s^{\\prime} \\sim P}{\\mathrm{E}}\\left[r(s, a)+\\gamma \\underset{a^{\\prime} \\sim \\pi}{\\mathrm{E}}\\left[Q_{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right)\\right]\\right]$\n",
        "\n",
        "Where $s' \\sim P$ is shorthand for for indicating that the next state $s'$ is sampled from the environment’s transition rules $P$. Intuitivly, these equations say the value of state you are in is equal to the reward you expect to get from being there, plus the value of the state you transition to next. The Bellman equations for the optimal value functions are:\n",
        "*   $V_{*}(s) = \\underset{a}{\\mathop{max}} \\underset{s^{\\prime} \\sim P}{\\mathrm{E}}\\left[r(s, a)+\\gamma V^{*}\\left(s^{\\prime}\\right)\\right]$\n",
        "*   $Q_{*}(s, a) =\\underset{a}{\\mathop{max}} \\underset{s^{\\prime} \\sim P}{\\mathrm{E}}\\left[r(s, a)+\\gamma \\underset{a^{\\prime} \\sim \\pi}{\\mathrm{E}}\\left[Q_{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right)\\right]\\right]$\n",
        "\n",
        "\n",
        "\n",
        "For a more indepth discussion of the Bellman Equations, see the [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html) website."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The Bellman Backup\n",
        "To learn to aproximate the optimal Q-value function, we can use the right-hand side of the Bellman equation as an update rule. In other words, suppose we have a Q-function $Q_\\theta$ aproximated using parameters $\\theta$ then we can iterativly update the paramters such that\n",
        "\n",
        "$Q_\\theta(s,a)\\leftarrow r(s, a) + \\underset{a'}{\\max}\\ Q_\\theta(s', a')$.\n",
        "\n",
        "Intuitivly, this says that the aproximation of the Q-value of action $a$ in state $s$ should be updated such that it is closer to being equal to the reward received from the environment $r(s, a)$ plus the value of best possible action in the next state $s'$. We can perform this optimisation by minimising the difference between the left and right-hand side, with respect to the parameters $\\theta$ using gradient descent. We can measure the difference between the two values using the [squared-error](https://en.wikipedia.org/wiki/Mean_squared_error#Loss_function).\n",
        "\n",
        "**Exercise X:** Implement the squared-error function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_squared_error(pred, target):\n",
        "    squared_error = None\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_squared_error(pred, target):\n",
        "    squared_error = None\n",
        "\n",
        "    # YOUR CODE\n",
        "    \n",
        "    squared_error = jnp.square(pred - target)\n",
        "\n",
        "    # END YOUR CODE\n",
        "    return squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The right-hand side of the Bellman equation is sometimes called the **Bellman target**.\n",
        "\n",
        "**Exercise X:** Implement a function that computes the Bellman target (right-hand side of the equation). If the timetep is the last timestep, then the Bellman target should just be equal to the reward, with not extra value at the end.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bellman target\n",
        "def compute_bellman_target(reward, done, next_q_values):\n",
        "    \"\"\"A function to compute the bellman target.\n",
        "    \n",
        "    Args:\n",
        "        reward: a scalar reward.\n",
        "        done: a scalar of value either 1.0 or 0.0, indicating if the transition is a terminal one.\n",
        "        next_q_values: a vector of q_values for the next state. One for each action.\n",
        "    Returns:\n",
        "        A scalar equal to the bellman target.\n",
        "    \n",
        "    \"\"\"\n",
        "    bellman_target = None # you will need to override this\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return bellman_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bellman target\n",
        "def compute_bellman_target(reward, done, next_q_values):\n",
        "    \"\"\"A function to compute the bellman target.\n",
        "    \n",
        "    Args:\n",
        "        reward: a scalar reward.\n",
        "        done: a scalar of value either 1.0 or 0.0, indicating if the transition is a terminal one.\n",
        "        next_q_values: a vector of q_values for the next state. One for each action.\n",
        "    Returns:\n",
        "        A scalar equal to the bellman target.\n",
        "    \n",
        "    \"\"\"\n",
        "    bellman_target = None # you will need to override this\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    bellman_target = reward + (1.0 - done) * jnp.max(next_q_values)\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return bellman_target\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now combine these two functions to compute the loss for Q-learning. The Q-learning loss is equal to the squared difference between the predicted Q-value of an action and its corresponding Bellman target.\n",
        "\n",
        "**Exercise X:** Implement the Q-learning loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "def q_learning_loss(q_values, action, reward, done, next_q_values):\n",
        "    \"\"\"Implementation of the Q-learning loss.T\n",
        "    \n",
        "    Args:\n",
        "        q_values: a vector of Q-values, one for each action.\n",
        "        action: an integer, giving the action that was chosen. q_values[action] is the value of the chose action.\n",
        "        done: is a scalar that indicates if this is a terminal transition.\n",
        "        next_q_values: a vector of Q-values in the next state.\n",
        "    Returns:\n",
        "        The squared difference between the q_value of the chosen action and the bellman target.\n",
        "    \"\"\"\n",
        "    squared_error = None\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    # END YOUR CODE\n",
        "    \n",
        "    return squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "def q_learning_loss(q_values, action, reward, done, next_q_values):\n",
        "    \"\"\"Implementation of the Q-learning loss.T\n",
        "    \n",
        "    Args:\n",
        "        q_values: a vector of Q-values, one for each action.\n",
        "        action: an integer, giving the action that was chosen. q_values[action] is the value of the chose action.\n",
        "        done: is a scalar that indicates if this is a terminal transition.\n",
        "        next_q_values: a vector of Q-values in the next state.\n",
        "    Returns:\n",
        "        The squared difference between the q_value of the chosen action and the bellman target.\n",
        "    \"\"\"\n",
        "    squared_error = None\n",
        "\n",
        "    # YOUR CODE\n",
        "    action_q_value = q_values[action]\n",
        "    bellman_target = compute_bellman_target(reward, done, next_q_values)\n",
        "    squared_error = compute_squared_error(action_q_value, bellman_target)\n",
        "    # END YOUR CODE\n",
        "    \n",
        "    return squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use `jax.vmap` to modify the `q_learning_loss` function so that it accepts batches of transitions. In adition, we will compute the Q-values by passing the observations through the `Q_NETWORK`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "def batched_q_learning_loss(params, obs, actions, rewards, next_obs, dones):\n",
        "    q_values = Q_NETWORK.apply(params, obs)\n",
        "    next_q_values = Q_NETWORK.apply(params, next_obs)\n",
        "    next_q_values = jax.lax.stop_gradient(next_q_values) # stop the gradients\n",
        "    squared_error = jax.vmap(q_learning_loss)(q_values, actions, rewards, dones, next_q_values)\n",
        "    mean_squared_error = jnp.mean(squared_error)\n",
        "    return mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Target Q-network\n",
        "Unfortunatly, it turns out that this form of the loss function is quite unstable and the Q-learner will often fail to converge on the optimal policy. The instability is there is a simple implementation trick we need to use inorder to get Q-learning to be more stable when using a neural network to aproximate the Q-function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Replay Buffer\n",
        "For Q-learning we will need an agent memory that stores entire transitions: `obs`, `action`, `reward`, `next_obs`, `done`. When we retrieve transitions from the memory, they should be chose randomly. In RL we often call such a module a **replay buffer**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransitionMemory(object):\n",
        "  \"\"\"A simple Python replay buffer.\"\"\"\n",
        "\n",
        "  def __init__(self, max_size=5000, batch_size=256):\n",
        "    self.batch_size = batch_size\n",
        "    self.buffer = collections.deque(maxlen=max_size)\n",
        "\n",
        "  def push(self, transition):\n",
        "\n",
        "      self.buffer.append(\n",
        "          (transition.obs, transition.action, transition.reward, \n",
        "           transition.next_obs, transition.done)\n",
        "      )\n",
        "\n",
        "  \n",
        "  def is_ready(self):\n",
        "    return self.batch_size <= len(self.buffer)\n",
        "\n",
        "  def sample(self):\n",
        "    random_replay_sample = random.sample(self.buffer, self.batch_size)\n",
        "    obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = zip(*random_replay_sample)\n",
        "\n",
        "    return Transition(\n",
        "        np.stack(obs_batch).astype(\"float32\"), \n",
        "        np.asarray(action_batch).astype(\"int32\"), \n",
        "        np.asarray(reward_batch).astype(\"float32\"), \n",
        "        np.stack(next_obs_batch).astype(\"float32\"), \n",
        "        np.asarray(done_batch).astype(\"float32\")\n",
        "    )\n",
        "\n",
        "Q_LEARNING_MEMORY = TransitionMemory(max_size=5000, batch_size=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q-learn function\n",
        "We now have everything we need to implement the `q_learn_step` function which takes some batch of transitions and does a step of Q-learning to update the network paramters. Once again, we will use optax to update our neural network parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {},
      "outputs": [],
      "source": [
        "QLearnerState = collections.namedtuple(\"LearnerState\", [\"optim_state\"])\n",
        "\n",
        "# Initialise Q-network optimizer\n",
        "Q_LEARN_OPTIMIZER = optax.adam(1e-3) # learning rate = 0.001\n",
        "Q_LEARN_OPTIM_STATE = Q_LEARN_OPTIMIZER.init(Q_NETWORK_WEIGHTS)\n",
        "Q_LEARNING_LEARN_STATE = QLearnerState(Q_LEARN_OPTIM_STATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "def q_learner_step(rng, params, learner_state, memory):\n",
        "    grad_loss = jax.grad(batched_q_learning_loss)(params, memory.obs, \n",
        "                                            memory.action, memory.reward, \n",
        "                                            memory.next_obs, memory.done,\n",
        "                                            )\n",
        "\n",
        "    updates, opt_state = Q_LEARN_OPTIMIZER.update(grad_loss, learner_state.optim_state)\n",
        "\n",
        "    params = optax.apply_updates(params, updates)\n",
        "\n",
        "    # Increment learner step counter\n",
        "    learner_state = QLearnerState(opt_state)\n",
        "\n",
        "    return params, learner_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbHk03VVUHAV"
      },
      "source": [
        "### Random exploration\n",
        "We almost have everything we need for a functioning Q-learning agent. But one problem is that if we always chose the action with the highest Q-value as our policy then the agent's policy will be completly deterministic. This means the agent will always chose the same strategy. At the start of training this can pose a problem because the Q-network will be very inaccurate (i.e. a bad aproximation of the true Q-function) and so the agent will consistently choose suboptimal actions. Moreover, the agent will never deviate from its sub-optimal strategy and so never explores new, alternative actions. As a result, the Q-network remains inaccurate. Ideally, the agent should try out many different strategies so that it can observe the outcomes (rewards) of its actions in different states and so improve its approximation of the Q-function.\n",
        "\n",
        "One easy way to ensure that the agent tries out many different actions is to let it peiodically chose some random actions, instead of just the greedy (best) action.\n",
        "\n",
        "**Exercise 16:** Implement a function that, given the number of possible actions, returns a random action.\n",
        "\n",
        "**Useful methods:**\n",
        "\n",
        "*   [Jax random int](https://jax.readthedocs.io/en/latest/_autosummary/jax.random.randint.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "uUKkpMLXUtko"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random action number 0: None\n",
            "Random action number 1: None\n",
            "Random action number 2: None\n",
            "Random action number 3: None\n",
            "Random action number 4: None\n",
            "Random action number 5: None\n",
            "Random action number 6: None\n",
            "Random action number 7: None\n",
            "Random action number 8: None\n",
            "Random action number 9: None\n"
          ]
        }
      ],
      "source": [
        "def select_random_action(key, num_actions):\n",
        "    action = None\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "for i in range(10):\n",
        "  print(f\"Random action number {i}: {select_random_action(next(rng), 2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "cellView": "form",
        "id": "lrVadhcwRjM6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random action number 0: 0\n",
            "Random action number 1: 1\n",
            "Random action number 2: 1\n",
            "Random action number 3: 0\n",
            "Random action number 4: 0\n",
            "Random action number 5: 1\n",
            "Random action number 6: 1\n",
            "Random action number 7: 1\n",
            "Random action number 8: 0\n",
            "Random action number 9: 1\n"
          ]
        }
      ],
      "source": [
        "#@title Exercise 16 solution\n",
        "\n",
        "def select_random_action(key, num_actions):\n",
        "    action = None\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "    action = jax.random.randint(\n",
        "        key, \n",
        "        shape=(1,), \n",
        "        minval=0, \n",
        "        maxval=num_actions\n",
        "    )[0] # important to take zeroth element\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "for i in range(10):\n",
        "  print(f\"Random action number {i}: {select_random_action(next(rng), 2)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-kKDFT6XU6y"
      },
      "source": [
        "### Epsilon-greedy action selection\n",
        "At the start of training, when the accuracy of the Q-network is low, it is worthwhile for the agent to mostly take random actions. But overtime as the accuracy of the Q-network improves the agent should start taking fewer random actions and instead start chosing the greedy actions with respect to the Q-values. In RL we often call the ratio of random to greedy actions **epsilon**. Epsilon is usually a decimal value in the interval $[0,1]$, where for example $epsilon=0.4$ means that the agent choses a random action 40% of the time and the greedy action 60% of the time. Its common in RL to linearly decrease the value of epsilon ove time so that the agent becomes increasingly greedy as the accuracy of its Q-network improves through learning.\n",
        "\n",
        "\n",
        "**Exercise 17:** Implement a function that takes the number of timesteps as input and returns the current epsilon value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epsilon after 10 timesteps: None\n",
            "Epsilon after 10 010 timesteps: None\n"
          ]
        }
      ],
      "source": [
        "EPSILON_DECAY_TIMESTEPS = 1_000\n",
        "EPSILON_MIN = 0.1 # 10%\n",
        "\n",
        "def get_epsilon(num_timesteps):\n",
        "    epsilon = None\n",
        "\n",
        "    # YOUR CODE\n",
        "\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return epsilon\n",
        "\n",
        "# TEST\n",
        "\n",
        "print(\"Epsilon after 10 timesteps:\", get_epsilon(10))\n",
        "print(\"Epsilon after 10 010 timesteps:\", get_epsilon(5_010))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "QTpezAeUYFR7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epsilon after 10 timesteps: 0.99\n",
            "Epsilon after 10 010 timesteps: 0.1\n"
          ]
        }
      ],
      "source": [
        "#@title Exercise 17 solution\n",
        "\n",
        "def get_epsilon(num_timesteps):\n",
        "\n",
        "    # YOUR CODE\n",
        "    epsilon = 1.0 - num_timesteps / EPSILON_DECAY_TIMESTEPS\n",
        "\n",
        "    epsilon = jax.lax.select(\n",
        "        epsilon < EPSILON_MIN,\n",
        "        EPSILON_MIN,\n",
        "        epsilon\n",
        "    )\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return epsilon\n",
        "\n",
        "# TEST\n",
        "\n",
        "print(\"Epsilon after 10 timesteps:\", get_epsilon(10))\n",
        "print(\"Epsilon after 10 010 timesteps:\", get_epsilon(5_010))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t56oo58TVQ_s"
      },
      "source": [
        "**Exercise X:** Now lets put these functions together to do epsilon-greedy action selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "NlQx8K4vKUXj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy actions: None None None None None None None None None None \n",
            "Random actions: None None None None None None None None None None "
          ]
        }
      ],
      "source": [
        "def select_epsilon_greedy_action(key, q_values, num_timesteps):\n",
        "    action = None\n",
        "    \n",
        "    # YOUR CODE HERE\n",
        "\n",
        "\n",
        "    # END YOUR CODE\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "dummy_q_values = jnp.array([0,1], jnp.float32)\n",
        "num_timesteps = 5010 # very greedy\n",
        "print(\"Greedy actions:\", end=\" \")\n",
        "for i in range(10):\n",
        "    print(select_epsilon_greedy_action(next(rng), dummy_q_values, num_timesteps), end=\" \")\n",
        "print()\n",
        "\n",
        "num_timesteps = 0 # completly random\n",
        "print(\"Random actions:\", end=\" \")\n",
        "for i in range(10):\n",
        "    print(select_epsilon_greedy_action(next(rng), dummy_q_values, num_timesteps), end=\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "bXDRQhORRjM6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy actions: 1 1 0 1 1 1 1 1 0 1 \n",
            "Random actions: 0 1 0 1 0 1 0 0 1 0 "
          ]
        }
      ],
      "source": [
        "#@title Exercise 18 solution\n",
        "\n",
        "# Now make a function that takes an epsilon-greedy action\n",
        "\n",
        "def select_epsilon_greedy_action(key, q_values, num_timesteps):\n",
        "\n",
        "    epsilon = get_epsilon(num_timesteps)\n",
        "\n",
        "    should_explore = jax.random.uniform(key, (1,))[0] < epsilon\n",
        "\n",
        "    num_actions = len(q_values)\n",
        "\n",
        "    action = jax.lax.select(\n",
        "        should_explore,\n",
        "        select_random_action(key, num_actions), \n",
        "        select_greedy_action(q_values)\n",
        "    )\n",
        "\n",
        "    return action\n",
        "\n",
        "# TEST\n",
        "dummy_q_values = jnp.array([0,1], jnp.float32)\n",
        "num_timesteps = 5010 # very greedy\n",
        "print(\"Greedy actions:\", end=\" \")\n",
        "for i in range(10):\n",
        "    print(select_epsilon_greedy_action(next(rng), dummy_q_values, num_timesteps), end=\" \")\n",
        "print()\n",
        "\n",
        "num_timesteps = 0 # completly random\n",
        "print(\"Random actions:\", end=\" \")\n",
        "for i in range(10):\n",
        "    print(select_epsilon_greedy_action(next(rng), dummy_q_values, num_timesteps), end=\" \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5W23MnobN9x"
      },
      "source": [
        "### Q-learning select action\n",
        "\n",
        "We now have everything we need to make the `q_learning_select_action` function. We will use the `actor_state` to store a counter which keeps track of the current number of timesteps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "81TysLc0RjM6"
      },
      "outputs": [],
      "source": [
        "# Actor state stores the current number of timesteps\n",
        "QActorState = collections.namedtuple(\"ActorState\", [\"count\"])\n",
        "\n",
        "def q_learning_select_action(key, params, actor_state, obs, evaluation=False):\n",
        "    obs = jnp.expand_dims(obs, axis=0) # add dummy batch dim\n",
        "    q_values = Q_NETWORK.apply(params, obs)[0] # remove batch dim\n",
        "\n",
        "    action = select_epsilon_greedy_action(key, q_values, actor_state.count)\n",
        "    greedy_action = select_greedy_action(q_values)\n",
        "\n",
        "    action = jax.lax.select(\n",
        "        evaluation,\n",
        "        greedy_action,\n",
        "        action\n",
        "    )\n",
        "\n",
        "    next_actor_state = QActorState(actor_state.count + 1) # increment timestep counter\n",
        "\n",
        "    return action, next_actor_state\n",
        "\n",
        "Q_LEARNING_ACTOR_STATE = QActorState(0) # counter set to zero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z884-1oNRGEr"
      },
      "source": [
        "### Training\n",
        "We can now put everything together using the agent-environment loop. But first,lets jit the select action function and the learn function for some extra speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "xbdHDbd1RjM8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 0\tEpisode Return: 12.0\tAverage Episode Return: 12.0\tEvaluator Episode Return: 9.28125\n",
            "Episode: 100\tEpisode Return: 9.0\tAverage Episode Return: 20.9\tEvaluator Episode Return: 9.5\n",
            "Episode: 200\tEpisode Return: 24.0\tAverage Episode Return: 25.8\tEvaluator Episode Return: 30.96875\n",
            "Episode: 300\tEpisode Return: 195.0\tAverage Episode Return: 159.1\tEvaluator Episode Return: 264.71875\n",
            "Episode: 400\tEpisode Return: 500.0\tAverage Episode Return: 500.0\tEvaluator Episode Return: 500.0\n",
            "Episode: 500\tEpisode Return: 500.0\tAverage Episode Return: 500.0\tEvaluator Episode Return: 500.0\n",
            "Episode: 600\tEpisode Return: 306.0\tAverage Episode Return: 285.35\tEvaluator Episode Return: 471.09375\n",
            "Episode: 700\tEpisode Return: 500.0\tAverage Episode Return: 461.5\tEvaluator Episode Return: 499.9375\n",
            "Episode: 800\tEpisode Return: 500.0\tAverage Episode Return: 495.75\tEvaluator Episode Return: 500.0\n",
            "Episode: 900\tEpisode Return: 500.0\tAverage Episode Return: 485.95\tEvaluator Episode Return: 500.0\n",
            "Episode: 1000\tEpisode Return: 500.0\tAverage Episode Return: 500.0\tEvaluator Episode Return: 500.0\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABBj0lEQVR4nO2deZwdVZn3f89dekl3J53QnRCykIREIOwY2VEIiwiMyLgBvoKIw4ygqDgqzjgu44j6jjMs78ygCCg6yCKLIPIRCIu4kZAQ1rCFhCSdvbN00nvfe8/7Ry23bt2qW6dOrff28/18+tO3tnNOVd371FO/85znkBACDMMwTGORSboBDMMwTPiwcWcYhmlA2LgzDMM0IGzcGYZhGhA27gzDMA1ILukGAEBXV5eYM2dO0s1gGIapK1asWNErhOh22pYK4z5nzhwsX7486WYwDMPUFUS0zm0byzIMwzANCBt3hmGYBoSNO8MwTAPCxp1hGKYBYePOMAzTgEgZdyJ6h4heJqIXiGi5vm4KET1ORG/p/yfr64mIbiSi1UT0EhEdHeUJMAzDMNX48dxPFUIcKYRYpC9fA+AJIcQCAE/oywDwAQAL9L/LAdwUVmMZhmEYOYLEuZ8H4BT98+0AngbwNX39L4SWS/hZIuokoulCiM1BGsokz/odg7jv+R5kM4TmXAYbdg1i+qRWXP7eeXh+3S5MaWvCgmkdUmU99OImrN66N+IWA9v7RzGpNY/PL56PtuYcRgsl/Pwva9E/XJAuY0pbEy45YQ6ICACwafcQ7lm+AaVSdbrskWIJ2/aM4IpTDpC+FnYeeXkzjpu3D6a0NQHQrtUpB3ZjYku+apsKfUNjuO7xN7F3uIAD920HgdCzaxB7Rwrobm9Gc87Z51vTO4CLjp2NEw7oAgBs2zuMO5duwLqdA9h3YgsyRMholwh7hgsYKRQxNFrE7CkTHMvrHymif2QMg6NFzOtqUz4fKz27hxzPYU3vgFlH78AomrIZTGypbf7W9A5gblcbyLJuc98wZkxuRXdHM3IZwqTWJuwdHsOm3cNY09uPWZMnYOfgKPYMjWFedzuEECBb/XZOO3gajpjVGeCsnZE17gLAY0QkAPxECHEzgGkWg70FwDT98wwAGyzH9ujrKow7EV0OzbPH7Nmz1VrPxMr3HlmFR1/dWrX+xPld+PjNzwIA3vnBOVJlfeXXL2KkUAKR976qWKcqePf+k3HGwml4eWMfrn3kdQCQqtso47SDp2GWbqTuW9GD65e85ViGsX8uQ/j3jx7hu829/SO44o7nsWj/ybj3sydg9ba9uOrOlThj4TRce/5hFdtU+fI9L2LJa9X30cDpuhjn9fBLm817/NsXN+O6JW9K1VmrzFr7+MFanrUsrykrvNpmbA9j6gunuqZObEnUuJ8khNhIRFMBPE5Er1s3CiGEbvil0R8QNwPAokWLeMaQOmDb3hHH9UUHD9aLYkngilMOwFfPOihos1zZumcYx177hFkfAJT0X+gdnzkWJ87v8izjgZU9+NLdL5rHAUBR/7z2+2eb3rxB3+AYjvjXx6D6hR4rlgAAPbuGAABDo9ry5r4hjOrbNu4eUiwdUsev/X71A/rqu1/A/Ss3VqwrlkpS9f3vZcfipAXV13pL3zBufPIt/Grpehyy30T87qqTpcpzY8W6XfjwTX8BUHkOdy1bj2vufxnvfVc3/uOjR+A931tibnvgihNw1OzJVWU9+MJGfOGuFzBtYjOW/tPpAIDXNu/BB274o+92NecyGCmUcP5RM3Ddx4/0fbwqUpq7EGKj/n8bgAcAHANgKxFNBwD9/zZ9940AZlkOn6mvY+ocd8+lfp7Nqt6X9bhaZUyakMeMztZQvDw3InzZ8YXsM93NI993UguOnNkZWnvcaG3Kmp8ztrZk7SsiIBPl62mter12IKI2IuowPgM4E8ArAB4CcIm+2yUAHtQ/PwTgYj1q5jgAfay3M3YEgr+Ge1FZvJq1Jb0UP0cTAWFPX/nKxj1Y1zsQaplBkT3FWrc5zO+AW1kTmsoChd3Q+jHuqm2N4wHihIwsMw3AA/rrZw7Ar4QQvyei5wDcQ0SXAVgH4GP6/o8AOBvAagCDAC4NvdVMItSPf+5NkJ+bcR3skoxZNgW/VsKhhItuWRqw1HBxaqMjNS52HF5tPluuQ9a4O91bUvzWGFXEbeI9jbsQYg2Aqp4hIcQOAKc5rBcArgyldUxdoOKkalEE8bnuRhv9etTlzjT54wik7Lmbbwp18CSVbWMtA54JcRilzLeJbPVl3R7QTusCeu5x31IeocrIE6LFiUOWqUkg1732dQjiuctcE7c3hrRSq7Vx69H22tw9d2P/4O1LrebOMF6k1cm0/jCF7b9ffGnuCP4cLEs/wcpxLDukh7RsObUeRobhC+M83eo5du4+OHlBF7557sHysozZ1yIs69TIpFhzZxiNEC2NEOmJ+vCL11sHEal77orHJYF0h6qE5h6lDNXalMUvLzsWADA0WqzYFkuHKnvuTOqpByHYQq2BLLKv26RgfDTPPdi1qodLLRsKWct+ZuPW3CVDIZ3tsZqRTipaho07ExhlQ5SARxOG0fR86wigudeBTTeRjpapcbUoRFlGhipZJoYO1TA7jX3Vm0y1TD0SluExvNqof8/W8u2GSPaHWq3aSx6jeLGMkbC9/c6jgdNE2mQZuc7oymV/nrsa3KHK1C1hD9gJC2sHmxkKGcIjSkB4dhKq1mO9lLsGRpXKiIvgfnu4sowM8oOYnOLc1WDNnRk3GAasHiL6VJJGEQGSaVeqsFZj5Jlxa1PiSF6UWp5rmLKMTD+KbPoBp1BI1RBUI1om7tvGxp2RJqUOuiuVsoz7Nr94ae6EIJ67NR2hUhGxIe25JxwtU9kW2VBIjTBCIQ3PnQcxMXWH3y+tGcOdhPXy2Vj13DL+6jGotO3ptu7yuWXczyNMyUKlKD96uGpTk3rTYuPOxI7ZoRp19oGKUEh7h6psKKRxvKUseLc9Si8tzLzngcqRPMvanrv3PlGS85FbRhUOhWTqlnqSa+JoKhGF47m72ISgHn0YncqAFucuY7i8BnwByX2HvGSZynX1FefOI1QZacIyCmVZJlrCkDWctFdNc6/RSagfoYK1nhc37MaytTuVyokDITRZpehxrjVlmYQMn0GtzJ4y62RIKhSSjTszLony9xaW5n7Z7cvDaVBECMjlkIhLlokiN3wYDgLLMkzd4tejjy0U0iH9gF+j66y51zZqQbJClupM41IZ8l+5LZ2dxo6Jw5Q99zBapFBvMtUy9UjYHXFx/rCrRqhGWFeQfO4yRyVhD53aJZu2uZb3G6bhS2t0EY9QZZiQcZzV3rdP7dDh5+GxZkg+qZadunLchZAyXLUTh6XTIDvdYFVnhGUZJvW4Gh6fBim2ASsJ1KlVrJ7yN454ntDewMKQZUL0tkPV3CXXycDGnWFiwK/eb2ru1mgZj+MJ6vl26slzLwlZbzYe4xZuh6pDbpk6i5Zh484EJq0Zf50Sh8VBkPOS0tzViw8VAQEC8M4Pzqm5X0r7TGsSZpw7d6gy4wbTe07UTEmOUNX/V0TLeEzuTVB/mMTxEAovdTOkLmNcnmsUEg+HQjJMCqnQ3G3/I603SMrfupquQ1Jzj7wV4RNuKCQbdybluPan+u1QNUMhg7UnDpy0VyFkNHe1+upFcx8pFDE4WpDS3OO6z5HLfIrHcfoBZtwR+Rgmh8RhqknL/OZzVzXSMoOY0jDw56zr/4i1vQOYPCHvuW9a489r4XiJVT13lmWYekV1hGqcKHf6upRV6+caLJ+70mE+6wheydreAQByD5rYPPeUlsUzMTGpJ6zp9MzEYZG/Rnsl95IowyEUUuaYepFXghI0zj21OA1iqrOskGzcmfGBqPgXrCiP+O6wEoe5lq9WdCTITUpdh3HuTnOococqM97w3aFq6N4Rm6kwMw360tyDyDJ1Fi0j86iRi6hJ0yPLJeWvYllxTwJuwB2qTOzEJctU1lnpugfxJo3BO25E7blHxVfefyCOnj3Z1zEyl7GW57pwv4m4+Pj98ekT5/qq16U1IZRRo3TVCbI5nztTr9Sbr+kH5TlUFeuL41q61XHlqfN9lxVUc89mCP963qG+642aMM2xES0Tt4lnWYaJnfhmunevM8gPzWtkZqCUv3Umust4pXE1N/LcMoplGdEycTtB0sadiLJEtJKIHtaX5xLRUiJaTUR3E1GTvr5ZX16tb58TUdsZRpryCFW/s3Xox/kw1sEm65BukjohWpkU5Q0LlTCn2auHaJkvAHjNsvxDANcJIeYD2AXgMn39ZQB26euv0/djGgA3++bbSw1B95Yhyk46r5LV307qS+RKU2dp1HHuqueRVCiolHEnopkAzgFwi75MABYDuFff5XYAH9I/n6cvQ99+GqVhSB2TGsz0A3HWaZtmTzrlr3G8j7ooQD73eouPL0o0OK3zcdQizBGqaR/EdD2ArwIo6cv7ANgthCjoyz0AZuifZwDYAAD69j59/wqI6HIiWk5Ey7dv367WeiZW3CSNtNqjqH5TQojace7aTmplS+yTtK9kfVMrSuhI8cW5p3OMamrTDxDRuQC2CSFWhFmxEOJmIcQiIcSi7u7uMItmUk5sE2Rb66yaQ1WucsNg+LHVmSDRMml9UlqwtrEgY9wjbEucqH5fkzp/mVDIEwF8kIjOBtACYCKAGwB0ElFO985nAtio778RwCwAPUSUAzAJwI7QW84wHjjmhQlBC/eciYlIKgGYYy11YN1Lvj33KFtjqSfMskIcxJSUdff03IUQXxdCzBRCzAFwAYAnhRCfAPAUgI/ou10C4EH980P6MvTtT4p6+MYynoQ2h6r+P/qskOUaVL+BKm2kAPXVww/F2sY0yTJh4ii5K55HUqNvg8S5fw3A1US0Gpqmfqu+/lYA++jrrwZwTbAmMo1GOe1u/F96v6NjndIPeE0MnfbcMkEfINY2pspzl6znqX88RaKs8OLck3q2+RqhKoR4GsDT+uc1AI5x2GcYwEdDaBtTJ6Q1H4r1NxVvCwNEy8TQ0qAv0r5lmUC1ySPrIc/tapMoKzySem/hEapM7CSRW0bVlXZKPyDgES1D6ga03gRMqQ7VlMoyC6dPdN0W5iCmuvDcGaaecPpRxdH9Ezi1QcpxauPpB0/Dkte2Ou4fVySgHyP68rfPRFPOn2+btsyVXrDnzkgT2hyqRihkoNb4rFPxuNg1d4mWJu0Il4RAaz5bse6WSxbhuo8f4bh/Go1iR0sezbms63bO584wCpgGLOr0A06TW5vbIqw35dPsBUUAaG+pful3M+JJP4xUCHfij2Rg485I4yZp1IVBChgKaT137zj3Bg+FFAIdzQ7GvQ6NeBDmT22X25E9d2bckIAsYydKqSATKLdMDNEyCsfMnjLB/FwSQGuTu6RhJy5ZItyUv97rZPsS2HNnUk/YZifW9AO60fRtOw3N3W2DyzHKI1SlmhS/ufj84vk4eUGXtiCAnI9e0nr06B01d9s62YdWqrNCMkwt/JqxepAeauFlt7XEYXJlre0dsBWu0qLoyWUzWHzQVABan4lTjnK3kMfY4txDtKKhhkLW4QhVhlGiHC0T35de2D7Jp/x1ThzmlVtGxkY/9cY2nPqjp/HbFzdZWhfHICa144xTLgl/E1CkNc49KOy5Mw1PPaQOCtrESqNbuzAtt4x3ha9v3gsAeGVTn7muVHLbu8wbW/cmcs3LGTKd63azYfGNUA2xLInEYRlJ68maOzNuED695yjwm1umar3HMerpB+TYumdEsQZ1DGddwPlh6Xqt6tBxd45zZ82daVRCdhaTGMSk7PDaBjHVwndWyIqy5Q5MJJ+PbqVKwrl29zj3BomWqdpH1riz5s7UKb47VNOv4pi4Sg2emnt00TJBUW2becqiPqS4IDin/K1clg6FZM+dGS8kkTjMDIU06vY7E1NFWR7HSOzjRhw2U7WOjOVaOHruCcsvYXbQy5yLtCzD0TJM2gkrt0wjUOsHS0Ryedkdi5CUZQJcc+VoGb29JSGcNXf1JqUQGc1dsiT23JnxgjlZR4LmINBkHV7RMjGk/JXJox425VQM6QzHj9uISmvuEbfDDTbujDSh66wJfOvDOoWamjvUjZ+szVYdAQuo30erLON0IZOWZcJETpYJr6woYOPOhIA/Y5GEjBM4cZh1GJSX5u4zcZh11yt/9bzUMUE8d+UjDVmmJNcl+6kT5qjWpESoce4S+8gO5GLNnRl3xBsKKSr+B627Zpx7gJS/sgQy7gEfdJ+9Y4VjGfZ13/7gIXjnB+eoVZYwMpILx7kzDU8jd6g6a+7ex0R9TYpBZBnFB49hzF7ZuAdDY0Xl+iMjzDj38IpKDDbuTOyYuWVidGmMOst1+zzetuw5h6pEmUHOPojnrnqo9ZRHCukz7kmGQu6/zwTX/XgQE5N6Qk/5G3J50VDdSm+nWS4U0k7Jh9VNQpaxGrPhseokOA38AucI53NnGh7/KX/jNwNBa/QTYUJ+cv5a8BMBEywUUnGEqsVKDY+m0HMPVZbxp7l75RpKAjbujDRh6ciq0kgodZuf1DvDPOPc4TNaRt/Zj70OFgqpfKjJcKGIo2d3Yvk3Tg9eWAqR+W5ynDvDpIDgKX8rqfW7zhApGV9/nrvv4k1UL4XVUx0rCnS1N6OrvblcbsK6TNxG1CrL1O6DYc2dqVP8/qiTyC1j1m2Mjg2il3qcbzZDvmQT48dvGPevnnWg5zEFmcTvLqgOYrJfs0YatGTHb4cqyzJMXROWVp5E+oHAbbcdXusHm/Np3O2yTD6T8RwgE8C2K1+JJNNFyBDuNHsSmjtP1sE0OonkFveJerIsh2gZj2OyWcKYQoen8UAgAvLZ2iYhSJy7n6gcK/bnTdqNfRBkzkz6/FmWYcYLScoyBrJVO6Uf0Na7l5DPZFBQEMUNDz6bIeQ93MJiEFlG8TgvWSbph3zcX6eK8/fINZQEbNwZxidemnUuSygJ/x6ysXuGCPmcl3H3VXQlQZPLGEuN67iHm8+dNXemXvHdoZqgg+d3dKxT+gHreifyWe1nNebTu/7Dm9sAaPKHpyyTQOKwtMsy8ce5W/cPVlYUeBp3ImohomVE9CIRvUpE39HXzyWipUS0mojuJqImfX2zvrxa3z4n4nNgYiI8o2xoy3GmHwjvieJVUk7/1ReK3nnfrXzp7hcBAJkMmQ8IN5JI+Vt1v+yyTPq7XkKlIlrGIx1FEsh47iMAFgshjgBwJICziOg4AD8EcJ0QYj6AXQAu0/e/DMAuff11+n5MA9PIv2nD66ry3Gsck9MNs5dxdyNDhJ5dQzX3KSTgudvPOV1+e/wect0PYhIa/fpiXv8TABYDuFdffzuAD+mfz9OXoW8/jZKK4mdCJfQRquEU56tO1ZS/1lP3ug6GpKIaiy6Ts0Q14gUIkFvGZi3G+8/a4+XKJM2eO4goS0QvANgG4HEAbwPYLYQo6Lv0AJihf54BYAMA6Nv7AOwTYpuZBqEebINbG2sZNiNGXda7thtbmY66YFkhFWUZe4eqcgsiIuYGyXeoplRzBwAhRFEIcSSAmQCOAXBQ0IqJ6HIiWk5Ey7dv3x60OCZB/Gq4Scg4Rp3KKX8t5+jVfiOMcUwxpCVq4x5SsIxDKOT4wmq0a3eoJoOvaBkhxG4ATwE4HkAnEeX0TTMBbNQ/bwQwCwD07ZMA7HAo62YhxCIhxKLu7m611jN1SVmWibNDNdzyamvuch2qrmXXKHxqh5bLJcggJlUrbH/opM1zj9tBfv8h06TqTq3nTkTdRNSpf24FcAaA16AZ+Y/ou10C4EH980P6MvTtT4rQZ1ZmmHip1Ny94tz1DlUPzd3t4Var+JsvXgQgqOeuKsvYlutBV4uQuV1tuOGCIz33S+oq5bx3wXQAtxNRFtrD4B4hxMNEtArAXUT0bwBWArhV3/9WAL8kotUAdgK4IIJ2M3WM2akZ47fenEPV51uDaxtrxbnrmvuYoudeSxM3OmsTmUO1diRkqOGmKsRtRK3hqmmL+QckjLsQ4iUARzmsXwNNf7evHwbw0VBaxzQ06fs5uONnDtWgoZC1bKRhUJIZxORh3ccZ+WxG6u0l1dEyDAO4e2b1MEK1HAqpIZ/y13nHmpq7GS0j16Fqvxy1ZBOj7EQGMVUt29MRJGvt464/ny1fgTQqVGzcGWnCsslJzsTkl3Ib5V13IxRS1buW8dwfe3WrUtkAMLWjRe1Ar2iZcSbL5GQD3RMi3a1jUoXbbzfpbIAyhN1CmTh3VeWk1nFGJM6yd3aqFQ6gq6PJ/PyzS98jfVzao2XiJp+lVDsobNwZaUKbrKMsjoRSnq+6fXqXTonDPOdQ1Y/xkk7cDENtWab8k1X1lK2HHTmzU/q46mgZpeojI+725DOZVHakGrBxZ6QJ0IfnSKw/xpAlg1pNNzxcVV281nW2ZotUjcaxNsvPPUhaU08bmQx77kyDUNcdqrb/fjtU/eSWMYy76nnW8sit0++NKo6AbdiUvyG2x3hBmjyhqeZ+5Q7VdF0LgI0744PQPfdwi4sUP/ncM5KyjFvZQgBXn/Eux5zuVt17tKBo3C0V+jGI42mC7KkdLfjuhw717JNwugYTmrIVy0n1M7NxZ6QJy3NPgrAG7siURaYso1ZnSQhcddoCvPW9s6u2WY27au6aCnwZaHvoY/DqwyTs9nzyuP0xfVKrXN2Wz23NMmNDo4eNOyNNWJ6739mQQkWxbj9zqPr13O3Uus7WZqt67qrtqk5FXLmiHh7yYWN8j6ynnpaOZzbujDRhxzHH2p8aUj4VmbLKmnvt/VxDS2scZ/Xc//fZdTXLl6k3SIdq2jz3JMg63Oub/s/ROHPhNLdDYoONOyONe5y7z3ISjItXnqzDl+ZuDGKSa0st7vy743DbpxZZyi5v+8kzazyPd663jJ/rUD1ClTE6uK3fj6NnTzYTvCVJOsQhpi4IMuTdShIjVOPU3I1IC6/rZZdfZk5uRc+uIfzt0TPNdccfUDnPjewEEbVQfQOrGsSUMuueRHsyhnG3PDLTEjnDnjsjjZtJUJ9wWb0tcePnDFVkmYde3ISeXUM4eUEXprS5h9+Fcc0qPHcfBVbnDbNp7nUwUjksXvzWmQDKskzYkWRhwMadkSY0zz2UUtTq9P/WUL2jV/szEj/4J1/fit1Do+byVXeu1NtVu2GheIVWzd3HYWkPhYwr7n5eVxsmteYByL+lJQHLMow04c9mFJ91CNp2uxdey8h6Rcvs6B/Bp3++3HGbQ2h7TeZc8zu89O0zMbElL32M6qVI/RyqMbDyX85AS74cx17uUHU/Zp92bfasmZPlwirDgo07I01oHaoJejnlrDZypimKOHf7yNLhQtH8rKKp9w2O+TLu1oeOv2gZ+3K6QiHjeJOYbJPMyh2q7if/vnd145aLF+F9B8Y7nSgbd0Ya11dPv+kHjA9xdqiGGApZaz1Q9txlH2K7B8vyjIrs4n+yb8uxPm5CGJ25jUZGMgPo6QmERrLmzkjj2qEasuFMP3Jx7m4PQ3ue9/6RsucukyL84uP3r1iWnBPERPl+pV5zj5+sx71OEjbujBQbdg66Tj5RD4nDDHvmt0OVJDRVO6ZxdzG69us4NFqoOrYW/3reoThm7pRyeT4vqPIgpqrldMkySeAU554W2LgzUlx827LQy0xLPLAMVm9XiNpG0djmZnTtqXoHRvxr7k0WF9/vjE/qMf9pj3OPv0GyYa9JwMadkWLP0JjrNv9f6/h/COWUv/5GqKqYC69ONvvcqgNWz706gYsjOUtYTRBJIFCHqnKtjUPQWbeihI07I0Wt766qLFNPxqFyJia59ANuP/iCzXPfO2yVZeTaY52RyV6eF2FNkJ02EtHc9duQxgFcbNyZwITVQRclcY6i9Ypzt6fqtb4VycoyQ2PlB4Jfz9360AkSLVM1QbavVoRPIukHeIQq08j49tyjaUbtOm0dqn7dvArPXYiaRtErzr1g22BddjNQ3//bw7D4oKnm8p9X7zA/+9bc4V2fE15x7uMRmTj3pOA4d0aKKL68aZumzQmVNnrFudeaZMPNc7/wmNm48JjZjtuCRMv4wX4tXt+yt2K5s1V+IFUUJNuhGnvVnrDn7pMlq7biy/e8mHQzYsf+3f2vi45Cd0ez4zbPshL4IQjbf79GW9g+S2nuLh51LU/baWo9J649/zDz8zu9A1LHGCinH7A17Zk3t1csn3bwVNxwwZGKpdcn5Q7V9Fl3Nu4++cwvluO+53uSbkbinHv4fvjt507SFnx7jnrESvodd0XN3V+HqpWsZI/qgmnt5uerfTobYaU//psj9rNtJ5x35Ay1wusUjpZhGhr1RFTxYRo0RctmlViEqN128sgUWEuWsUbB1CIvM5TVFYFJrXkctG8HcrLhOaiWPb7zwUMCtKExsI5GPmjfjopt0yY2J9EkE9bcGXzvd6tw74oerPzmma771LKJ9dChaifKtwav9AP2DlUrssa2KYBxFwL4myOm498+dJj3zhbsTWtrzjrvOI4w37QEcO9nT6jIE/T41e9DvyXMNW7YuDP46R/Xeu7j1DmoaiBVI1aCEDRc06651zp5rwkcannuWUnNvSmnfvFKHtE+btiPac6xcbfmlmlvzqG9uWxSJ7bkfWXrDBuWZRgpag9ikjOcOwdGMeea3+HpN7eF0ygF4niuGHbfXZYJw3NXN6xeHcJuSCpG44ryZB3JtsMJ9twZZQz7IPO9/t7vVqG3X3tlvWvZBv349E/WQQ6uuxbn7o5XeNzwWNF5A4CsrOYewHP36jNww+qpv33t2cr1NxJ1HS1DRLOI6CkiWkVErxLRF/T1U4jocSJ6S/8/WV9PRHQjEa0mopeI6OioT4JJljuXrffc56d/XIsHVm4EAMyeMgFAMtEyqr9BP7KOOULVxZ1zMu7GMflYNHehFBPenCvXKRvV0+jUe5x7AcCXhRALARwH4EoiWgjgGgBPCCEWAHhCXwaADwBYoP9dDuCm0FvNxE+NL++bW/t9FSWbHCtKZI2b62QdAXLLOBl3wyuW1dzzucqf7pJVW6WOA9Q7tJtzrMvYMR5yaQzr9bxbQojNQojn9c97AbwGYAaA8wDcru92O4AP6Z/PA/ALofEsgE4imh52w5l4cTQIyh2q/jIzhkHQEbZ+DvfS3IfGrJNzaDs36YZTNVrmM79wnpPVEY+UxW5wuoFq8tkMrlo8H/dfcULSTanCl+ZORHMAHAVgKYBpQojN+qYtAIx5pGYA2GA5rEdft9myDkR0OTTPHrNnOw+rZhqTkjmIKX5j4ffB4tbEmnHuRCByf6AMjZajZXIZQrEkTK9YJZ+7X4SPehhvrj7zwKSb4Ij0N4SI2gHcB+CLQog91m1C+xb7co2EEDcLIRYJIRZ1d8c7cSzjn9am6ugM1Q5Rv9PChUFQSbQiFFKisAyRuyxjmRDbCJ1rb9H+14qkqSg/Q3j0i++V2tdOyaNDmGkMpIw7EeWhGfY7hBD366u3GnKL/t+Ib9sIYJbl8Jn6OqYOeebN7bh+yZs41jKtW1CM3CqJdKj63N/tAeb11pEh94Rew6Nl496hG/W2JsO4yz/5VCM0vGaSYhoDmWgZAnArgNeEEP9p2fQQgEv0z5cAeNCy/mI9auY4AH0W+YZJMU4ywsW3LcP1S95CSQjMn9rucJR/Rn0YsLCwn5pf41Y5WYe3Uc3qcosTVs+9rblSGR0tyF8be/kDI3KjIQXUomWY+kLGcz8RwCcBLCaiF/S/swH8AMAZRPQWgNP1ZQB4BMAaAKsB/BTAFeE3O3nSmL85KLWyFRZLwhyNZ6BqHwwDVg/mRUVzB7SONjcv3CpLteQ1uct4YPjx3Od2tVUsH/KtR9Gza9DzONU4d6a+8OxQFUL8Ce7fhdMc9hcArgzYrtTTiK+2tUbZFYoitPMdMYx7AukHylP8+U35W5k4zIumbAZ3LduAz55yAKZ2tFRss8opH1s0EyvW7cKh+03CKxv3+HqraWvO4fXvnoWD/uX35rrNfcOYOXlCzeO09AnS1VRw7fmHmVISk274LinSeH57tYb7ysY+8/PAaKFq4IqqbR4tuI/QjAr1CSpc1nuc/I4BbTTu5361Evf8/fEV24wMgg9ccSJam7I49/D9cL+eRtqPLANonv+pB3bjqTe03OqteYm0BEK9M/yiY70j2x7+/EnYZUmgxSQDG3dFtBeUxnLdrQZw3Y4BnPv//mQuD44WQxuVWI4ISSAUUvU44fzZi969I1XrSgLIZcmMQGprzmHShCYAqEg8FbSdrvtASE/ErcKhMyZFVzgjDRt3RRrRc7dGd+weHKvY1j9SCC3D3Yjuuccry9iQDnR3Wy1XwJreASxZtRWnL5xmriuJ6v6Lcw+bjh39I7jgPf7HfBheOwAUJOJMSw0oKTLV8HhiRRqlP9Wa/8Qqy9h//IMjxSpvTzXiwqgyXwdpBg0jXpny19/Nt48e1Yxr5bXLZAiXnjjXcTyBH7509ws1t//n42+iWFJL+cvUF+n/daUU1fzgacPagScsTp99BKOT5h6UOJNPGc8t5Sn+qmIpg7QlOlnknR21o2VufOItAOy5jwfYuCvSKJ671bi7DboBNM3dbvCD2oecZJKsJHEygkHvfbEkEh/+n/4rzwSFjfs4xxqdYZ0SzG57iiWBplwGs6dMwOdOnR9K3fGmjQ2YOMy2HKTlJRGucf/pxYv8H8Sue8PDxl2RhvHcLcb94tuWmp+dzm/j7iE889VT8Y/vDydRkp/JmcMmSH9q0FtfEuHOanT4TP/RKWzaGx827oo0iub+6qZyDjirXus0ifN2W1hfUOcvCc09rOODnLsI2XOXzbM+ZMlpk7QsxEQPG/dxzIadg/g7WySH0eFYdAqpU53FyMWy5mKMlil3qGr/pSfrcBTdg7WlJMI1ri0yA5cAPPzSJvMz2/bGh+PcFWkEWWbP8FjVurlffwQ3XngU9p3YUrXtMNvrv2w4ndu1qqep2uwPKNlzf9+7uvH29sqZqoql8FI5AHK53YdGi/jKvS+Zy9Zr/6vPHBteY5jUwMZdkQaw7a6v8/et6MHUjuaq9dd9/EiletyuVZyau5lbxuedc9bcvUcn//maxRgYKeC2P63Fa5srpj8IXZaxT1voNEfqpr6hiuW8JVLphPldobWFSQ8syyjSCFkh3QzMH97cjl+v6KlaP6lVbYSq27WSnS80CnyHuduP9yhgRmcr3jWtA025TFUysJII/63lsS+VJ+5w6i8p2CYBydbBADImGHyHFal/0+5/soe8/fVf0j6lwXOvlfGyFkHj3JtzGQyPFSsecKUIBjFZMzUOOUzAbU8lXD+CGKMKG3dFGsBxh985M1S9zTRo7kaahXKHqr/jVaNl5na1Y3ishLW9A+W2OKQfCMpkPfEYAGzaPVS13Z6rv466OxhF2Lir0hDGPdhJyNonN507zmgZJ6lCBqeOUz8lLdxvIgBgzXaLcS+F77m35LP474uOBgCcdf0fq7YbCcX230fL9W7X6ZnGg427ImmMc//r2zsw55rfYdWmPd47Q30OThm27RlGvz7tm1s1cdgXY2pA40GmnPLXtiwbLWP0U1gjk0pCRPLWUmsSDSPNslEvm/bGh427ImmUZR59dQsA4K9rdkjtH9Rzr8Ux1z6Bs2+o9iANchmKZR7PJVe/Dwft21GVClc6K6K+m1Uz99OZbhj3vqFK4x7FuVtz9Sx/Z2fFNuNeG6mGeQ7VxoeNuyIptO2+deRaicKk6vPYvn6nNuLVqZo49fZak1V7YXT62o+XvdaGN71nqJy3R4Q8iMmgransuX/kx3+t2GZ0qBrXnUeoNj5s3BVJYyik3yaVIvTcrThJWCM+p5MLQi5Dpubut0O1SR8LYM3B4+eq5bMZtDVlqzz3KJ5tR8zqxOwpzvOnGqGQhsfOknvjw8ZdkfSZdv8YBu8QvdPPL7Kv9kk/B4N67kTVoYR+mNSarzDuxZAHMVn50hkLHNcbspTxJsKOe+PDxn0cY3juR83udN3nesVRqVasZnVCwJmGVMhlMqbn6nuEKhHy2QxGrJOa+HxOTGzNV3aolqKTRc47YgYO0+cwtT7QjAd5JtPYmvu5h0/Hf3z0iKSbkQrYuCuStDfqhKrmft6RM1z3sc79KYOTXGVdlyHCgqnt+PDRM32VG4QgnjsANGczFbIM4M84TrR57lHOxJTJEM49fDoAYNgymMl4uBn1NqZpB/7roqPx4XfH991KM5xbRpE0hkL6xTB4bl7k3793Htqbc3jqH0+pmMjDwOmokgDsWQWsV+q0g6fihguOirXPIpclc1JuFZpyGWXNHdBkmQ07y+mUw84KaceYh3VorIi2Zu0nbshK7fpyk2SaYKZ+4TusSoptu6zh3LpnGIDm2V7zgYMqth0xcxK+fvbBAIC5XW1VGSH91G2s+sxJc/F/P3I4gHhlAavnrjJCNZ/NBBq+P7Eljz32DtUIf3mtegpga/52Q5b5xjkL8dlTDsDZh02PrgFMKmDjrkgabbufGe1HCyV87b6XAWixz6cc2F2xfWDU29OVTnWur5ze2YrmXBKaOymPUAWqPXe/mtyk1jz2WN58oopzN7B67gY3P7MGADClrQlfO+ug6jxBTMPBd1iRNGruBv0jBex1yNVupWJWnkx1TnD7rEuyOI16NSSspHTeoJq7U2ZHP7Z5Umse/SMFUwPXpKsIjbuD527ktklyakMmXti4K5Jmzf36JW/hsG8/VnOfwbGyJ5nLZCo02LMO2RfXnn+YUt1ODz3VZF1hkctkqjx3P285+WwGowXLCFWf9edzWl3GZBlRxbkbmMbdITuk7KxNTP3Dxl2RNHvuMgxavLpsprKD7ceffDfOOdxbk3VMquVk3M39k6FSc/d/4xw9dx/Hb+3T+jYeeXkzAC0ENW5Z5oDuNhw4rcPcxjQ+bNwVqXPbjsGRysmSW0Py6JzeaAyDmlRstaa5q8sqWihk+Xr5fT5ceuJcANobERBftMzwaGWbF0xrj6xOJn2wcVckjekH/DA4WpZlshlCR4vaLEt2anruCbnu2QyhWFS/X5lM9WQffh5Uc7raMKOz1ZRE4pJlrG9nI4UShz+OMzzvNhHdRkTbiOgVy7opRPQ4Eb2l/5+sryciupGIVhPRS0R0dJSNT5I02nY/xnNwrNJzD6s+xw5VQ3NXqiU4uWx1bhk/ZIgqs0IqvLe1NWfNB2pUKX8NnDT3sWJJaiJtpnGQuds/B3CWbd01AJ4QQiwA8IS+DAAfALBA/7scwE3hNJMJG6ssYxia5/75dCz/xumBynUye6YxTMh1d4qW8dOSDFUer5I+oK05Z4aXRjETk5UWQ5axGPfRInvu4w3Puy2EeAbATtvq8wDcrn++HcCHLOt/ITSeBdBJRDxaIoXYZRkA6O5oRld7c6ByHT3jhD33LJH5RqHywkVUKcuoyCptTTkMmJOXxBQtY5FlRgvsuY83VO/2NCHEZv3zFgBGApIZADZY9uvR11VBRJcT0XIiWr59+3bFZiRHGmUZP1j12DB/9I4jVPX/SWnuRA6eu4/GZDM2WUahQ7QpVx7lWixFlxUSsOSgt7R5jD33cUfguy20b71vUyeEuFkIsUgIsai7u9v7gJShGuc+PFbEc+/YX4Tix2rcjdmCwqBmnHtCvrtmnNWPzxBVe+4+fzkZKufy0aJl1NvjhfHgMrJ+lkoCY0XBo1LHGap3e6sht+j/t+nrNwKYZdlvpr6u4VA1Fv/0wMv46I//ip5dg947R4hVllGdLNmtQ9XuvZsjVBPy3DNU7uhV61CtTJ9bVEgfQPoDwrg2UU9Qnc2UH0hGjD577uML1bv9EIBL9M+XAHjQsv5iPWrmOAB9FvmmoVB1BF/Z2AcA5uTRcbC2dwCbdg9VrBuUyB2jgkC1AU06WiZDVDWloN8O1ZJNlvGbPiCrR9wYBjfqae4yVJZlDDmINffxhWfKXyK6E8ApALqIqAfAtwD8AMA9RHQZgHUAPqbv/giAswGsBjAI4NII2pwKVOPc39zaD0CLuAgbu7kolrSQu1N/9DQA4J0fnGNuMyfT/vriAPU5j1C1h0OmQXM3jKqKnKaFQpaXVTpUMxntfhjXJuoUL9YHkvFdi3PeWiZ5PI27EOJCl02nOewrAFwZtFH1gIppt3rrQfKLy/L4qq0469B9Hbdt6RvGlLYmTJ/UGmqdQlSbT3OEamKae/XD2M+DJpOp7JwsKUyTZxhbQ96JerRuhsjU3IsxPVCYdMHvaYqoOO7W2XickjqFzba9w47rRwslFEoCl54wJ/Q6BRw894STy9g7RNWOr4xz92ucM6bmXl6OEqvmbjxQ2HMfX7BxV8a/tegbLBv34QiMu91eCAEUHCZ2NtIBTwwYJePeoeqyf6Da1LGGQqqPUC0vayNM/ZahHWc8JKKWv8nSCZx0bh8mGdi4K6JiJHYPjZqfo+jQtLdpaKyIvZZJIt7YshcAcP/zWgDTxNbwZ1kUokaHakLGxXBYrdKMn7bYo2WUZBl9lGxZc4/eczfOtyjYcx+PsHFXROUt3zrVWlTRKlaGRovYY5m04/3XP4P+kQK+98hrAILHtzuZCm3Qg0soZKDa1DEiW0pC7b5lMjZZRiF9gOH9l2J60FkjhIw6o5wghEkfbNwVUfHcrTp7xbRtIWH/7Q4XihU6PwC8uGG3+Xn2lAmht6FUEq76dmJx7saITUXhvUqWKfmPljFSIBidnPFEy2ifS2YnbrR1MumCjXuMDI+VDXoUxr2qvtFq425IM9MnteCA7mjye1cNYko4VYNh1JwyVsrgJMv4lTjsoZBRSyQZKht17lAdn4Qvuo4TVOKlh20pWKNm6dqduP2v6yrW9faPIJsh/OWaxYGlAafjS6Lac086zt3Qt4XTCCsJsg6yjF/N3Ii1j0uWsWbCjOuBwqQL9twVUXEC7Z57b/8INuwMLw2B3WC8rnvpVv7n6bcxqTUfmXFxyjSUeJy7qbmryRPWQVBGOX7LMGQZM/1AnLIMR8uMS9hzV0TFuFsHLo0VS1j0b0sAVI4cDdYmuUbtHBj13kkCtw7V9I1Q1f4XHQZYyWDNTQOoZYUsh0IayxHLMplym42XRO5QHV+wcVdETZbRc2oTMBpg2jc31vYmm4wM0OPcbeuS1txNWUY3cn5NnH0QU1GhQ9UIhYxrtGhF+gEeoTouYVlGERmDtbZ3AE+/sc1cHh4rojmfQVM2E0mH6pLXtpqfD50x0XW/zgnhpfi145RbBkhWFjCMmnqHankov1GO36yOZiikGS0TseZuGbhl/I86EyWTLti4R8ipP3oan/rZc+bySKGIlny2YuKGqPjfy4513fa7q04OpQ5n+1Q9QjXprJBGR2KxxujZWthDIYPIMnGlHyAqX3ezQ5VlmXEFG3dFvIzE1j3lvC6GFj48VkJzLoN8liIPhaw1QGlGZ7jJwqxYc5YbJK+52ztU/Rvm6sRh/tpQNUI14l9eZbSM0YZo62TSBd9uRbw09wtuftb8bEyMPDymee7FEvD61upIFhX+8OZ2fPTHf8Fdy9ab6265eBGICAft2wEAuPCYWW6HB8LJSAqHUaBJz8RkDYVU6Suxh0IWFbNCCmHN0BhDVkhhk2XYcx9XcIeqIl6e+7odA+bn/uEC2ptzGCmU0JLPoLd/BL39I1XH9A2O4Zm3tuNvjtjPs/5SSQvHu+S2ZQCA597ZBQD4xjkH4/SF2pS2v//ie839C0WBX6/o8Sw3KNbkWAZpmIkJsHjuPo+vyAevSysqskzREgoZS8pfc/YpjnMfj7DnroiX/2f98RtZGIfHimjJZavL0n98X/71i/j8nSuxZnt/zbL7RwqY90+P4PN3rqza1uwyldonjtvfo8XhYM2fYl0HJDgTU+D0A+WOUFXN3Ew/EFOeF6eUv+y5jy/Yc48I7Yek/aiWr9uFBdM6MDxWxISmHPad2IItFk2+UBLIZ8mcCs8rqdguPU794ZeqZzDc4RLDfuSsTvz5msUYjHh6PwGHOVTNUZmRVu1KhSyjYN+tsoxqWCEZskxsuWXKdcUlBTHpoiE8955dgxiIcU5SwHvAkLXz6p1eTaIZHtNkmSsXz6/Y14icyWW1H1/Bw8O0R9oY2joATGyp3ZG6YFqH6/YwcEz5W+5SjbRuN6pkmQAjVE1D6dM6Z21vD5HLMhmrLKOvY9s+rmgIz/2kHz6Fw2dOwkOfOym2Ov3IMiN6ZMxwoYjmXBbNtpkaxgoCn7pjGV7q6QOgzW965KxOx3KfXbOjaoTpg587EU3ZDJ5+cztOnt/l70RCpnY+9/jbA5TvRbGkPkIV0KQZVVnGKMN4MMc5iIkTh41PGsJzB4CXevowUijiuw+vqpjxKCq8Xu+tP/5R/Qc9MlZCcz6D5nzlZX/opU14+o3t5vJNT7/tOIMSoEXhXHHH8+bytInNaM5lQUQ49cCpyCU8w71Th6pB0pq7mbTLZ0usuWmCyDJA+a0sjkFMxsTYqm8bTH1T98bdKlE8+MIm3PqntfjRY2/EUHNt62797Y7pnrs5iMlmgP/lN69UHf+JW5bi0z9/Dlv6ytr8yvW7qvY7eUG3n0ZHjkPesFTOxOTreMvDQTU3jOE1F4rxeNFkic0XrLmPS+reuFsnwDB+OHHkSveyE9aoFcNzHx4roSWXrfLcnVi6dieefH0bjvv+E7j7ufUYGCng/P/5S9V++0cw4UYQRK1QyCQahLJRKzlIRjJY88Gr6vaGLS/o7nTUdrZimj1OHDYuqXvNfdgSWRJnPLWXjZjX3Y7e/p2Y0JQ1HzbaIKYMmrLV4ZC1+Np9L+Op17dXrf/muQtx0bGzfZUVNU4GNHnNXftfLOsyvshaNHvV3DCZmGWZjGNumUirZFJG3d9ua9jgej03eizGvYZ1/9XS9Vi2dieOnNWJuV1tGC2UUCiWUCgJrUNVwnO38/tXt1Stu+jY2WjJ+3tQRI9DKKT+P+kOVS1jpX/XferEZgDAsrU7y3HqConDgPLbZfQpfysHXsVRJ5Mu6t64P7Byo/n5J39YE1u9tfTbf3rgZQBA39AY8tkMRoslDOvee4ueFTIM0mfY3dIPGLJM8ukHtHb4Y/GB2ojft7f3K3eomrJMTNEyTdkMhnTHx9DeOVpmfFHXxn3pmh244Ym3HLZoX+Jla3fix394O5K63Uy7dSq95lwGTTktve+Ivr4ln8W87raq4x770nsxf2o7jrCEQF7wnuqcMD+79D1ozmVwUsIhj25onY7OnntSorshRxQVO1QntubQms9ic9+wKbH57Rw2O1RjSr87f2o71vT2Y7RQsnQCR1olkzLqWnN3G41p8LGf/BUAcMnxczA0VsQVd6zAv3/kCMwKoRPSzU7ssUxInSFCcy6D/pFChefe0ZLHr//heHS1N+ObD76Cq05bgHdN68CSq98HQPPuHlu1FR84dF/c9dyGivIPmT4Rr37n/al9xTZyr1Su0/4n1WIyNfMSBkYKaG3y98ZDRNivswXrdw7iO799FQDQ0eLvp9M5oQkAzOinqO/f/KntGCsKbNo9FFsOeSZd1LVxrzWS05pH5OBv/h5nLJyGZ9fsxFV3rcQvLzsWW/cM49fLe/D5xfOxc2AUMzpbXb2pbXuH0daUw59W95rr3LTbNyzZHjMZaLJMoWR69IaU8p45UwAAv3TIu57LZnD2YdMBAPtNasEm3SB8+sS5mDqxxfWc04CT5570ZB0z9RTHa3sH8de3d2Bah/9reOiMSViyaisGRotoymVwjn5/ZFm4nzZ5ysoNWjhr1F701A6tn2BNbz/W7dD6oibWSAPNNB51bdz7h51TDty5bD3utKTABYDHV2mzFK1cvxuHfutRc/1f3u7FSz19mD+1HcfMnYJrzz8Mz6/fhc7WPOZ2afLJMd97wrGe0UIJv1m5EcfOm4IZna3YOTCKT966zNyeJUJrPovB0aJp3JsdEofV4jdXnog1vQM4bt4+vo5LirueW4+/O3meufyh//4z8npahaT8xgO629HRnMM9yzfg7e0DrsnVanHUrE48+MImAMCNFxzpe7DYvK42zNlnAh55WesYr5VvPwy62jXj/umfL8fJC7pwyH4TzXXM+KCujfuuweATPRtD/ldv68fqbf341dL1HkdoXPTTpZ77XHz8HCxftxNrewdwzo1/AgDfkTJTJ7ak3lu38uALm7C2t5zu+IUNu83PiUXLZAgL95uIpWt3AgCuPf8w32WcqPdx5LOEw2d2+j6eiHDC/C68s0P7fhmOQ1TMmlKekOWPb/Vi/tT2SOtj0kddG/d/eN8BeH3LXrx7dicuPHY2Xt+8F9v3juCzd6zAWFHgl5cdg2/85hXztRSonKEmSh656mQs3G8iBkYr3y7mdzf+j8x4YNppa07u63bu4dOxdO1OTO1oxoffPdP38QumdeD1754FQD1K6fKT5+Evq3txyQlzIk8T0dGSx1WL5+PGJ1cDAA6fOSnS+pj0QapDsmsWSnQWgBsAZAHcIoT4Qa39Fy1aJJYvXx56OwAtl/qWvmFMaWvCWFGgvSWHrXuGsf+UCRgtlvDKxj04fOYkvLFlL+bs04av3Psi8tkMvnzmu5DLZHD/yh7M627HUbM68fqWvXj6jW346lkHYfW2fkztaEbf0BgGRgqY29WGsZLAfpNasLZ3APN0I14sCTP+vjWfxb6T6scLl2H9jkFMm6Tlt1mzvR/779OGFet24cBpHbj1T2uwT3sz1vYO4PgD9sGZC6clprsLIfBSTx9mTm7FPuNEnhgeK2LrnmHkshlMmdDkuyOZST9EtEIIschxW9jGnYiyAN4EcAaAHgDPAbhQCLHK7ZgojTvDMEyjUsu4R/FueAyA1UKINUKIUQB3ATgvgnoYhmEYF6Iw7jMAWIOze/R1FRDR5US0nIiWb99enTeFYRiGUSexEapCiJuFEIuEEIu6u9OVtpZhGKbeicK4bwRgHTc/U1/HMAzDxEQUxv05AAuIaC4RNQG4AMBDEdTDMAzDuBB64LEQokBEnwPwKLRQyNuEEK+GXQ/DMAzjTiSjSoQQjwB4JIqyGYZhGG/qOuUvwzAM40wkI1R9N4JoO4B1iod3Aej13Kux4HMeH/A5jw+CnPP+QgjHcMNUGPcgENFytxFajQqf8/iAz3l8ENU5syzDMAzTgLBxZxiGaUAawbjfnHQDEoDPeXzA5zw+iOSc615zZxiGYappBM+dYRiGscHGnWEYpgGpa+NORGcR0RtEtJqIrkm6PWFARLOI6CkiWkVErxLRF/T1U4jocSJ6S/8/WV9PRHSjfg1eIqKjkz0DdYgoS0QriehhfXkuES3Vz+1uPVcRiKhZX16tb5+TaMMVIaJOIrqXiF4noteI6PhGv89E9CX9e/0KEd1JRC2Ndp+J6DYi2kZEr1jW+b6vRHSJvv9bRHSJ33bUrXHXZ3z6bwAfALAQwIVEtDDZVoVCAcCXhRALARwH4Er9vK4B8IQQYgGAJ/RlQDv/Bfrf5QBuir/JofEFAK9Zln8I4DohxHwAuwBcpq+/DMAuff11+n71yA0Afi+EOAjAEdDOvWHvMxHNAHAVgEVCiEOh5Z66AI13n38O4CzbOl/3lYimAPgWgGOhTYD0LeOBII0Qoi7/ABwP4FHL8tcBfD3pdkVwng9Cm7LwDQDT9XXTAbyhf/4JtGkMjf3N/erpD1pq6CcALAbwMACCNmovZ7/f0JLSHa9/zun7UdLn4PN8JwFYa293I99nlCfymaLft4cBvL8R7zOAOQBeUb2vAC4E8BPL+or9ZP7q1nOH5IxP9Yz+GnoUgKUApgkhNuubtgCYpn9ulOtwPYCvAijpy/sA2C2EKOjL1vMyz1nf3qfvX0/MBbAdwM90KeoWImpDA99nIcRGAD8CsB7AZmj3bQUa+z4b+L2vge93PRv3hoaI2gHcB+CLQog91m1Ce5Q3TAwrEZ0LYJsQYkXSbYmRHICjAdwkhDgKwADKr+oAGvI+T4Y2n/JcAPsBaEO1fNHwxHVf69m4N+yMT0SUh2bY7xBC3K+v3kpE0/Xt0wFs09c3wnU4EcAHiegdaBOqL4amR3cSkZGW2npe5jnr2ycB2BFng0OgB0CPEGKpvnwvNGPfyPf5dABrhRDbhRBjAO6Hdu8b+T4b+L2vge93PRv3hpzxiYgIwK0AXhNC/Kdl00MAjB7zS6Bp8cb6i/Ve9+MA9Fle/+oCIcTXhRAzhRBzoN3HJ4UQnwDwFICP6LvZz9m4Fh/R968rD1cIsQXABiI6UF91GoBVaOD7DE2OOY6IJujfc+OcG/Y+W/B7Xx8FcCYRTdbfeM7U18mTdMdDwE6LswG8CeBtAP+cdHtCOqeToL2yvQTgBf3vbGha4xMA3gKwBMAUfX+CFjX0NoCXoUUiJH4eAc7/FAAP65/nAVgGYDWAXwNo1te36Mur9e3zkm634rkeCWC5fq9/A2Byo99nAN8B8DqAVwD8EkBzo91nAHdC61MYg/aGdpnKfQXwaf3cVwO41G87OP0AwzBMA1LPsgzDMAzjAht3hmGYBoSNO8MwTAPCxp1hGKYBYePOMAzTgLBxZxiGaUDYuDMMwzQg/x+2IciHxtOSxQAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Jit functions\n",
        "q_learning_select_action_jit = jax.jit(q_learning_select_action)\n",
        "q_learner_step_jit = jax.jit(q_learner_step)\n",
        "\n",
        "# Initialise memory\n",
        "memory = TransitionMemory(5_000, 512) # store 10000 transitions\n",
        "\n",
        "# Run environment loop\n",
        "episode_returns, evaluator_returns = run_environment_loop(\n",
        "                                        rng, \n",
        "                                        env, \n",
        "                                        Q_NETWORK_WEIGHTS, \n",
        "                                        q_learning_select_action_jit, \n",
        "                                        Q_LEARNING_ACTOR_STATE,\n",
        "                                        q_learner_step_jit, \n",
        "                                        Q_LEARNING_LEARN_STATE, \n",
        "                                        memory,\n",
        "                                        num_episodes=1_001,\n",
        "                                        learn_steps_per_episode=8\n",
        "                                    )\n",
        "\n",
        "plt.plot(episode_returns)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see from the plot of episode returns, our Q-learning implementation is fairly unstable. It turns out that there are a number of implementation tricks we need to use in order to get Q-learning to work reliably with neural networks. These implementation tricks are out of the scope of this tutorial but they are included in the **Additional Resources** at the end of this tutorial. The reader is encourged to try and improve this Q-learning implementation by implementing those tricks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AR_xjOaYTGhv"
      },
      "source": [
        "## Additional Resources\n",
        "**Intermediate:**\n",
        "*   [REINFORCE with baseline](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#baselines-in-policy-gradients)\n",
        "*   [Deep Q-Network (DQN)](https://arxiv.org/abs/1312.5602v1)\n",
        "\n",
        "**Advanced:**:\n",
        "*   [Double DQN](https://arxiv.org/pdf/1509.06461.pdf)\n",
        "*   [Proximal Policy Optimisation (PPO)](https://arxiv.org/pdf/1707.06347.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "## Conclusion\n",
        "**Summary:**\n",
        "\n",
        "[Summary of the main points/takeaways from the prac.]\n",
        "\n",
        "**Next Steps:** \n",
        "\n",
        "[Next steps for people who have completed the prac, like optional reading (e.g. blogs, papers, courses, youtube videos). This could also link to other pracs.]\n",
        "\n",
        "**Appendix:** \n",
        "\n",
        "[Anything (probably math heavy stuff) we don't have space for in the main practical sections.]\n",
        "\n",
        "**References:** \n",
        "\n",
        "[References for any content used in the notebook.]\n",
        "\n",
        "For other practicals from the Deep Learning Indaba, please visit [here](https://github.com/deep-learning-indaba/indaba-pracs-2022)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "## Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OIZvkhfRz9Jz"
      },
      "outputs": [],
      "source": [
        "#@title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "HTML(\n",
        "\"\"\"\n",
        "<iframe \n",
        "\tsrc=\"https://forms.gle/bvLLPX74LMGrFefo9\",\n",
        "  width=\"80%\" \n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "intro_to_rl.ipynb",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
