{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# **Paying Attention to Transformers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0RWJRsNiFHX"
      },
      "source": [
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" width=\"40%\" />\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2022/blob/main/practicals/attention_and_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "\n",
        "© Deep Learning Indaba 2022. Apache License 2.0.\n",
        "\n",
        "**Authors:** Ruan van der Merwe, Marianne Monteiro\n",
        "\n",
        "**Reviewers:** Sebastian Ruder, Tom Makkink, Jan Buys\n",
        "\n",
        "**Introduction:** \n",
        "\n",
        "The transformer architecture, introduced in Vaswani et al. 2017's paper [Attention is All You Need](https://arxiv.org/abs/1706.03762?amp=1), has significantly impacted the deep learning field. It has arguably become the de-facto architecture for complex Natural Language Processing (NLP) tasks. It can also be applied in various domains reaching state-of-the-art performance, including computer vision and reinforcement learning.\n",
        "\n",
        "Transformers, as the title of the original paper implies, are almost entirely based on a concept known as attention. Attention allows models to \"focus\" on different parts of an input; while considering the entire context of the input versus an RNN, that operates on the data sequentially.\n",
        "\n",
        "In this practical, we will introduce attention in greater detail and build the entire transformer architecture block by block to see why it is such a robust and powerful architecture.\n",
        "\n",
        "**Topics:** \n",
        "\n",
        "Content: <font color='blue'>`Attention mechanisms, Transformers, NLP`</font>  \n",
        "Level: <font color='grey'>`Advanced`</font>\n",
        "\n",
        "\n",
        "**Aims/Learning Objectives:**\n",
        "\n",
        "- Learn how different attention mechanisms can be implemented.\n",
        "- Learn and create the basic building blocks from scratch for the most common transformer architectures.\n",
        "- Learn how to train a sequence-sequence model.\n",
        "- Learn how to use the [Hugging Face](https://huggingface.co/) library for quicker development cycles and build a chatbot intent model.\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "- Basic understanding of Jax and Haiku\n",
        "- Basic understanding of linear algebra\n",
        "- RNN based sequence-sequence models\n",
        "- Word2Vec\n",
        "\n",
        "**Outline:** \n",
        "\n",
        ">[Installation and Imports](#scrollTo=6EqhIg1odqg0)\n",
        "\n",
        ">[Attention](#scrollTo=-ZUp8i37dFbU)\n",
        "\n",
        ">>[Sequence to sequence attenion mechanisms](#scrollTo=ii__Bc27epiJ)\n",
        "\n",
        ">>[Self-attention](#scrollTo=7RLO7ZHe3spv)\n",
        "\n",
        ">>[Multihead Attention](#scrollTo=7SrT6swYgCm9)\n",
        "\n",
        ">[Transformers](#scrollTo=e9NW58_3hAg2)\n",
        "\n",
        ">>[High level overview](#scrollTo=6HxXjYTFXaal)\n",
        "\n",
        ">>[Tokenisation](#scrollTo=sq0kPkVphbFG)\n",
        "\n",
        ">>[Positional encodings](#scrollTo=Ln7esAMHhdaz)\n",
        "\n",
        ">>[Feed Forwad block](#scrollTo=3l8mzIOeXbrA)\n",
        "\n",
        ">>[Add and Norm](#scrollTo=GQONPcKfXbfj)\n",
        "\n",
        ">>[Building the Encoder](#scrollTo=AIHHQTrcXb1b)\n",
        "\n",
        ">>[Building the Decoder](#scrollTo=RxSD1tLIX4CJ)\n",
        "\n",
        ">>[Putting it all together](#scrollTo=Ts-WLr0zY26X)\n",
        "\n",
        ">>[Training our model to invert sentence](#scrollTo=_bqOml7HYNmm)\n",
        "\n",
        ">[Hugging Face](#scrollTo=rectkTs9iFHg)\n",
        "\n",
        ">>[Datasets Package - Beginner](#scrollTo=Wq_pX3XTefu3)\n",
        "\n",
        ">>[Transformers Package](#scrollTo=iL4j1JZse8wi)\n",
        "\n",
        ">>[Training a chatbot intent model  - Advanced](#scrollTo=QSJ-Fh49fU-9)\n",
        "\n",
        ">[Conclusion](#scrollTo=fV3YG7QOZD-B)\n",
        "\n",
        ">[Feedback](#scrollTo=o1ndpYE50BpG)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Before you start:**\n",
        "\n",
        "For this practical, you will need to use a TPU/GPU to speed up training. To do this, go to the \"Runtime\" menu in Colab, select \"Change runtime type\" and then in the popup menu, choose \"TPU\"/\"GPU\" in the \"Hardware accelerator\" box."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Installation and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4boGA9rYdt9l"
      },
      "outputs": [],
      "source": [
        "## Install and import anything required. Capture hides the output from the cell.\n",
        "# @title Install and import required packages. (Run Cell)\n",
        "\n",
        "!pip install git+https://github.com/deepmind/dm-haiku flax optax\n",
        "!pip install transformers==4.12.1 datasets\n",
        "!pip install seaborn umap-learn\n",
        "!pip install livelossplot\n",
        "\n",
        "import os\n",
        "import math\n",
        "import urllib.request\n",
        "\n",
        "# https://stackoverflow.com/questions/68340858/in-google-colab-is-there-a-programing-way-to-check-which-runtime-like-gpu-or-tpu\n",
        "if int(os.environ[\"COLAB_GPU\"]) > 0:\n",
        "    print(\"a GPU is connected.\")\n",
        "elif \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
        "    print(\"A TPU is connected.\")\n",
        "    import jax.tools.colab_tpu\n",
        "\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "else:\n",
        "    print(\"Only CPU accelerator is connected.\")\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "\n",
        "import haiku as hk\n",
        "import transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "import datasets\n",
        "\n",
        "import optax\n",
        "\n",
        "from PIL import Image\n",
        "from livelossplot import PlotLosses\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "# download images used in notebook\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://images.unsplash.com/photo-1529778873920-4da4926a72c2?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8Y3V0ZSUyMGNhdHxlbnwwfHwwfHw%3D&w=1000&q=80\",\n",
        "    \"cat.png\",\n",
        ")\n",
        "\n",
        "\n",
        "import gensim\n",
        "from nltk.data import find\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"word2vec_sample\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9X10jhocGaS"
      },
      "outputs": [],
      "source": [
        "# @title Helper Functions. (Run Cell)\n",
        "\n",
        "\n",
        "def plot_position_encodings(P, max_tokens, d_model):\n",
        "    \"\"\"Function that takes in a position encoding matrix and plots it.\"\"\"\n",
        "\n",
        "    plt.figure(figsize=(20, np.min([8, max_tokens])))\n",
        "    im = plt.imshow(P, aspect=\"auto\", cmap=\"Blues_r\")\n",
        "    plt.colorbar(im, cmap=\"blue\")\n",
        "\n",
        "    if d_model <= 64:\n",
        "        plt.xticks(range(d_model))\n",
        "    if max_tokens <= 32:\n",
        "        plt.yticks(range(max_tokens))\n",
        "    plt.xlabel(\"Embedding index\")\n",
        "    plt.ylabel(\"Position index\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_image_patches(patches):\n",
        "    \"\"\"Function that takes in a list of patches and plots them.\"\"\"\n",
        "    axes = []\n",
        "    fig = plt.figure(figsize=(25, 25))\n",
        "    for a in range(patches.shape[1]):\n",
        "        axes.append(fig.add_subplot(1, patches.shape[1], a + 1))\n",
        "        plt.imshow(patches[0][a])\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_projected_embeddings(embeddings, labels):\n",
        "    \"\"\"Function that takes in a list of embeddings projects them onto a 2D space and plots them using UMAP.\"\"\"\n",
        "    import umap\n",
        "    import seaborn as sns\n",
        "\n",
        "    projected_embeddings = umap.UMAP().fit_transform(embeddings)\n",
        "\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    plt.title(\"Projected text embeddings\")\n",
        "    sns.scatterplot(\n",
        "        x=projected_embeddings[:, 0], y=projected_embeddings[:, 1], hue=labels\n",
        "    )\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def get_word2vec_embedding(words):\n",
        "    \"\"\"\n",
        "    Function that takes in a list of words and returns a list of their embeddings,\n",
        "    based on a pretrained word2vec encoder.\n",
        "    \"\"\"\n",
        "    word2vec_sample = str(find(\"models/word2vec_sample/pruned.word2vec.txt\"))\n",
        "    model = gensim.models.KeyedVectors.load_word2vec_format(\n",
        "        word2vec_sample, binary=False\n",
        "    )\n",
        "\n",
        "    output = []\n",
        "    words_pass = []\n",
        "    for word in words:\n",
        "        try:\n",
        "            output.append(jnp.array(model.word_vec(word)))\n",
        "            words_pass.append(word)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    embeddings = jnp.array(output)\n",
        "    del model  # free up space again\n",
        "    return embeddings, words_pass\n",
        "\n",
        "\n",
        "def plot_attention_weight_matrix(weight_matrix, x_ticks, y_ticks):\n",
        "    \"\"\"Function that takes in a weight matrix and plots it with custom axis ticks\"\"\"\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    ax = sns.heatmap(weight_matrix, cmap=\"Blues\")\n",
        "    plt.xticks(np.arange(weight_matrix.shape[1]) + 0.5, x_ticks)\n",
        "    plt.yticks(np.arange(weight_matrix.shape[0]) + 0.5, y_ticks)\n",
        "    plt.title(\"Attention matrix\")\n",
        "    plt.xlabel(\"Attention score\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"Function that takes in a string and removes all punctuation.\"\"\"\n",
        "    import re\n",
        "\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfwoGkW3cLuk"
      },
      "outputs": [],
      "source": [
        "# @title Check what device you are using (Run Cell)\n",
        "print(f\"Num devices: {jax.device_count()}\")\n",
        "print(f\" Devices: {jax.devices()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZUp8i37dFbU"
      },
      "source": [
        "## **Attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5MqDkvKiFHb"
      },
      "source": [
        "In order to understand the transformer architecture, one must understand the concept of attention and how it is implemented in deep learning. The attention mechanism is inspired by how humans would look at an image or read a sentence. \n",
        "\n",
        "Let us take the image of the dog in human clothes below (image and example [source](https://lilianweng.github.io/posts/2018-06-24-attention/)). When paying *attention* to the red blocks of pixels, we will say that the yellow block of pointy ears is something we expected (correlated) but that the grey blocks of human clothes are unexpected for us (uncorrelated). This is *based on what we have seen in the past* when looking at pictures of dogs, specifically one of a Shiba Inu. \n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1iEU7Cph2D2PCXp3YEHj30-EndhHAeB5T\" alt=\"drawing\" width=\"450\"/>\n",
        "\n",
        "Assume we want to identify the dog breed in this image. When we look at the red pixels, we tend to pay more *attention* to relevant pixels that are more similar or relevant to them, which could be the ones in the yellow box. We almost completely remove the snow in the background and the human clothing for this task. However, when we begin looking at the background in an attempt to identify what is in it, we will fade out the dog pixels because they are irrelevant to the current task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAnfB8-JsdYA"
      },
      "source": [
        "The same thing happens when we read. In order to understand the entire sentence, we will learn to correlate and *attend to* certain words based on the context of the entire sentence.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1j23kcfu_c3wINU6DUvxzMYNmp4alhHc9\" alt=\"drawing\" width=\"350\"/>\n",
        "\n",
        " For instance, in the first sentence in the image above, when looking at the word \"coding\", we pay more attention to the word \"Apple\" and \"computer\" because we know that when we speak about coding, \"Apple\" is actually referring to the company. However, in the second sentence, we realise we should not consider \" apple \" when looking at \"code\" because given the context of the rest of the sentence, we know that this apple is referring to an actual apple and not a computer.\n",
        "\n",
        "We can build better models by allowing by building mechanisms that mimic attention. It will enable our models to learn better representations of our input data by contextualising what it knows about some parts of the input based on other parts. In the following sections, we will delve deeper into the mechanisms that enable us to train our deep learning models to attend to input data in the context of other input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii__Bc27epiJ"
      },
      "source": [
        "### Sequence to sequence attenion mechanisms - <font color='blue'>`Intermediate`</font>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWXE-pm0hhhX"
      },
      "source": [
        "The first attention mechanisms were used in sequence-to-sequence models. These models were usually RNN encoder and decoder structures. The input sequence was processed sequentially by an RNN, encoding the sequence in a single context vector, which is then fed into another RNN that generates a new sequence. Below is an example of this ([source](https://lilianweng.github.io/posts/2018-06-24-attention/)).\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1FKfaArN1rsLjzVWaJGpMLEcxEshSLXd6\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "Due to there only being one context vector, it was often found that for longer input sequences, information gets lost due to the inability of the encoders to remember longer sequences. The attention mechanism introduced in [Bahdanau et al., 2015](https://arxiv.org/pdf/1409.0473.pdf) was proposed to solve this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knytnRDG62Bb"
      },
      "source": [
        "Here, instead of relying on one static context vector, which is also only used once in the decoding process, let us provide information on the entire input sequence at every decoding step using a dynamic context vector. By doing this, the decoder can access a larger \"bank\" of memory and attend to the input's required information based on the current decoder RNN output state, $s_t$. This is shown below.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1fB5KObXcKo5x35xlIDIcjHTq1q75ejIB\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "In deep learning, attention can be interpreted as a vector of \"importance.\" To predict or infer one element, such as a pixel in an image or a word in a sentence, we estimate how strongly it is correlated with, or \"attends to,\" other elements using the attention vector/weights. These attention weights are then used to generate a new weighted sum of the remaining elements, which represents the target [(source)](https://lilianweng.github.io/posts/2018-06-24-attention/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyBuSnVj4__6"
      },
      "source": [
        "This, usually, consists of two steps for each decoding step $t$: \n",
        "\n",
        "1. Calculate the score (importance) for each $h_n$, given $s_{t-1}$ and generate an attention vector, $w_{n}$. \n",
        "  - $\\text{score} = a(s_{t−1}, h_{n})$, where $a$ can be any differentiable function\n",
        "  - $w_{n} = \\frac{\\exp \\left\\{a\\left(s_{t-1}, h_{n}\\right)\\right\\}}{\\sum_{j=1}^{N} \\exp \\left\\{a\\left(s_{t-1}, h_{j}\\right)\\right\\}}$, where we use the softmax function to generate relative attention weights\n",
        "2. Generate the final context vector, $c_t$\n",
        "  - $c_t=\\sum_{n=1}^{N} w_n h_{n}$ \n",
        "\n",
        "The final state fed into the RNN to generate $s_{t+1}$, is given below, where $f$ can again be any combination method. \n",
        "\n",
        "$s_{t+1} = f\\left ( c_t, s_t \\right)$ \n",
        "\n",
        "In Bahdanau et al., 2015, $f$ was a learned feedforward layer taking in the concatenated vector $[c_t; s_t]$, with $a(s_{t−1}, h_{n})$ being the dot product. Next, let us build up this attention schema."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP5aFhzZQ00Y"
      },
      "source": [
        "In dot product attention, the score is given by\n",
        "\n",
        "$a(s_{t-1}, h_n)=s_{t-1}^\\top h_n$\n",
        "\n",
        "**Code task**: Complete the dot product attention function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1v3EhnW9THCW"
      },
      "outputs": [],
      "source": [
        "def dot_product_attention(hidden_states, previous_state):\n",
        "  \"\"\"\n",
        "  Calculate the dot product between the hidden states and previous states.\n",
        "\n",
        "  Args:\n",
        "    hidden_states: A tensor with shape [T_hidden, dm]\n",
        "    previous_state: A tensor with shape [T_previous, dm]\n",
        "  \"\"\"\n",
        "\n",
        "  scores = # FINISH ME \n",
        "  w_n = # FINSIH ME \n",
        "  c_t = jnp.matmul(w_n, hidden_states)\n",
        "\n",
        "  return w_n, c_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qzChSK0emmY4"
      },
      "outputs": [],
      "source": [
        "# @title Run me to test your code\n",
        "\n",
        "key = jax.random.PRNGKey(42)\n",
        "x = jax.random.normal(key, [2, 2])\n",
        "w_n, c_t = dot_product_attention(x, x)\n",
        "\n",
        "w_n_correct = jnp.array([[0.9567678, 0.04323225], [0.00121029, 0.99878967]])\n",
        "c_t_correct = jnp.array([[0.11144122, 0.95290256], [-1.5571996, -1.5321486]])\n",
        "\n",
        "assert jnp.allclose(w_n_correct, w_n), \"w_n is not calculated correctly\"\n",
        "assert jnp.allclose(c_t_correct, c_t), \"c_t is not calculated correctly\"\n",
        "\n",
        "print(\"It seems correct. Look at the answer below to compare methods.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "APwzi2xmY8Qe"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "def dot_product_attention(hidden_states, previous_state):\n",
        "\n",
        "    # [T,d]*[d,N] -> [T,N]\n",
        "    scores = jnp.matmul(previous_state, hidden_states.T)\n",
        "    w_n = jax.nn.softmax(scores)\n",
        "\n",
        "    # [T,N]*[N,d] -> [T,d]\n",
        "    c_t = jnp.matmul(w_n, hidden_states)\n",
        "\n",
        "    return w_n, c_t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_sdxOrgawet"
      },
      "source": [
        "In order to show how the dot product can produce attention weights that make sense, let us use pretrained [word2vec](https://jalammar.github.io/illustrated-word2vec/) embeddings. These word2vec embeddings are generated by an encoder network that was trained to generate similar embeddings for words with similar meanings. \n",
        "\n",
        "Even though we are not processing something sequentially that contains context, the attention matrix should indicate which words are correlated—and would thus attend to each other.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkGoImvQdX2H"
      },
      "outputs": [],
      "source": [
        "# when changing these words, note that if the word is not in the original\n",
        "# training corpus it will not be shown in the weight matrix plot.\n",
        "words = [\"king\", \"queen\", \"royalty\", \"food\", \"apple\", \"pear\", \"computers\"]\n",
        "word_embeddings, words = get_word2vec_embedding(words)\n",
        "weights, _ = dot_product_attention(word_embeddings, word_embeddings)\n",
        "plot_attention_weight_matrix(weights, words, words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUKsSUr3ql7F"
      },
      "source": [
        "Looking at the matrix, assuming the function was implemented correctly, we can see which words have similar meanings. The \"royal\" group of words have higher attention scores with each other than the \"food\" words, which all attend to one another. We also see that \"computers\" have very low attention scores for all of them, which shows that they are neither very related to \"royal\" or \"food\" words.  \n",
        "\n",
        "**Group task:** \n",
        "  - Play with the word selections above. See if you can find word combinations whose attention values seem counter-intuitive. Think of possible explanations. Which sense of a word did the attention scores capture? \n",
        "  - Ask your friend if they found examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENocLBe1YJMN"
      },
      "source": [
        "Dot product is only one of the ways to implement the scoring function for attention mechanisms, there is a more extensive list in this [blog](https://lilianweng.github.io/posts/2018-06-24-attention/#summary) post by Dr Lilian Weng. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUkb7CA30E8q"
      },
      "source": [
        "More resources:\n",
        "\n",
        "[A basic encoder-decoder model for machine translation](https://www.youtube.com/watch?v=gHk2IWivt_8&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=1) \n",
        "\n",
        "[Training and loss for encoder-decoder models](https://www.youtube.com/watch?v=aBZUTuT1Izs&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=2)\n",
        "\n",
        "[Basic attention](https://www.youtube.com/watch?v=BSSoEtv5jvQ&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x22cVV00c9ZK"
      },
      "source": [
        "### Self-attention to Multihead Attention- <font color='blue'>`Intermediate`</font>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnfU62CY4PmW"
      },
      "source": [
        "Self-attention and multi-head attention (MHA) are the core building blocks for the transformer architecture. We will build up the intuition and implementation here in detail. Then in the **Transformers** section, you will see how this mechanism is utilised to build an attention only sequence-to-sequence model.\n",
        "\n",
        "\n",
        "Going forward in this section, we will represent a sentence by splitting it up into a list of words, then using the word2vec model from above to encode each word. In the transformers section, we will dive deeper into how we transform an input into a sequence of vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xo1zsFH969Tf"
      },
      "outputs": [],
      "source": [
        "def embed_sentence(sentence):\n",
        "    # Embed a sentence using word2vec; for example use cases only.\n",
        "    sentence = remove_punctuation(sentence)\n",
        "    words = sentence.split()\n",
        "    word_vector_sequence, words = get_word2vec_embedding(words)\n",
        "    return jnp.expand_dims(word_vector_sequence, axis=0), words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RLO7ZHe3spv"
      },
      "source": [
        "#### Self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeDD9MdEho7D"
      },
      "source": [
        "Self-attention is an attention mechanism where each vector of a given input sequence attends to the entire sequence. To gain an intuition for why self-attention is important, let us think about the following sentence (example taken from [source](https://jalammar.github.io/illustrated-transformer/)):\n",
        "\n",
        "`\"The animal didn't cross the street because it was too tired.\"`\n",
        "\n",
        "A simple question about this sentence is what the word \"it\" refers to? Even though it might look simple, it can be tough for an algorithm to learn this. This is where self-attention comes in, as it can learn an attention matrix for the word \"it\" where a large weight is assigned to the word \"animal\".\n",
        "\n",
        "Self-attention also allows the model to learn how to interpret words with the same embeddings, such as apple, which can be a company or food, depending on the context. This is very similar to the hidden state found within an RNN, but this process, as you will see, allows the model to attend over the entire sequence in parallel, allowing longer sequences to be utilised. \n",
        "\n",
        "Self-attention consists of three concepts:\n",
        "\n",
        "- Queries, keys and values\n",
        "- Scaled dot product attention\n",
        "- Masks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9SFONkT5Ds3"
      },
      "source": [
        "##### **Queries, keys and values**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVISm9pK5-FR"
      },
      "source": [
        "Typically all attention mechanisms can be written in terms of `key-value` pairs and `queries` to calculate the attention matrix and new context vector. \n",
        "\n",
        "To gain intuition, one can interpret the `query` vector as containing the information we are interested in, which is used to determine the `values` we should attend to, based on the similarity between the `keys` (which are paired with the `values`) and the `query`. Thus the similarity between the `queries` and `keys` gives us our attention score, where that score then determines the attention put in conjunction with the `values`. Or as [Lena Voita](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html) puts it:\n",
        "\n",
        "- Query: asking for information\n",
        "- Key: saying that it has some information\n",
        "- Value: giving the information\n",
        "\n",
        "In transformer architectures, we use learnable weights matrices, represented as $W_Q,W_K,W_V$, to project each sequence vector to unique $q$, $k$, and $v$ vectors. \n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1-96YjPxhcqW6FczUYwErGXHp6YpoLltq\" alt=\"drawing\" width=\"600\"/>\n",
        "\n",
        "You will notice that the vectors $q,k,v$ are smaller in size than the input vectors. This will be covered at a later stage, but just know that it is a design choice for transformers and not required at all to work.\n",
        "\n",
        "This process can also be parallelised, as the input sequence can be represented as a matrix $X$, which can be transformed into query, key, and value matrices $Q$, $K$, and $V$ respectively:\n",
        "\n",
        "$Q=W_QX \\\\ K=W_KX \\\\ V=W_VX$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqPxQdvjOQZN"
      },
      "source": [
        "**Code Task**: Code up a Haiku module that creates three linear layers such that an input can be projected down to `output_size` in separate matrices $Q,K,V$. Hint: Use`hk.Linear`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeSPp3cmJhLd"
      },
      "outputs": [],
      "source": [
        "class SequenceToQKV(hk.Module):\n",
        "\n",
        "  '''\n",
        "  Project a sequence of embeddings to query, key and value matrices.\n",
        "\n",
        "  Args:\n",
        "    output_size: The embedding dimension for the Q,K,V matrices.\n",
        "  '''\n",
        "\n",
        "  def __init__(self, output_size):\n",
        "  \n",
        "    super().__init__()\n",
        "\n",
        "    self.output_size = output_size\n",
        "\n",
        "  def __call__(self, X):\n",
        "    '''\n",
        "    Project X to Q,K,V matrices.\n",
        "\n",
        "    Args:\n",
        "      X: Sequence of embeddings with shape [B,T,d], where T is sequence length.\n",
        "    Returns: \n",
        "      Q,K,V: The three projected matrices with shape [B,T,output_size]\n",
        "    '''\n",
        "\n",
        "    initializer = hk.initializers.VarianceScaling(0.5)\n",
        "    \n",
        "    # this can also be one layer, how do you think you would do it?\n",
        "    q_layer = # FINISH ME\n",
        "    k_layer = # FINISH ME\n",
        "    v_layer = # FINISH ME\n",
        "\n",
        "    Q = q_layer(X)\n",
        "    K = k_layer(X)\n",
        "    V = v_layer(X)\n",
        "\n",
        "    return Q, K, V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "H921UuRSmuD1"
      },
      "outputs": [],
      "source": [
        "# @title Run me to test your code\n",
        "def x_to_qkv(sequences):\n",
        "    qkv_transforms = SequenceToQKV(2)\n",
        "    return qkv_transforms(sequences)\n",
        "\n",
        "\n",
        "x_to_qkv = hk.transform(x_to_qkv)\n",
        "\n",
        "# initialise model\n",
        "key = jax.random.PRNGKey(42)\n",
        "x = jax.random.normal(key, [1, 1, 1])\n",
        "params = x_to_qkv.init(key, x)\n",
        "Q, K, V = x_to_qkv.apply(params, key, x)\n",
        "\n",
        "Q_correct = jnp.array([[[-0.22509536, 0.03467613]]])\n",
        "K_correct = jnp.array([[[-0.01319892, 0.17290935]]])\n",
        "V_correct = jnp.array([[[-0.07143524, -0.01425235]]])\n",
        "\n",
        "assert jnp.allclose(Q_correct, Q), \"Q is not calculated correctly\"\n",
        "assert jnp.allclose(K_correct, K), \"K is not calculated correctly\"\n",
        "assert jnp.allclose(V_correct, V), \"V is not calculated correctly\"\n",
        "\n",
        "print(\"It seems correct. Look at the answer below to compare methods.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qoII_QovZCkJ"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "\n",
        "\n",
        "class SequenceToQKV(hk.Module):\n",
        "    def __init__(self, output_size):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, X):\n",
        "        initializer = hk.initializers.VarianceScaling(0.5)\n",
        "\n",
        "        # this can also be one layer, how do you think you would do it?\n",
        "        q_layer = hk.Linear(self.output_size, w_init=initializer)\n",
        "        k_layer = hk.Linear(self.output_size, w_init=initializer)\n",
        "        v_layer = hk.Linear(self.output_size, w_init=initializer)\n",
        "\n",
        "        Q = q_layer(X)\n",
        "        K = k_layer(X)\n",
        "        V = v_layer(X)\n",
        "\n",
        "        return Q, K, V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvbKqrRniVDa"
      },
      "source": [
        "##### **Scaled dot product attention**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpWykKCgGrdR"
      },
      "source": [
        "Now that we have our `query`, `key` and `value` matrices, it is time to calculate the attention matrix. Remember, in attention mechanisms; we must first find a score for each sequence vector and then use these scores to create a new context vector. We do this in self-attention using scaled dot product attention with the formula below. \n",
        "\n",
        "$\\operatorname{Attention}(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V$\n",
        "\n",
        "What happens here is similar to what we did in the dot product attention in the previous section, just applying the mechanism to the sequence itself. For each element in the sequence, we calculate the attention weight matrix between $q_i$ and $K$. We then multiply $V$ by each weight and finally sum all weighted vectors $v_{weighted}$ together to form a new representation for $q_i$. By doing this, we are essentially drowning out irrelevant vectors and bringing up important vectors in the sequence when our focus is on $q_1$.\n",
        "\n",
        "$QK^\\top$ is scaled by the square root of the dimension of the vectors, $\\sqrt{d_k}$, to ensure more stable gradients during training. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRO-fhwmodb6"
      },
      "source": [
        "**Code Task:** Code up the scaled dot product attention function. This does not have to be a Haiku module as we are just doing matrix multiplications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNQ8Dgx2Wl8Q"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value):\n",
        "  '''\n",
        "  Calculate the scaled dot product attention using \n",
        "  query, key and value tokens.\n",
        "\n",
        "  Args:\n",
        "    query: Query matrix, with shape [B,T,d]\n",
        "    key: Key matrix, with shape [B,T_key,d]\n",
        "    value: Value matrix, with shape [B,T_key,d]\n",
        "\n",
        "  Returns:\n",
        "    value: Weighted sum of values with shape [B,T,d]\n",
        "    attention_weights: The attention weights between query and key, shape [B,T,T_key]\n",
        "  '''\n",
        "  \n",
        "  d_k = key.shape[-1]\n",
        "  logits = # FINSIH ME\n",
        "  scaled_logits = # FINISH ME\n",
        "  attention_weights = jax.nn.softmax(scaled_logits, axis=-1) \n",
        "  value = jnp.matmul(attention_weights, value)\n",
        "  return value, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Xw4b5UYOmyw3"
      },
      "outputs": [],
      "source": [
        "# @title Run me to test your code\n",
        "key = jax.random.PRNGKey(42)\n",
        "x = jax.random.normal(key, [1, 2, 3])\n",
        "value, weights = scaled_dot_product_attention(x, x, x)\n",
        "\n",
        "value_correct = jnp.array(\n",
        "    [[[0.5013704, 0.965935, 1.0586468], [-0.57923466, -0.56055915, 0.2919087]]]\n",
        ")\n",
        "weights_correct = jnp.array([[[0.9221789, 0.0778211], [0.16385838, 0.8361416]]])\n",
        "\n",
        "assert jnp.allclose(value_correct, value), \"value is not calculated correctly\"\n",
        "assert jnp.allclose(\n",
        "    weights_correct, weights\n",
        "), \"attention_weights is not calculated correctly\"\n",
        "\n",
        "print(\"It seems correct. Look at the answer below to compare methods.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lDq7CAcAd49i"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(query, key, value):\n",
        "    d_k = key.shape[-1]\n",
        "    logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
        "    scaled_logits = logits / jnp.sqrt(d_k)\n",
        "    attention_weights = jax.nn.softmax(scaled_logits, axis=-1)\n",
        "    value = jnp.matmul(attention_weights, value)\n",
        "    return value, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqVTaqqhir4e"
      },
      "source": [
        "Let's now see scaled dot product attention in action. We will take a sentence, embed each word using word2vec, and see what the final self-attention weights look like.\n",
        "\n",
        "We will not use the linear projection layers as they are not trained. Instead, we are going to make $X=Q=V=K$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVBq5ie0iz8G"
      },
      "outputs": [],
      "source": [
        "sentence = \"I drink coke, but eat steak\"\n",
        "word_embeddings, words = embed_sentence(sentence)\n",
        "Q = K = V = word_embeddings\n",
        "\n",
        "# calculate weights and plot\n",
        "values, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
        "words = remove_punctuation(sentence).split()\n",
        "plot_attention_weight_matrix(attention_weights[0], words, words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tJJuTztnbdy"
      },
      "source": [
        "Keep in mind that we have not trained our attention matrix yet. However, we can see that by utilising the word2vec vectors as our sequence, we can see how scaled dot product attention already is capable of attending to \"eat\" when \"steak\" is our query and that the query \"drink\" attends more to \"coke\" and \"eat\". "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3ihUTrl_cxh"
      },
      "source": [
        "**Group task:** See if you can find any interesting results using the above, untrained, attention weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IheQSi6c0E8u"
      },
      "source": [
        "More resources:\n",
        "\n",
        "[Attention with Q,K,V](https://www.youtube.com/watch?v=k-5QMalS8bQ&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rd7xAVznfBQU"
      },
      "source": [
        "##### **Masked attention** \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNC0zv5eB4sa"
      },
      "source": [
        "There are cases where applying self-attention over the entire sequence is not practical. These can include:\n",
        "\n",
        "- Uneven length sequences batched together.\n",
        "  - When sending a batch of sequences through a network, the self-attention expects each sequence to be the same length. One handles this by padding the sequence. When calculating attention, ideally, these padding vectors should not be taken into consideration\n",
        "- Training a decoding model.\n",
        "  - When training decoder models, such as GPT-3, the decoder has access to the entire target sequence when training (as training is done in parallel). In order to prevent the method from cheating by looking at future tokens, we have to mask the future sequence data so that earlier data can not attend to it.\n",
        " \n",
        "By applying a mask to the final score calculated between queries and keys, we mitigate the influence of the unwanted sequence vectors. The vectors are masked by making the score between the query and their respective keys a VERY large negative value. This results in the softmax function pushing the attention weight very close to zero, and the resulting value will be summed out and not influence the final representation.\n",
        "\n",
        "\n",
        "Putting everything together, masked scaled dot product attention visually looks like this:\n",
        "\n",
        "<img src=\"https://windmissing.github.io/NLP-important-papers/AIAYN/assets/5.png\" alt=\"drawing\" width=\"200\"/>. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qm1cMDA_hQIr"
      },
      "outputs": [],
      "source": [
        "# example of building a mask for tokens of size 32\n",
        "mask = jnp.tril(jnp.ones((32, 32)))\n",
        "sns.heatmap(mask, cmap=\"Blues\")\n",
        "plt.title(\"Example of mask that can be applied\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQJtf6sCfBQY"
      },
      "source": [
        "**Code Task:** Try and implement the masking operation for your SCD function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LICrxTN5fBQZ"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "  '''\n",
        "  Calculate the scaled dot product attention using \n",
        "  query, key and value tokens with mask being applied.\n",
        "\n",
        "  Args:\n",
        "    query: Query matrix, with shape [B,T,d]\n",
        "    key: Key matrix, with shape [B,T_key,d]\n",
        "    value: Value matrix, with shape [B,T_key,d]\n",
        "    mask: Mask to be applied, with shape [T_mask, T_mask], where T_mask>=max(T_key,T)\n",
        "\n",
        "  Returns:\n",
        "    value: Weighted sum of values with shape [B,T,d]\n",
        "    attention_weights: The attention weights between query and key, shape [B,T,T_key]\n",
        "  '''\n",
        "  d_k = key.shape[-1]\n",
        "  T_k = key.shape[1]\n",
        "  T_q = query.shape[1]\n",
        "  logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
        "  scaled_logits = logits/jnp.sqrt(d_k)\n",
        "\n",
        "  if mask is not None:\n",
        "    scaled_logits = # FINISH ME\n",
        "\n",
        "  attention_weights = jax.nn.softmax(scaled_logits, axis=-1) \n",
        "  attention = jnp.matmul(attention_weights, value)\n",
        "  return attention, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8M-zuP8jm4LN"
      },
      "outputs": [],
      "source": [
        "# @title Run me to test your code\n",
        "mask = jnp.tril(jnp.ones((2, 2)))\n",
        "\n",
        "key = jax.random.PRNGKey(42)\n",
        "x = jax.random.normal(key, [1, 2, 3])\n",
        "\n",
        "value, weights = scaled_dot_product_attention(x, x, x, mask)\n",
        "\n",
        "value_correct = jnp.array(\n",
        "    [[[0.6122652, 1.1225883, 1.1373317], [-0.57923466, -0.56055915, 0.2919087]]]\n",
        ")\n",
        "weights_correct = jnp.array([[[1.0, 0.0], [0.16385838, 0.8361416]]])\n",
        "\n",
        "assert jnp.allclose(value_correct, value), \"value is not calculated correctly\"\n",
        "assert jnp.allclose(\n",
        "    weights_correct, weights\n",
        "), \"attention_weights is not calculated correctly\"\n",
        "\n",
        "print(\"It seems correct. Look at the answer below to compare methods.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iE1YNtLkfBQZ"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    d_k = key.shape[-1]\n",
        "    T_k = key.shape[-2]\n",
        "    T_q = query.shape[-2]\n",
        "    logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
        "    scaled_logits = logits / jnp.sqrt(d_k)\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_logits = jnp.where(mask[:T_q, :T_k], scaled_logits, -jnp.inf)\n",
        "\n",
        "    attention_weights = jax.nn.softmax(scaled_logits, axis=-1)\n",
        "    attention = jnp.matmul(attention_weights, value)\n",
        "    return attention, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAJnz3uOmHvC"
      },
      "source": [
        "In order to test if this works, lets apply a masked attention to our sequence, as one would do in a autoregressive setting, where the current output is only allowed to attend to the current and previous outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lq5Fnf2Xnd9D"
      },
      "outputs": [],
      "source": [
        "mask = jnp.tril(np.ones((word_embeddings.shape[1], word_embeddings.shape[1])))\n",
        "\n",
        "attention, attention_weights = scaled_dot_product_attention(\n",
        "    word_embeddings, word_embeddings, word_embeddings, mask\n",
        ")\n",
        "\n",
        "plt.title(\"Attention weights with masked applied\")\n",
        "sns.heatmap(attention_weights[0], cmap=\"Blues\");"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SrT6swYgCm9"
      },
      "source": [
        "#### Multihead Attention - <font color='blue'>`Advanced`</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eILi2mwrhtCD"
      },
      "source": [
        "Rather than only computing the attention once, the multi-head attention (MHA) mechanism runs through the scaled dot-product attention multiple times in parallel. According to the paper, Attention is all you need, \"multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this\". \n",
        "\n",
        "Multi-head attention can be viewed as a similar strategy to stacking convolution kernels in a CNN layer. This allows the kernels to focus on and learn different features and rules, which is why multiple heads of attention also work. The process for MHA is given below.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1q0Oq6IVEkkMfVSpY4LkHBP866mcoIFsh\" alt=\"drawing\" width=\"1000\"/>\n",
        "\n",
        "As can be seen from the figure, the scaled dot product attention discussed earlier is just repeated $N$ times, with $3N$ learnable matrices for each head. The outputs from the different heads are then concatenated, whereafter it is fed through a linear projection, which produces the final representation. \n",
        "\n",
        "Due to these large amount of computations and memory requirements, a common design choice is to have the $W_{Qn}, W_{Kn}, W_{Vn}$ matrices produce embeddings of length $d_m/N$, where $d_m$ is the input sequence embedding size and $N$ is the number heads. By doing this, the MHA function is similar computation-wise to using a single head of attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXZPL9qVci0I"
      },
      "source": [
        "**Code Task:** Code up a Haiku module that implements the entire multi-head attention mechanism. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Px2JQqOnYs17"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(hk.Module):\n",
        "\n",
        "  '''\n",
        "  Apply multi-head attention to a sequence of embeddings, or to Q, K and V\n",
        "  matrices if they are supplied.\n",
        "\n",
        "  Args:\n",
        "    num_heads: Number of heads\n",
        "    d_m: Embedding/Token embeddings size\n",
        "  '''\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      num_heads,\n",
        "      d_m,\n",
        "      name = None,\n",
        "  ):\n",
        "    super().__init__(name=name)\n",
        "    self.num_heads = num_heads \n",
        "    self.sequence_to_qkv = SequenceToQKV(d_m)  # Implemented in the previous exercises\n",
        "\n",
        "  def __call__(self, X=None, Q=None, K=None, V=None, mask = None, return_weights=False):\n",
        "    '''\n",
        "    Args:\n",
        "      X: Sequence of embeddings, with shape [B,T,dm]. If not provided, must provide Q,K,V\n",
        "      Q: Query matrix, with shape [B,T,dm]. Must be provided if X is not\n",
        "      K: Query matrix, with shape [B,T_key,dm]. Must be provided if X is not\n",
        "      V: Query matrix, with shape [B,T_key,dm]. Must be provided if X is not\n",
        "      mask [optional, default=None]: Mask to be applied, with shape [T_mask, T_mask], where T_mask>=max(T_key,T).\n",
        "      return_weights [optional, default=False]: Whether to return attention weights calculated\n",
        "\n",
        "    Returns:\n",
        "      X_new: Transformed sequence of embeddings, with shape [B,T,dm] \n",
        "      attention_weights: Returned when return_weights=True, with shape [B,num_heads,T,T_key]\n",
        "\n",
        "    '''\n",
        "\n",
        "\n",
        "    if None in [Q, K, V]:\n",
        "      assert not X is None, 'X has to be provided if either Q,K,V not provided'\n",
        "  \n",
        "      # project all data to Q, K, V\n",
        "      Q,K,V = # FINISH ME\n",
        "\n",
        "    # get the batch size, sequence length and embedding size\n",
        "    B, T, d_m = K.shape\n",
        "\n",
        "    # calculate heads embedding size (d_m/N)\n",
        "    head_size = d_m // self.num_heads\n",
        "\n",
        "    # B,T,d_m -> B, T, N, dm//N -> B, N, T, dm//N\n",
        "    q_heads = # FINSIH ME (REMEMBER, Q.shape is not always = K.shape)\n",
        "    k_heads = # FINISH ME\n",
        "    v_heads = # FINISH ME\n",
        "    \n",
        "    attention, attention_weights = scaled_dot_product_attention(q_heads,k_heads,v_heads,mask)\n",
        "\n",
        "    # (B, nh, T, hs) -> (B, T, nh, hs) -> (B, T, d_m) - re-assemble all head outputs\n",
        "    attention = # FINISH ME\n",
        "\n",
        "    # apply Wo\n",
        "    initializer = hk.initializers.VarianceScaling(0.5)\n",
        "    Wo = hk.Linear(d_m, w_init=initializer)\n",
        "    X_new = # FINISH ME\n",
        "\n",
        "    if return_weights:\n",
        "      return X_new, attention_weights \n",
        "    else:\n",
        "      return X_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UjK0cPTRm9Cr"
      },
      "outputs": [],
      "source": [
        "# @title Run me to test your code\n",
        "def mha_test(sequences):\n",
        "    mha = MultiHeadAttention(2, 8)\n",
        "    return mha(sequences)\n",
        "\n",
        "\n",
        "mha_test = hk.transform(mha_test)\n",
        "\n",
        "# initialise model\n",
        "key = jax.random.PRNGKey(42)\n",
        "x = jax.random.normal(key, [1, 2, 8])\n",
        "params = mha_test.init(key, x)\n",
        "x_new = mha_test.apply(params, key, x)\n",
        "\n",
        "x_correct = jnp.array(\n",
        "    [\n",
        "        [\n",
        "            [\n",
        "                0.8122552,\n",
        "                -0.3184167,\n",
        "                0.54364735,\n",
        "                1.0808588,\n",
        "                0.28692305,\n",
        "                0.44296914,\n",
        "                -0.0463711,\n",
        "                0.12337256,\n",
        "            ],\n",
        "            [\n",
        "                0.8388398,\n",
        "                -0.39584604,\n",
        "                0.30241585,\n",
        "                0.9982894,\n",
        "                0.3577416,\n",
        "                0.35978147,\n",
        "                -0.07165723,\n",
        "                -0.18380073,\n",
        "            ],\n",
        "        ]\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "assert jnp.allclose(x_correct, x_new), \"Not returning the correct value\"\n",
        "print(\n",
        "    \"It seems correct. Look at the answer below to compare methods then move to the transformers section.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ozNp_pfhc12I"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "class MultiHeadAttention(hk.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_heads,\n",
        "        d_m,\n",
        "        name=None,\n",
        "    ):\n",
        "        super().__init__(name=name)\n",
        "        self.num_heads = num_heads\n",
        "        self.sequence_to_qkv = SequenceToQKV(d_m)\n",
        "\n",
        "    def __call__(self, X=None, Q=None, K=None, V=None, mask=None, return_weights=False):\n",
        "        if None in [Q, K, V]:\n",
        "            assert not X is None, \"X has to be provided if either Q,K,V not provided\"\n",
        "\n",
        "            # project all data to Q, K, V\n",
        "            Q, K, V = self.sequence_to_qkv(X)\n",
        "\n",
        "        # get the batch size, sequence length and embedding size\n",
        "        B, T, d_m = K.shape\n",
        "\n",
        "        # calculate heads embedding size (d_m/N)\n",
        "        head_size = d_m // self.num_heads\n",
        "\n",
        "        # B,T,d_m -> B, T, N, dm//N -> B, N, T, dm//N\n",
        "        q_heads = Q.reshape(B, -1, self.num_heads, head_size).swapaxes(1, 2)\n",
        "        k_heads = K.reshape(B, -1, self.num_heads, head_size).swapaxes(1, 2)\n",
        "        v_heads = V.reshape(B, -1, self.num_heads, head_size).swapaxes(1, 2)\n",
        "\n",
        "        attention, attention_weights = scaled_dot_product_attention(\n",
        "            q_heads, k_heads, v_heads, mask\n",
        "        )\n",
        "\n",
        "        # (B, nh, T, hs) -> (B, T, nh, hs) -> (B, T, d_m) - re-assemble all head outputs\n",
        "        attention = attention.swapaxes(1, 2).reshape(B, -1, d_m)\n",
        "\n",
        "        # apply Wo\n",
        "        initializer = hk.initializers.VarianceScaling(0.5)\n",
        "        Wo = hk.Linear(d_m, w_init=initializer)\n",
        "        X_new = Wo(attention)\n",
        "\n",
        "        if return_weights:\n",
        "            return X_new, attention_weights\n",
        "        else:\n",
        "            return X_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w05LJDmGtXX3"
      },
      "source": [
        "Now that we have an understanding for how the MHA works, lets see how the transformer architecture uses them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WILOYJH4gCnD"
      },
      "source": [
        "### Section Quiz \n",
        "\n",
        "[Test knowledge on all the previous material of attention]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "N5iFeOKOgCnE"
      },
      "outputs": [],
      "source": [
        "# @title Generate Quiz Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe \n",
        "\tsrc=\"https://forms.gle/h6jjUePGPxGdWkjW8\",\n",
        "  width=\"80%\" \n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9NW58_3hAg2"
      },
      "source": [
        "## **Transformers**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3NUhE2jUo8O"
      },
      "source": [
        "Attention, and specifically MHA, is the backbone of transformer architectures. Let us now use what we learned and built in the previous section to implement a transformer architecture in Haiku for a simple task: inverting a sentence. Each block will be built up from scratch and will be able to do more complex tasks.\n",
        "\n",
        "<img src=\"https://media.geeksforgeeks.org/wp-content/cdn-uploads/reverse-Words.jpg\" width=\"250\"/>\n",
        "\n",
        "We base a lot of our code on the [Deepmind Haiku example](https://github.com/deepmind/dm-haiku/tree/main/examples/transformer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HxXjYTFXaal"
      },
      "source": [
        "### High level overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxjJTlrhajcq"
      },
      "source": [
        "Originally the transformer was designed for machine translation, hence the encoder-decoder structure seen below. \n",
        "\n",
        "The encoder will receive an input sentence in one language and process it through multiple stacked `encoder blocks`. This creates a final representation, which contains helpful information necessary for the decoding task. This output is then fed into stacked `decoder blocks` that produce new outputs in an autoregressive manner. \n",
        "\n",
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" width=\"350\" />\n",
        "\n",
        "\n",
        "The encoder consists of $N$ identical blocks, which process a sequence of token vectors sequentially. These blocks consist of 3 parts:\n",
        "\n",
        "1. A multi-head attention block. These are the transformer architecture's backbone. They process the data to generate representations for each token, ensuring that the necessary information for the task at hand is represented in the vectors. These are exactly the MHA we covered in the attention section previously. \n",
        "2. An MLP is applied to each input token separately and identically.\n",
        "3. Residual connection that adds the input tokens to the attended representations and a residual connection between the input to the MLP and its outputs. For both these connections, the result is normalized using layernorm. In certain implementations, these normalization steps are applied to the inputs rather than the outputs. Just like a Resnet, transformers are designed to be very deep models thus, these add and norm blocks are essential for a smooth gradient flow.  \n",
        "\n",
        "The decoder block consists of the same parts, except for one extra MHA block that appears after the initial MHA block. This block receives the output of the final encoder block, the transformed tokens, and uses that as the key-value pairs, while using the output of the first MHA block as the query. In doing this, the model attends over the input required to perform the sequence task. This MHA block thus performs _cross-attention_ by looking at the encoder inputs while the first MHA block performs _self-attention_ on the output sequence. The initial MHA block is also masked in a decoder to block future tokens when making predictions. \n",
        "\n",
        "This entire architecture can then be trained end to end. These models expect data to be broken up into a sequence of vectors to be processed. Next, we will deep dive into how this is done.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sq0kPkVphbFG"
      },
      "source": [
        "### Tokenisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wd4hPASBiEGe"
      },
      "source": [
        "Before data can be fed into transformers, it has to be transformed into an acceptable format: a sequence of tokens and a vector representing each token. Below, we briefly discuss how we can transform text and vision data into tokens that transformers can process [source](https://huggingface.co/docs/transformers/preprocessing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8GylOMmBW4n"
      },
      "source": [
        "##### **Text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS_jLXJw0kdN"
      },
      "source": [
        "\n",
        "Transformers can not handle raw strings of text. So to process text, the text is first split up into tokens. The tokens are then indexed and each token is assigned an embedding of size $d_{model}$. These embeddings can be learned during training or can come from a pretrained vocabulary of embeddings. This new sequence of token embeddings is then fed into the transformer architecture. This idea is visualised below. \n",
        "\n",
        "\\\\\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=16euh4LADP_mcXywFwKKY3QQQkVplepiI\" alt=\"drawing\" width=\"450\"/>\n",
        "\n",
        "\n",
        "These token IDs are typically predicted when a model generates text, fills in missing words, etc.\n",
        "\n",
        "This process of splitting up text into tokens and assigning an ID to each token is called [tokenisation](https://huggingface.co/docs/transformers/tokenizer_summary). There are various ways to tokenise text, with some methods being trained directly from the data. When using pre-trained transformers, it is crucial to use the same tokeniser that was used to train the model. The previous link has in-depth descriptions of many widely known techniques.\n",
        "\n",
        "Below we show how the [BERT](https://arxiv.org/abs/1810.04805) model's tokeniser tokenises a sentence. We use [Hugging Face](https://huggingface.co/) for this part. We will have a deep dive into Hugging Face later in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0C-3v0y0iZe"
      },
      "outputs": [],
      "source": [
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "encoded_input = bert_tokenizer(\"The practical is so much fun\")\n",
        "print(f\"Token IDs: {encoded_input['input_ids']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-408xpj794l"
      },
      "source": [
        "Here we can see that the tokeniser returns the IDs for each token, as shown in the figure. But counting the number of IDs, we see that it is larger than the number of words in the sentence. Let's print the tokens associated with each ID.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dYMGIgi8p-q"
      },
      "outputs": [],
      "source": [
        "print(f\"Tokens: {bert_tokenizer.decode(encoded_input['input_ids'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phjOR7ftAVt_"
      },
      "source": [
        "We can see the tokeniser attaches new tokens, `[CLS]` and `[SEP]`, to the start and end of the sequence. This is a BERT-specific requirement for training and inference. Adding special tokens is a very common thing to do. Using special tokens, we can tell a model when a sentence starts or ends or when a new part of the input starts. This can be helpful when performing different tasks. \n",
        "\n",
        "For instance, to pretrain specific transformers, they perform what is known as masked prediction. For this, random tokens in a sequence are replaced by the `[MASK]` token, and the model is trained to predict the correct token ID for the token replaced with that token. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syL_uNAMVHQB"
      },
      "source": [
        "**Group task**:\n",
        "\n",
        "- Discuss with a friend what you think the current issue is when feeding these raw tokens into a transformer architecture? Think about the difference in meaning between a sentence and that same sentence where the word orders are random.\n",
        "- How would you fix that issue? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jWreoeTBUCD"
      },
      "source": [
        "##### **Images - <font color='blue'>`Advanced`</font>** (optional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcyHr8a_BdaT"
      },
      "source": [
        "As mentioned, transformers are versatile and can be applied to roughly any data which can be converted into a sequence of tokens.\n",
        "\n",
        "For example, to use the transformers encoder architecture with images, one can split an image into different patches, flatten these image patches and project each image patch into a fixed-sized embedding using any projection method. By doing this, the image has been converted into a sequence of image tokens, and the transformer will be able to process the data. \n",
        "\n",
        "This process is shown in the image below. \n",
        "\n",
        "\\\\\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ERF0f3Y_0wNb4kQ07xMYUysqNYIPkQMD\" alt=\"drawing\" width=\"650\"/>\n",
        "\n",
        "\n",
        "**Code task (OPTIONAL and ADVANCED):** Write a function that can take in a batch of images with shape (Batch, Height, Widht, Channels) and divide it into equal-sized patches. You can use the output of `image_to_patch` and `plot_image_patches` functions to visualise and test your function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htWcVGlDHcba"
      },
      "outputs": [],
      "source": [
        "def image_to_patch(image, patch_size):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        x - array of shape [B, H, W, C]\n",
        "        patch_size - Number of pixels per dimension of the patches (integer)\n",
        "\n",
        "    Outputs:\n",
        "        patches - array of shape [B, H'*W', p_H, p_W, C]\n",
        "    \"\"\"\n",
        "    B, H, W, C = image.shape # HINT: You will need these\n",
        " \n",
        "    image = # FINISH ME\n",
        "\n",
        "    return image_patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oqFn3peRTqm4"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "\n",
        "\n",
        "def image_to_patch(image, patch_size):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        x - array of shape [B, H, W, C]\n",
        "        patch_size - Number of pixels per dimension of the patches (integer)\n",
        "    \"\"\"\n",
        "    B, H, W, C = image.shape\n",
        "    image = image.reshape(\n",
        "        B, H // patch_size, patch_size, W // patch_size, patch_size, C\n",
        "    )\n",
        "    image = image.transpose(0, 1, 3, 2, 4, 5)  # [B, H', W', p_H, p_W, C]\n",
        "    image_patches = image.reshape(B, -1, *image.shape[3:])  # [B, H'*W', p_H, p_W, C]\n",
        "    return image_patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBXMQInfTuJK"
      },
      "outputs": [],
      "source": [
        "# do not change these lines, only run them to test your function\n",
        "print(\"Original image:\")\n",
        "image = np.array(Image.open(\"cat.png\"))\n",
        "plt.imshow(image)\n",
        "plt.show()\n",
        "\n",
        "print(\"Image broken into patches\")\n",
        "img = jnp.array(image)\n",
        "img = jax.image.resize(img, (512, 512, 3), \"nearest\")\n",
        "img = jnp.expand_dims(img, 0)\n",
        "patches = image_to_patch(img, 256)\n",
        "plot_image_patches(patches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln7esAMHhdaz"
      },
      "source": [
        "\n",
        "### Positional encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8MrN6YJXUM_"
      },
      "source": [
        "In most domains where a transformer can be utilised, there is an underlying order to the tokens produced, be it the order of words in a sentence, the location from which patches are taken in an image or even the steps taken in an RL environment. This order is very important in all cases; just imagine you interpret the sentence \"I have to read this book.\" as \"I have this book to read.\". Both sentences contain the exact same words, yet they have completely different meanings based on the order. \n",
        "\n",
        "As both the encoder and the decoder blocks process all tokens in parallel, the order of tokens is lost in these calculations. To cope with this, the sequence order has to be injected into the tokens directly. This can be done by adding *positional encodings* to the tokens at the start of the encoder and decoder blocks (though some of the latest techniques add positional information in the attention blocks). An example of how positional encodings alter the tokens is shown below.\n",
        "\n",
        "\n",
        "\\\\\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1eSgnVN2hnEsrjdHygDGwk1kxEi8-dcFo\" alt=\"drawing\" width=\"650\"/>\n",
        "\n",
        "Ideally, these encodings should have these characteristics ([source](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)):\n",
        "* Each time-step should have a unique value\n",
        "* The distance between time steps should stay constant.\n",
        "* The encoding should be able to generalise to longer sequences than seen during training.\n",
        "* The encoding must be deterministic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WHx-9m9h5JA"
      },
      "source": [
        "\n",
        "##### **Sine and cosine functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrkqm8gC90GU"
      },
      "source": [
        "\n",
        "In Attention is All you Need, the authors used a method that can satisfy all these requirements. This involves summing a combination of sine and cosine waves at different frequencies, with the formula for a position encoding at position $D$ shown below, where $i$ is the embedding index and $d_m$ is the token embedding size. \n",
        "\n",
        "\\\\\n",
        "\n",
        "$P_{D}= \\begin{cases}\\sin \\left(\\frac{D}{10000^{i/d_{m}}}\\right), & \\text { if } i \\bmod 2=0 \\\\ \\cos \\left(\\frac{D}{10000^{((i-1)/d_{m}}}\\right), & \\text { otherwise } \\end{cases}$\n",
        "\n",
        "\\\n",
        "\n",
        "Assuming our model as $d_m=8$, the position embedding will look like this:\n",
        "\n",
        "\\\n",
        "$P_{D}=\\left[\\begin{array}{c}\\sin \\left(\\frac{D}{10000^{0/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{0/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{2/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{2/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{4/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{4/8}}\\right)\\\\ \\sin \\left(\\frac{D}{10000^{8/8}}\\right)\\\\ \\cos \\left(\\frac{D}{10000^{8/8}}\\right)\\end{array}\\right]$\n",
        "\n",
        "\\\\\n",
        "\n",
        "Let's first create a function that can return these encodings to understand why this will work. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKobhuyj9-74"
      },
      "outputs": [],
      "source": [
        "def return_frequency_pe_matrix(token_sequence_length, token_embedding):\n",
        "\n",
        "    assert token_embedding % 2 == 0, \"token_embedding should be divisible by two\"\n",
        "\n",
        "    P = jnp.zeros((token_sequence_length, token_embedding))\n",
        "    positions = jnp.arange(0, token_sequence_length)[:, jnp.newaxis]\n",
        "\n",
        "    i = jnp.arange(0, token_embedding, 2)\n",
        "    frequency_steps = jnp.exp(i * (-math.log(10000.0) / token_embedding))\n",
        "    frequencies = positions * frequency_steps\n",
        "\n",
        "    P = P.at[:, 0::2].set(jnp.sin(frequencies))\n",
        "    P = P.at[:, 1::2].set(jnp.cos(frequencies))\n",
        "\n",
        "    return P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4_9-Oh4yZuF"
      },
      "outputs": [],
      "source": [
        "token_sequence_length = 50  # Number of tokens the model will need to process\n",
        "token_embedding = 10000  # token embedding (and positional encoding) dimensions, ensure it is divisible by two\n",
        "P = return_frequency_pe_matrix(token_sequence_length, token_embedding)\n",
        "plot_position_encodings(P, token_sequence_length, token_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gw8nOF2Ra93B"
      },
      "source": [
        "Looking at the graph above, we can see that for each position index, there is a unique pattern forming, where each position index will always have the same encoding. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHZXceUVNWy9"
      },
      "source": [
        "**Group task**:\n",
        "\n",
        "- Discuss with your friend why we are seeing that specific pattern when `token_sequence_length` is 1000, and `token_embedding` is 768.\n",
        "- You can try playing around with smaller values for `token_sequence_length` and  `token_embedding` to get a better intuition for the above discussion.\n",
        "- Ask your friend why they think the 10000 constant is used in the functions above. \n",
        "- Make `token_sequence_length` to be 50 and `token_embedding` something large, like 10000. What do you notice? Is a large token embedding always needed?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFiRpEaGd6TB"
      },
      "source": [
        "**Math task (optional):** Notice in our function that we do not directly implement the equation we describe above for numerical stability when calculating the frequency steps. See if you can derive by hand how we got to this new equation. Hint: Think about log rules.\n",
        "\n",
        "Original equation:\n",
        "\n",
        "$\\text{f} = \\frac{D}{x^{i/d_{m}}}$\n",
        "\n",
        "Code equation:\n",
        "\n",
        "$\\text{f} = \\text{exp} \\left( \\frac{-i\\log(x)}{d_m} \\right)D$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zhdifRibxSI2"
      },
      "outputs": [],
      "source": [
        "#@title Answer (but first try yourself)\n",
        "%%latex\n",
        "\\frac{𝐷}{x^{𝑖/𝑑_𝑚}} \\\\\n",
        "= D\\left( x^{-i/d_m}\\right)\\\\\n",
        "=\\text{exp}\\left(\\log{x^{-i/d_m}}\\right)D\\\\\n",
        "=\\text{exp}\\left(\\frac{-i}{d_m}\\log{x}\\right)D\\\\\n",
        "=\\text{exp}\\left(\\frac{-i\\log{x}}{d_m}\\right)D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GeFt_xwh722"
      },
      "source": [
        "##### **Learned positional embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnTNW1kNyZ75"
      },
      "source": [
        "\n",
        "Another method which is commonly used is to allow the model to learn the positional information required. In this method, the model learns a lookup table, where each index in the table refers to a positional embedding. Whereas the previous method allowed for an infinite amount of tokens, this method caps the maximum token sequence length as the lookup table has to be set beforehand. \n",
        "\n",
        "Using Haiku, this method is relatively straightforward as we can use [Haiku's embed function](https://dm-haiku.readthedocs.io/en/latest/api.html#embed) or [Haiku's get parameter function](https://dm-haiku.readthedocs.io/en/latest/api.html#get-parameter)\n",
        "\n",
        "**Code Task:** Try and implement a lookup table, which can be learned, for using Haikus get parameter function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXbQ_-RnXaNY"
      },
      "outputs": [],
      "source": [
        "class PositionEmbeddingsLookup(hk.Module):\n",
        "    \"\"\"\n",
        "    A position embedding of shape [max_sequence_len, d_model]\n",
        "    \"\"\"\n",
        "    def __init__(self, max_sequence_len, d_model):\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_sequence_len = max_sequence_len\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def __call__(self, sequence):\n",
        "        \n",
        "        assert sequence.shape[0]<=self.max_sequence_len, f\"Sequence to long, max lenght={self.max_sequence_len}\"\n",
        "\n",
        "        lookup_table = hk.get_parameter( \n",
        "            name=\"position_embedding\",\n",
        "            shape=, # FILL ME IN\n",
        "            init=jnp.zeros\n",
        "        )\n",
        "\n",
        "        return lookup_table[:sequence.shape[0], :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ECM4bYujnIFl"
      },
      "outputs": [],
      "source": [
        "# @title Run me to test your code\n",
        "def pos_test(sequences):\n",
        "    pos_emb = PositionEmbeddingsLookup(2, 8)\n",
        "    return pos_emb(sequences)\n",
        "\n",
        "\n",
        "pos_test = hk.transform(pos_test)\n",
        "\n",
        "# initialise model\n",
        "key = jax.random.PRNGKey(42)\n",
        "x = jax.random.normal(key, [1, 2, 8])\n",
        "params = pos_test.init(key, x)\n",
        "pos_encoding = pos_test.apply(params, key, x)\n",
        "\n",
        "pos_correct = jnp.array([[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]])\n",
        "\n",
        "assert jnp.allclose(pos_correct, pos_encoding), \"Not returning the correct value\"\n",
        "print(\"It seems correct. Look at the answer below to compare methods.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "R1_WpVc0Y3Hk"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "\n",
        "\n",
        "class PositionEmbeddingsLookup(hk.Module):\n",
        "    \"\"\"\n",
        "    A position embedding of shape [max_sequence_len, d_model]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_sequence_len, d_model):\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_sequence_len = max_sequence_len\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def __call__(self, sequence):\n",
        "\n",
        "        assert (\n",
        "            sequence.shape[0] <= self.max_sequence_len\n",
        "        ), f\"Sequence to long, max lenght={self.max_sequence_len}\"\n",
        "\n",
        "        lookup_table = hk.get_parameter(\n",
        "            name=\"position_embedding\",\n",
        "            shape=(self.max_sequence_len, self.d_model),\n",
        "            init=jnp.zeros,\n",
        "        )\n",
        "\n",
        "        return lookup_table[: sequence.shape[0], :]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l8mzIOeXbrA"
      },
      "source": [
        "### Feed Forward block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jn0EFwjXi1rI"
      },
      "source": [
        "These blocks are just a single 2-layer MLP that uses ReLU activation in the original model. GeLU has also become very popular, and we will be using it throughout the practical.\n",
        "\n",
        "$$\n",
        "\\operatorname{FFN}(x)=\\max \\left(0, x W_{1}+b_{1}\\right) W_{2}+b_{2}\n",
        "$$\n",
        "\n",
        "One can interpret this block as processing what the MHA block has produced and then projecting these new token representations to a space that the next block can use more optimally. Usually, the first layer is very wide, in the range of 2-8 times the size of the token representations. They do this as it is easier to parallelize computations for a single wider layer during training than to parallelize a feedforward block with multiple layers. Thus they can add in more complexity but keep training and inference optimized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGEy7dapluco"
      },
      "source": [
        "**Code task:** Code up a Haiku Module that implements the Feed forward block. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh6mpR6rl47u"
      },
      "outputs": [],
      "source": [
        "class FeedForwardBlock(hk.Module):\n",
        "  \"\"\"\n",
        "  A 2-layer MLP which widens then narrows the input.\n",
        "  \n",
        "  Args:\n",
        "    widening_factor [optional, default=4]: The size of the hidden layer will be d_model * widening_factor.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               widening_factor: int = 4, \n",
        "               name=None):\n",
        "    super().__init__(name=name)\n",
        "    self._init_scale = 0.25\n",
        "    self._widening_factor = widening_factor\n",
        "\n",
        "  def __call__(self, x):\n",
        "    '''\n",
        "    Args:\n",
        "      x: [B, T, d_m]\n",
        "\n",
        "    Return:\n",
        "      x: [B, T, d_m]\n",
        "    '''\n",
        "    d_m = x.shape[-1]\n",
        "    layer1_size = self._widening_factor * d_m\n",
        "\n",
        "    initializer = hk.initializers.VarianceScaling(self._init_scale)\n",
        "    layer1 = # FINSIH ME\n",
        "    layer2 = # FINISH ME\n",
        "\n",
        "    x = jax.nn.gelu(layer1(x))\n",
        "    x = layer2(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5t7cgF8IYIUF"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "\n",
        "\n",
        "class FeedForwardBlock(hk.Module):\n",
        "    \"\"\"A 2-layer MLP which widens then narrows the input.\"\"\"\n",
        "\n",
        "    def __init__(self, widening_factor: int = 4, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self._init_scale = 0.25\n",
        "        self._widening_factor = widening_factor\n",
        "\n",
        "    def __call__(self, x):\n",
        "        d_m = x.shape[-1]\n",
        "        layer1_size = self._widening_factor * d_m\n",
        "\n",
        "        initializer = hk.initializers.VarianceScaling(self._init_scale)\n",
        "        layer1 = hk.Linear(layer1_size, w_init=initializer)\n",
        "        layer2 = hk.Linear(d_m, w_init=initializer)\n",
        "\n",
        "        x = jax.nn.gelu(layer1(x))\n",
        "        x = layer2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQONPcKfXbfj"
      },
      "source": [
        "### Add and Norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyyWfxPKsUm6"
      },
      "source": [
        "In order to get transformers to go deeper, the residual connections are very important to allow an easier flow of gradients through the network. For normalisation, `layer norm` is used. This normalises each token vector independently in the batch. It is found that normalising the vectors improves the convergence and stability of transformers. \n",
        "\n",
        "There are two learnable parameters in layernorm, `scale` and `bias`, which rescales the normalised value. Thus, for each input token in a batch, we calculate the mean, $\\mu_{i}$ and variance $\\sigma_i^2$. We then normalise the token with:\n",
        "\n",
        "$\\hat{x}_i = \\frac{x_i-\\mu_{i}}{\\sigma_i^2 + ϵ}$.\n",
        "\n",
        "Then $\\hat{x}$ is rescaled using the learned `scale`, $γ$, and `bias` $β$, with:\n",
        "\n",
        "$y_i = γ\\hat{x}_i + β = LN_{γ,β}(x_i)$.\n",
        "\n",
        "So our add norm block can be represented as $LN(x+f(x))$, where $f(x)$ is either a MLP or MHA block."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9rPxLLD69nl"
      },
      "source": [
        "**Code task:** Code up a Haiku Module that impliments the add norm block. It should take as input the processed and unprocessed tokens. Hint: `hk.LayerNorm `"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIY5a7I969Fw"
      },
      "outputs": [],
      "source": [
        "class AddNorm(hk.Module):\n",
        "  \"\"\"A block that impliments the add and norm block\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               name=None):\n",
        "    super().__init__(name=name)\n",
        "\n",
        "  def __call__(self, x, processed_x):\n",
        "    '''\n",
        "    Args:\n",
        "      x: Sequence of tokens before feeding into MHA or FF blocks, with shape [B, T, d_m]\n",
        "      x: Sequence of after being processed by MHA or FF blocks, with shape [B, T, d_m]\n",
        "\n",
        "    Return:\n",
        "      add_norm_x: Transformed tokens with shape [B, T, d_m]\n",
        "    '''\n",
        "    \n",
        "    added = # FINISH ME\n",
        "    normalised = #FINISH ME\n",
        "    return normalised(added)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5OOLMaYXsR_O"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "\n",
        "\n",
        "class AddNorm(hk.Module):\n",
        "    \"\"\"A block that impliments the add and norm block\"\"\"\n",
        "\n",
        "    def __init__(self, name=None):\n",
        "        super().__init__(name=name)\n",
        "\n",
        "    def __call__(self, x, processed_x):\n",
        "\n",
        "        added = x + processed_x\n",
        "        normalised = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)\n",
        "        return normalised(added)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIHHQTrcXb1b"
      },
      "source": [
        "### Building the Encoder "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDIHVioL9J7e"
      },
      "source": [
        "We now have all the building blocks that we need to build a single encoder block. \n",
        "\n",
        "**Code task:** Use your previous built modules and create a new haiku module that is a transformer encoder block. One should also be able to retrieve the attention weights as an optional flag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1Y18G_OJGJu"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(hk.Module):\n",
        "  \"\"\"\n",
        "  An encoder block for the Transformer stack.\n",
        "  \n",
        "  Args:\n",
        "    num_heads: The number of heads to be used in the MHA block.\n",
        "    d_m: Token embedding size\n",
        "    widdening_factor: The size of the hidden layer will be d_m * widening_factor.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_heads, d_m, widening_factor=4, name=None):\n",
        "    super().__init__(name=name)\n",
        "\n",
        "    self._num_heads = num_heads\n",
        "    self.d_m = d_m\n",
        "    self.mha = MultiHeadAttention(num_heads, d_m)\n",
        "    self.add_norm1 = AddNorm()\n",
        "    self.add_norm2 = AddNorm()\n",
        "    self.MLP = FeedForwardBlock(widening_factor=widening_factor)\n",
        "\n",
        "  def __call__(self, X, mask=None, return_att_weight=True):\n",
        "    '''\n",
        "    Args:\n",
        "      X: Batch of tokens, with shape [B, T, d_m]\n",
        "      mask [optional, default=None]: Mask to be applied, with shape [T, T].\n",
        "      return_att_weight [optional, default=True]: Whether to return the attention weights.\n",
        "    '''\n",
        "\n",
        "    sequence_len = X.shape[-2]\n",
        "    positions = # FINSIH ME, MUST BE FIXED POSITIONAL EMBEDDINGS\n",
        "    X = X + positions\n",
        "    attention, attention_weights = # FINISH ME\n",
        "    X = self.add_norm1(X, attention)\n",
        "    projection =  # FINISH ME\n",
        "    X = # FINISH ME\n",
        "    return (X, attention_weights) if return_att_weight else X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xrAGr3Td97i7"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "\n",
        "\n",
        "class EncoderBlock(hk.Module):\n",
        "    \"\"\"An encoder block for the Transformer stack.\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, d_m, widening_factor=4, name=None):\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        self._num_heads = num_heads\n",
        "        self.d_m = d_m\n",
        "        self.mha = MultiHeadAttention(num_heads, d_m)\n",
        "        self.add_norm1 = AddNorm()\n",
        "        self.add_norm2 = AddNorm()\n",
        "        self.MLP = FeedForwardBlock(widening_factor=widening_factor)\n",
        "\n",
        "    def __call__(self, X, mask=None, return_att_weight=True):\n",
        "        sequence_len = X.shape[-2]\n",
        "        positions = return_frequency_pe_matrix(sequence_len, self.d_m)\n",
        "        X = X + positions\n",
        "        attention, attention_weights = self.mha(X, mask, return_weights=True)\n",
        "        X = self.add_norm1(X, attention)\n",
        "        projection = self.MLP(X)\n",
        "        X = self.add_norm2(X, projection)\n",
        "        return (X, attention_weights) if return_att_weight else X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdaFnZ8P979f"
      },
      "source": [
        "Now that we have a single encoder block, let's create a new transformer encoder stack that stacks the encoder blocks into one single transformer encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tedqHar-YJfV"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(hk.Module):\n",
        "    \"\"\"\n",
        "    Transformer encoder consisting of several layers of encoder blocks.\n",
        "\n",
        "    Args:\n",
        "      num_heads: The number of heads to be used in the MHA block.\n",
        "      num_layers: The number of encoder blocks to be used.\n",
        "      d_m: Token embedding size\n",
        "      widdening_factor: The size of the hidden layer will be d_m * widening_factor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, num_heads: int, num_layers: int, d_m: int, widening_factor=4, name=None\n",
        "    ):\n",
        "        super().__init__(name=name)\n",
        "        self.num_layers = num_layers\n",
        "        self.num_heads = num_heads\n",
        "        self.widening_factor = widening_factor\n",
        "        self.blocks = [\n",
        "            EncoderBlock(self.num_heads, d_m, self.widening_factor)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "    def __call__(self, X, mask, return_att_weights=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          X: Batch of tokens, with shape [B, T, d_m]\n",
        "          mask [optional, default=None]: Mask to be applied, with shape [T, T].\n",
        "          return_att_weight [optional, default=True]: Whether to return the attention weights.\n",
        "        \"\"\"\n",
        "\n",
        "        if return_att_weights:\n",
        "            att_weights = []\n",
        "\n",
        "        for block in self.blocks:\n",
        "            out = block(X, mask, return_att_weights)\n",
        "\n",
        "            if return_att_weights:\n",
        "                X = out[0]\n",
        "                att_weights.append(out[1])\n",
        "            else:\n",
        "                X = out\n",
        "\n",
        "        return (\n",
        "            X if not return_att_weights else (X, jnp.array(att_weights).swapaxes(0, 1))\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Y3-JlhO4nOcS"
      },
      "outputs": [],
      "source": [
        "# @title Run me to test your encoder\n",
        "B, T, d_m, N = 2, 2, 2, 2\n",
        "\n",
        "# run this to test if your code is running\n",
        "def encode(\n",
        "    sequences,\n",
        "    num_heads=2,\n",
        "    num_layers=2,\n",
        "    widening_factor=4,\n",
        "    d_m=d_m,\n",
        "    return_weights=False,\n",
        "):\n",
        "    encoder = TransformerEncoder(num_heads, num_layers, d_m, widening_factor)\n",
        "    return encoder(sequences, None, return_weights)\n",
        "\n",
        "\n",
        "encode = hk.transform(encode)\n",
        "\n",
        "# initialise module\n",
        "key = jax.random.PRNGKey(42)\n",
        "X = jax.random.normal(key, [B, T, d_m])\n",
        "params = encode.init(key, X, N, d_m)\n",
        "encoded_x = encode.apply(params, key, X)\n",
        "\n",
        "correct_encoded_x = jnp.array(\n",
        "    [\n",
        "        [[-0.99999505, 0.99999505], [0.9999963, -0.9999963]],\n",
        "        [[-0.99999505, 0.99999505], [0.9999963, -0.9999963]],\n",
        "    ]\n",
        ")\n",
        "\n",
        "assert jnp.allclose(\n",
        "    correct_encoded_x, encoded_x\n",
        "), \"Not returning the correct value, investigate previous blocks\"\n",
        "print(\"It seems correct. Move on to building the decoder\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxSD1tLIX4CJ"
      },
      "source": [
        "### Building the Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CouFQCMIK86f"
      },
      "source": [
        "<img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" width=\"250\" />\n",
        "\n",
        "Most of the groundwork has happened. We have built the positional encoding block, the MHA block, the feed-forward block and the add&norm block.\n",
        "\n",
        "The only part needed is passing the encoder outputs to each decoder block and applying the extra MHA block found in the decoder blocks. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4puW2jKJO6E5"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(hk.Module):\n",
        "    \"\"\"\n",
        "    Transformer decoder block.\n",
        "\n",
        "    Args:\n",
        "      num_heads: The number of heads to be used in the MHA block.\n",
        "      d_m: Token embedding size\n",
        "      widdening_factor: The size of the hidden layer will be d_m * widening_factor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, d_m, widening_factor=4, name=None):\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        self._num_heads = num_heads\n",
        "        self.d_m = d_m\n",
        "        self.mha = MultiHeadAttention(num_heads, d_m)\n",
        "        self.mha_combine = MultiHeadAttention(num_heads, d_m)\n",
        "        self.add_norm1 = AddNorm()\n",
        "        self.add_norm2 = AddNorm()\n",
        "        self.add_norm3 = AddNorm()\n",
        "        self.MLP = FeedForwardBlock(widening_factor=widening_factor)\n",
        "\n",
        "    def __call__(self, X, encoder_output, mask=None, return_att_weight=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          X: Batch of tokens being fed into the decoder, with shape [B, T_decoder, d_m]\n",
        "          encoder_output: Batch of tokens with was processed by the encoder, with shape [B, T_encoder, d_m]\n",
        "          mask [optional, default=None]: Mask to be applied, with shape [T_decoder, T_decoder].\n",
        "          return_att_weight [optional, default=True]: Whether to return the attention weights.\n",
        "        \"\"\"\n",
        "\n",
        "        sequence_len = X.shape[-2]\n",
        "        positions = return_frequency_pe_matrix(sequence_len, self.d_m)\n",
        "        X = X + positions\n",
        "\n",
        "        attention, attention_weights_1 = self.mha(X, mask=mask, return_weights=True)\n",
        "\n",
        "        X = self.add_norm1(X, attention)\n",
        "\n",
        "        attention, attention_weights_2 = self.mha_combine(\n",
        "            Q=X, K=encoder_output, V=encoder_output, mask=mask, return_weights=True\n",
        "        )\n",
        "\n",
        "        X = self.add_norm2(X, attention)\n",
        "        projection = self.MLP(X)\n",
        "        X = self.add_norm3(X, projection)\n",
        "\n",
        "        return (X, attention_weights_2) if return_att_weight else X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxeVVOxDT0hK"
      },
      "source": [
        "As you can see in the code, we now use the encoder outputs passed to the module when calculating the final attention weights before feeding everything through the MLP. \n",
        "\n",
        "Next, we just put everything together, just like we did for the encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xZcdRdWKQETZ"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(hk.Module):\n",
        "    \"\"\"\n",
        "    Transformer encoder consisting of several layers of decoder blocks.\n",
        "\n",
        "    Args:\n",
        "      num_heads: The number of heads to be used in the MHA block.\n",
        "      num_layers: The number of decoder blocks to be used.\n",
        "      d_m: Token embedding size\n",
        "      widdening_factor: The size of the hidden layer will be d_m * widening_factor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, num_heads: int, num_layers: int, d_m: int, widening_factor=4, name=None\n",
        "    ):\n",
        "        super().__init__(name=name)\n",
        "        self.num_layers = num_layers\n",
        "        self.num_heads = num_heads\n",
        "        self.widening_factor = widening_factor\n",
        "        self.blocks = [\n",
        "            DecoderBlock(self.num_heads, d_m, self.widening_factor)\n",
        "            for _ in range(num_layers)\n",
        "        ]\n",
        "\n",
        "    def __call__(self, X, encoder_output, mask, return_att_weights=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          X: Batch of tokens being fed into the decoder, with shape [B, T_decoder, d_m]\n",
        "          encoder_output: Batch of tokens with was processed by the encoder, with shape [B, T_encoder, d_m]\n",
        "          mask [optional, default=None]: Mask to be applied, with shape [T_decoder, T_decoder].\n",
        "          return_att_weight [optional, default=True]: Whether to return the attention weights.\n",
        "        \"\"\"\n",
        "\n",
        "        if return_att_weights:\n",
        "            att_weights = []\n",
        "        for block in self.blocks:\n",
        "            out = block(X, encoder_output, mask, return_att_weights)\n",
        "            if return_att_weights:\n",
        "                X = out[0]\n",
        "                att_weights.append(out[1])\n",
        "            else:\n",
        "                X = out\n",
        "\n",
        "        return (\n",
        "            X if not return_att_weights else (X, jnp.array(att_weights).swapaxes(0, 1))\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts-WLr0zY26X"
      },
      "source": [
        "### Putting it all together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXPxlqooYY3T"
      },
      "source": [
        "There is nothing more to it. We have built every single component required to build a transformer, now we just have to combine everything into one module. \n",
        "\n",
        "**Code task**: Build a encoder-decoder transformer haiku module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26tL91_bY5ZL"
      },
      "outputs": [],
      "source": [
        "class Transformer(hk.Module):\n",
        "  \"\"\"\n",
        "  \n",
        "  A full transformer encoder-decoder architecture\n",
        "\n",
        "  Args:\n",
        "    num_heads: The number of heads to be used in the MHA blocks.\n",
        "    num_layers: The number of decoder and encoder blocks to be used.\n",
        "    d_m: Token embedding size\n",
        "    widdening_factor: The size of the hidden layer will be d_m * widening_factor. \n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, num_heads, num_layers, d_m, widening_factor=4, name=None):\n",
        "    super().__init__(name=name)\n",
        "\n",
        "    self.num_layers = num_layers\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.encoder = # FINISH ME\n",
        "    self.decoder = # FINISH ME\n",
        "\n",
        "  def __call__(self, X_encoder, X_decoder, encoder_mask, decoder_mask, return_weights):\n",
        "    '''\n",
        "    Args:\n",
        "      X_encoder: Batch of tokens being fed into the enecoder, with shape [B, T_encoder, d_m]\n",
        "      X_decoder: Batch of tokens being fed into the decoder, with shape [B, T_decoder, d_m]\n",
        "      encoder_mask: Mask to be applied, with shape [T_encoder, T_encoder]. If None, masking will not be applied.\n",
        "      decoder_mask: Mask to be applied, with shape [T_decoder, T_decoder]. If None, masking will not be applied.\n",
        "      return_att_weight: Whether to return the attention weights.\n",
        "    '''\n",
        "\n",
        "    encoder_out = # FINISH ME \n",
        "    attention_input = encoder_out[0] if return_weights else encoder_out\n",
        "    decoder_out = # FINISH ME\n",
        "    logits = decoder_out[0] if return_weights else decoder_out\n",
        "\n",
        "    return logits if not return_weights else (logits, encoder_out[1], decoder_out[1])   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qW6MLaDGYXgv"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "\n",
        "\n",
        "class Transformer(hk.Module):\n",
        "    \"\"\"A full transformer encoder-decoder architecture\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, num_layers, d_m, widening_factor=4, name=None):\n",
        "        super().__init__(name=name)\n",
        "        self.num_layers = num_layers\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.encoder = TransformerEncoder(num_heads, num_layers, d_m, widening_factor)\n",
        "        self.decoder = TransformerDecoder(num_heads, num_layers, d_m, widening_factor)\n",
        "\n",
        "    def __call__(\n",
        "        self, X_encoder, X_decoder, encoder_mask, decoder_mask, return_weights\n",
        "    ):\n",
        "        encoder_out = self.encoder(X_encoder, encoder_mask, return_weights)\n",
        "        attention_input = encoder_out[0] if return_weights else encoder_out\n",
        "        decoder_out = self.decoder(\n",
        "            X_decoder, attention_input, decoder_mask, return_weights\n",
        "        )\n",
        "        logits = decoder_out[0] if return_weights else decoder_out\n",
        "\n",
        "        return (\n",
        "            logits if not return_weights else (logits, encoder_out[1], decoder_out[1])\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3WZyV_s54_R"
      },
      "source": [
        "If everything is correct, then if we run the code below, everything should run without any issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbRxqe_lfRIq"
      },
      "outputs": [],
      "source": [
        "B, T, d_m, N = 18, 32, 16, 8\n",
        "\n",
        "# run this to test if your code is running\n",
        "def tranformer(\n",
        "    sequences,\n",
        "    num_heads=1,\n",
        "    num_layers=1,\n",
        "    widening_factor=4,\n",
        "    d_m=d_m,\n",
        "    return_weights=True,\n",
        "):\n",
        "    transform = Transformer(num_heads, num_layers, d_m, widening_factor)\n",
        "    mask = jnp.tril(np.ones((sequences.shape[1], sequences.shape[1])))\n",
        "    return transform(sequences, sequences, None, mask, return_weights)\n",
        "\n",
        "\n",
        "tranformer = hk.transform(tranformer)\n",
        "\n",
        "# initialise module and get dummy output\n",
        "key = jax.random.PRNGKey(42)\n",
        "X = jax.random.normal(key, [B, T, d_m])\n",
        "params = tranformer.init(\n",
        "    key, X, num_heads=1, num_layers=1, widening_factor=4, d_m=d_m, return_weights=True\n",
        ")\n",
        "\n",
        "# extract output from decoder\n",
        "logits, encoder_att_weights, decoder_att_weights = tranformer.apply(\n",
        "    params,\n",
        "    key,\n",
        "    X,\n",
        "    num_heads=1,\n",
        "    num_layers=1,\n",
        "    widening_factor=4,\n",
        "    d_m=d_m,\n",
        "    return_weights=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLeXyXNE5qq9"
      },
      "source": [
        "As a final sanity check, we can see that our attention weights behave as expected for now. The encoder weights can attend to all input sequences, and our decoder only attends to previous tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0PNKe1r3O9H"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
        "plt.suptitle(\"Decoder attention weights (left) vs Encoder attention weights(right\")\n",
        "sns.heatmap(encoder_att_weights[0, 0, 0, ...], ax=ax[0], cmap=\"Blues\")\n",
        "sns.heatmap(decoder_att_weights[0, 0, 0, ...], ax=ax[1], cmap=\"Blues\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bqOml7HYNmm"
      },
      "source": [
        "### Training our model to invert sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-f_jnA2P_k5"
      },
      "source": [
        "To showcase the encoder-decoder transformer model we have just created, we will train it to reverse the order of an input string. For example:\n",
        "\n",
        "- *Input*: \"Look at me, I am reversed.\"\n",
        "- *Output*: \"reversed. am I me, at Look\"\n",
        "\n",
        "Some might even say a little Yoda simulator ;) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3e4e_vSP8qM"
      },
      "source": [
        "#### Creating our dataset - optional, but run the code\n",
        "\n",
        "We will use the tinyShakespeare hosted by Andrej Karpathy [here](https://github.com/karpathy/char-rnn)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKtkOftcQijr"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yw7DnRaUP8Hj"
      },
      "outputs": [],
      "source": [
        "# Download data set\n",
        "! wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O input.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RldPc6ZOTDnP"
      },
      "source": [
        "Next, we need to build a dataset class that will return a sample of text from tinyShakespear and the reversed version of the sentence. For now, do not worry too much about how this class works. It is just a way to iteratively run over the dataset and randomly sample snippets of text for an \"infinite\" number of steps.\n",
        "\n",
        "Another critical step to note is that we tokenize the data at a char level. In other words, all text is split and encoded to a unique ASCII value, with 128 unique values. The model will thus receive a sequence of integers. For each ASCII character, it will look learn an embedding, which it looks up in a table of vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3N12LcdTV7p"
      },
      "outputs": [],
      "source": [
        "BREAK_ORDS = (ord(\" \"), ord(\"\\n\"), ord(\"\\t\"), ord(\".\"))\n",
        "\n",
        "\n",
        "def ord_to_char(list_of_ords):\n",
        "    \"\"\"Format ascii np.array to char\"\"\"\n",
        "    return \"\".join([chr(o) for o in list_of_ords])\n",
        "\n",
        "\n",
        "class AsciiDatasetForInversionTask:\n",
        "    \"\"\"In-memory dataset of a single-file ASCII dataset for inversion task.\"\"\"\n",
        "\n",
        "    def __init__(self, path: str, batch_size: int, sequence_length: int):\n",
        "        \"\"\"Load a single-file ASCII dataset in memory.\"\"\"\n",
        "        self.vocab_size = 128\n",
        "        self._batch_size = batch_size\n",
        "\n",
        "        with open(path, \"r\") as f:\n",
        "            corpus = f.read()\n",
        "\n",
        "        if not corpus.isascii():\n",
        "            raise ValueError(\"Loaded corpus is not ASCII.\")\n",
        "\n",
        "        if \"\\0\" in corpus:\n",
        "            # Reserve 0 codepoint for pad token.\n",
        "            raise ValueError(\"Corpus must not contain null byte.\")\n",
        "\n",
        "        # Tokenize by taking ASCII codepoints.\n",
        "        corpus = np.array([ord(c) for c in corpus if c != \"\\n\"]).astype(np.int32)\n",
        "        assert np.min(corpus) > 0\n",
        "        assert np.max(corpus) < self.vocab_size  # Double-checking ASCII codepoints.\n",
        "\n",
        "        crop_len = sequence_length + 1\n",
        "        num_batches, ragged = divmod(corpus.size, batch_size * crop_len)\n",
        "        if ragged:\n",
        "            corpus = corpus[:-ragged]\n",
        "        corpus = corpus.reshape([-1, crop_len])\n",
        "\n",
        "        if num_batches < 10:\n",
        "            raise ValueError(\n",
        "                f\"Only {num_batches} batches; consider a shorter \"\n",
        "                \"sequence or a smaller batch.\"\n",
        "            )\n",
        "\n",
        "        self._ds = AsciiDatasetForInversionTask._infinite_shuffle(\n",
        "            corpus, batch_size * 10\n",
        "        )\n",
        "\n",
        "    def __next__(self):\n",
        "        \"\"\"Yield next mini-batch.\"\"\"\n",
        "        batch = [next(self._ds) for _ in range(self._batch_size)]\n",
        "        batch = np.stack(batch)\n",
        "        # Create the language modeling observation/target pairs.\n",
        "        return dict(\n",
        "            input=batch[:, :], target=AsciiDatasetForInversionTask.invert_batch(batch)\n",
        "        )\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    @staticmethod\n",
        "    def invert_batch(batch):\n",
        "        inverted_batch = []\n",
        "        for instance in batch:\n",
        "            inverted_batch.append([])\n",
        "            last_seen_space_index = len(instance)\n",
        "            for i in range(len(instance) - 1, -1, -1):\n",
        "                if instance[i] in BREAK_ORDS:\n",
        "                    inverted_batch[-1].extend(instance[i + 1 : last_seen_space_index])\n",
        "                    inverted_batch[-1].append(instance[i])\n",
        "                    last_seen_space_index = i\n",
        "            inverted_batch[-1].extend(instance[i:last_seen_space_index])\n",
        "        return np.array(inverted_batch)\n",
        "\n",
        "    @staticmethod\n",
        "    def _infinite_shuffle(iterable, buffer_size):\n",
        "        \"\"\"Infinitely repeat and shuffle data from iterable.\"\"\"\n",
        "        ds = itertools.cycle(iterable)\n",
        "        buf = [next(ds) for _ in range(buffer_size)]\n",
        "        random.shuffle(buf)\n",
        "        while True:\n",
        "            item = next(ds)\n",
        "            idx = random.randint(0, buffer_size - 1)  # Inclusive.\n",
        "            result, buf[idx] = buf[idx], item\n",
        "            yield result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfJR9CMoUwzD"
      },
      "source": [
        "Now that we can load data and return infinite batches, lets see it in action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2pTRomeUtqB"
      },
      "outputs": [],
      "source": [
        "batch_size = 2\n",
        "seq_length = 32\n",
        "train_dataset = AsciiDatasetForInversionTask(\"input.txt\", batch_size, seq_length)\n",
        "vocab_size = train_dataset.vocab_size\n",
        "\n",
        "batch = next(train_dataset)\n",
        "\n",
        "for obs, target in zip(batch[\"input\"], batch[\"target\"]):\n",
        "    print(\"-\" * 10, \"Input\", \"-\" * 11)\n",
        "    print(\"TEXT:\", ord_to_char(obs))\n",
        "    print(\"ASCII:\", obs)\n",
        "    print(\"-\" * 10, \"Target\", \"-\" * 10)\n",
        "    print(\"TEXT:\", ord_to_char(target))\n",
        "    print(\"ASCII:\", target)\n",
        "\n",
        "\n",
        "print(f\"\\n Total vocabulary size: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1YoKTusQdJU"
      },
      "source": [
        "#### Training objective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4z7OBdEZAvx"
      },
      "source": [
        "Given an input sequence, which is a sequence of ASCII characters in this specific use case, our model should learn to take that sequence and reverse it. \n",
        "\n",
        "This can be framed as a sequence-to-sequence problem, where the decoder should predict the reversed input sequence, given the processed information from the encoder and the original sequence.\n",
        "\n",
        "The loss function will then be the sum of the loss over each token. The loss will be [cross-entropy](https://en.wikipedia.org/wiki/Cross_entropy), where the ASCII token is the correct label to predict. Take note, that because the sequence can be padded during training, the loss is also masked to ignore the result on padded tokens.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=16-F7uCuJEvFn3e9PgpJpEQmsuMAUkCLT\" alt=\"drawing\" width=\"290\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCVb0hylgN3o"
      },
      "source": [
        "**Code task**: Implimement the cross-entropy loss function below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBao6oI2ZDVp"
      },
      "outputs": [],
      "source": [
        "def sequence_loss_fn(logits, data):\n",
        "  '''\n",
        "  Compute the cross-entropy loss between predict token ID and true ID\n",
        "\n",
        "  Args:\n",
        "    logits: A array of shape [batch_size, sequence_length, vocab_size]\n",
        "    data: Batch of data that was used to create logits, This will have been generated by the dataset.\n",
        "\n",
        "  Returns:\n",
        "    loss: A scalar value representing the mean batch loss\n",
        "  '''\n",
        "\n",
        "  targets = jax.nn.one_hot(data['target'], 128)\n",
        "  assert logits.shape == targets.shape\n",
        "\n",
        "  mask = jnp.greater(data['input'], 0)\n",
        "  loss = #FINSIH ME\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "eehJHUOKna8a"
      },
      "outputs": [],
      "source": [
        "# @title Run me to test your code\n",
        "data = {\"input\": jnp.array([[0, 2, 0]]), \"target\": jnp.array([[0, 2, 0]])}\n",
        "key = jax.random.PRNGKey(42)\n",
        "X = jax.random.normal(key, [1, 3, 128])\n",
        "loss = sequence_loss_fn(X, data)\n",
        "real_loss = jnp.array(5.2058086)\n",
        "assert jnp.allclose(real_loss, loss), \"Not returning the correct value\"\n",
        "print(\"It seems correct. Look at the answer below to compare methods.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "vX38mblNhV4k"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!')\n",
        "def sequence_loss_fn(logits, data):\n",
        "    \"\"\"Compute the loss on data wrt params.\"\"\"\n",
        "    targets = jax.nn.one_hot(data[\"target\"], 128)\n",
        "    assert logits.shape == targets.shape\n",
        "    mask = jnp.greater(data[\"input\"], 0)\n",
        "    loss = -jnp.sum(targets * jax.nn.log_softmax(logits), axis=-1)\n",
        "    loss = jnp.sum(loss * mask) / jnp.sum(mask)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EKILfw9Au6X"
      },
      "source": [
        "**Viewing it as an generative task**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5R5y-JEA1ra"
      },
      "source": [
        "We can also solve this problem by iteratively generating a ASCII id, based on the previously generated ASCII's and input sequence. Notice here that the first token being fed into the decoder is a unique character, `<s>`, which is used to kickstart the process. This unique character we will represent with token ID $191$. \n",
        "\n",
        "\\\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1WvZiwA1Tsn1sby0tTn3rZwhm2safecHj\" alt=\"drawing\" width=\"290\"/>\n",
        "\n",
        "We are in essence, then trying to model the distribution: \n",
        "\n",
        "$$\n",
        "P\\left(y_{1}, y_{2}, \\ldots, y_{n}, \\mid x\\right)=\\prod_{t=1}^{n} p\\left(y_{t} \\mid y_{<t}, x\\right)\n",
        "$$\n",
        "\n",
        "The loss function will not change, but how we feed our data into the decoder will be different.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1eXwfOFZc4C"
      },
      "source": [
        "#### Training and inspecting models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YfDIMbnhph6"
      },
      "source": [
        "In the next section, we define all the processes required to train the model using the two objectives described above. A lot of this is now the work required to do training using Haiku. \n",
        "\n",
        "Below we define the forward functions that we can wrap in the transform function from Haiku. These functions take in our data and produce our logits as a full parallel prediction or generative iteratively.\n",
        "\n",
        "Please take note that in the below function, the ASCII characters are encoded using a lookup table and that there is an MLP at the end that predicts ASCII characters from our decoded token embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHLFuYEMYOGH"
      },
      "outputs": [],
      "source": [
        "def build_forward_fn(\n",
        "    vocab_size, d_model, num_heads, num_layers, widening_factor, generative=False\n",
        "):\n",
        "    def forward_fn(data, training=True, return_weights=False, vocab_size=vocab_size):\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        tokens = data[\"input\"]\n",
        "        tokens_target = data[\"target\"]\n",
        "\n",
        "        # if generative, we need new token to say start generating\n",
        "        if generative:\n",
        "            vocab_size = vocab_size + 1\n",
        "\n",
        "        # Embed the input tokens and positions.\n",
        "        embed_init = hk.initializers.TruncatedNormal(stddev=0.02)\n",
        "        token_embedding_map = hk.Embed(vocab_size, d_model, w_init=embed_init)\n",
        "        token_embs = token_embedding_map(tokens)\n",
        "\n",
        "        # Run the transformer over the inputs.\n",
        "        transformer = Transformer(num_heads, num_layers, d_model, widening_factor)\n",
        "\n",
        "        if generative:\n",
        "            # inject \"start\" token into target codes\n",
        "            start_token = jnp.ones(\n",
        "                (data[\"target\"].shape[0], 1), dtype=jnp.int32\n",
        "            ) + jnp.array(vocab_size, dtype=jnp.int32)\n",
        "            tokens_target = jnp.concatenate((start_token, tokens_target), axis=1)\n",
        "            token_target_emb = token_embedding_map(tokens_target)\n",
        "\n",
        "            max_mask_shape = max(tokens.shape[1], tokens_target.shape[1])\n",
        "            decoding_mask = jnp.tril(jnp.ones((tokens.shape[0], max_mask_shape)))\n",
        "            output_embeddings, weights1, weights2 = transformer(\n",
        "                token_embs, token_target_emb, None, decoding_mask, return_weights=True\n",
        "            )\n",
        "            # Predict the token IDs\n",
        "\n",
        "            if training:\n",
        "              logits = hk.Linear(vocab_size - 1)(output_embeddings)[\n",
        "                  :, :-1, ...\n",
        "              ]  # we don't want predictions after sequence size\n",
        "            else:\n",
        "              logits = hk.Linear(vocab_size - 1)(output_embeddings)\n",
        "        else:\n",
        "            output_embeddings, weights1, weights2 = transformer(\n",
        "                token_embs, token_embs, None, None, return_weights=True\n",
        "            )\n",
        "            # Predict the token IDs\n",
        "            logits = hk.Linear(vocab_size)(output_embeddings)\n",
        "\n",
        "        return logits if not return_weights else (logits, weights1, weights2)\n",
        "\n",
        "    return forward_fn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wuhz4ummZTfK"
      },
      "source": [
        "##### Training the parallel classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQTWhBV1l_j-"
      },
      "source": [
        "Next we need to initialise our model and setup our optimiser using optax. Feel free to play with hyperparameters. We also set up the training data here again to make it easy to apply hyperparameter changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV5e1HYenfak"
      },
      "outputs": [],
      "source": [
        "d_model = 128\n",
        "num_heads = 4\n",
        "num_layers = 1\n",
        "widening_factor = 4\n",
        "LR = 1e-3\n",
        "\n",
        "batch_size = 64\n",
        "seq_length = 32\n",
        "generative = False\n",
        "\n",
        "# set up the data\n",
        "train_dataset = AsciiDatasetForInversionTask(\"input.txt\", batch_size, seq_length)\n",
        "vocab_size = train_dataset.vocab_size\n",
        "batch = next(train_dataset)\n",
        "\n",
        "# initialise model\n",
        "rng = jax.random.PRNGKey(42)\n",
        "forward_fn = hk.transform(\n",
        "    build_forward_fn(\n",
        "        vocab_size, d_model, num_heads, num_layers, widening_factor, generative\n",
        "    )\n",
        ")\n",
        "params = forward_fn.init(rng, batch)\n",
        "\n",
        "# set up the optimiser\n",
        "optimiser = optax.chain(optax.clip_by_global_norm(1), optax.adam(LR, b1=0.9, b2=0.99))\n",
        "opt_state = optimiser.init(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-q-73UWd-KW4"
      },
      "outputs": [],
      "source": [
        "# @title Haiku functions required for training - optional, but run this code block\n",
        "\n",
        "# set up function calculating loss\n",
        "def loss(params, batch):\n",
        "    vocab_size = 128\n",
        "    key = jax.random.PRNGKey(42)\n",
        "    logits = forward_fn.apply(params, key, batch)\n",
        "    batch_loss = sequence_loss_fn(logits, batch)\n",
        "    return batch_loss\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def update(params, opt_state, batch):\n",
        "    # get data neded for training\n",
        "    grads = jax.grad(loss)(params, batch)\n",
        "    updates, opt_state = optimiser.update(grads, opt_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return params, opt_state\n",
        "\n",
        "\n",
        "# calculate loss per batch\n",
        "@jax.jit\n",
        "def calculate_batch_loss(params, batch, vocab_size):\n",
        "    key = jax.random.PRNGKey(42)\n",
        "    logits = forward_fn.apply(params, key, batch)\n",
        "\n",
        "    return sequence_loss_fn(logits, batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gn3y3AS2w-gb"
      },
      "outputs": [],
      "source": [
        "MAX_STEPS = 2000\n",
        "LOG_EVERY = 8\n",
        "losses = []\n",
        "\n",
        "\n",
        "plotlosses = PlotLosses()\n",
        "key = jax.random.PRNGKey(42)\n",
        "\n",
        "# Training & evaluation loop.\n",
        "for step in range(MAX_STEPS):\n",
        "    data = next(train_dataset)\n",
        "    params, opt_state = update(params, opt_state, data)\n",
        "    losses.append(calculate_batch_loss(params, data, vocab_size))\n",
        "\n",
        "    if step % LOG_EVERY == 0:\n",
        "        loss_ = jnp.array(losses).mean()\n",
        "        plotlosses.update(\n",
        "            {\n",
        "                \"loss\": loss_,\n",
        "            }\n",
        "        )\n",
        "        plotlosses.send()\n",
        "        losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-IvqQeuRX-L"
      },
      "outputs": [],
      "source": [
        "def get_model_prediction(params, data, generator_inference=True):\n",
        "  '''\n",
        "  Get the model output\n",
        "  '''\n",
        "  key = jax.random.PRNGKey(42)\n",
        "  logits, encoder_weights, decoder_weights = forward_fn.apply(\n",
        "      params, key, data, not generator_inference, True\n",
        "  )\n",
        "  argmax_out = jnp.argmax(logits, axis=-1)\n",
        "  text_out = [ord_to_char(out) for out in argmax_out]\n",
        "\n",
        "  if generator_inference:\n",
        "    return text_out, argmax_out, encoder_weights, decoder_weights\n",
        "  else:\n",
        "    return text_out, encoder_weights, decoder_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyfqX9AOF1fz"
      },
      "outputs": [],
      "source": [
        "data = next(train_dataset)\n",
        "\n",
        "predicted_string, encoder_weights, decoder_weights = get_model_prediction(\n",
        "    params, data, False\n",
        ")\n",
        "target_string = [ord_to_char(t) for t in data[\"target\"]]\n",
        "input_string = [ord_to_char(t) for t in data[\"input\"]]\n",
        "\n",
        "for y_hat, y, input_ in zip(predicted_string[:2], target_string[:2], input_string[:2]):\n",
        "    print(\"-\" * 10, \"Input\", \"-\" * 11)\n",
        "    print(input_)\n",
        "    print(\"-\" * 10, \"Target\", \"-\" * 10)\n",
        "    print(y)\n",
        "    print(\"-\" * 10, \"Prediction\", \"-\" * 10)\n",
        "    print(y_hat)\n",
        "    print()\n",
        "    print(\"*\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjeoBTTmzMUr"
      },
      "source": [
        "Lets plot the average attention matrices over a batch of data, and see what our model has learned. This is a very typical process in transformer development and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMeoRKVPXvPm"
      },
      "outputs": [],
      "source": [
        "def plot_weights(mean_weights, num_layers, num_heads, block=\"Encoder\"):\n",
        "    for layer in range(num_layers):\n",
        "        fig, ax = plt.subplots(1, num_heads, figsize=(45, 5))\n",
        "        plt.suptitle(f\"Encoder Layer {layer} attention heads\")\n",
        "        for h in range(num_heads):\n",
        "            ax[h].set_title(f\"Head {h}\")\n",
        "            sns.heatmap(mean_weights[layer, h, ...], ax=ax[h], cmap=\"Blues\")\n",
        "        fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jHgMM2A6zLnq"
      },
      "outputs": [],
      "source": [
        "mean_encoder_weights = jnp.mean(encoder_weights, axis=0)\n",
        "plot_weights(mean_encoder_weights, num_layers, num_heads, block=\"Encoder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMWj4j9AW1Mi"
      },
      "outputs": [],
      "source": [
        "mean_decoder_weights = jnp.mean(decoder_weights, axis=0)\n",
        "plot_weights(mean_decoder_weights, num_layers, num_heads, block=\"Decoder\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idM0U9G2146L"
      },
      "source": [
        "**Group task**: Discuss with your friend whether these outputs make sense."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK4kUFOsZbkJ"
      },
      "source": [
        "##### Training generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGVkYlYRZi2g"
      },
      "source": [
        "Next, let us train our generator and see how it performs. The previous method could have been solved by just using the encoder part, but now we show it is possible by utilising the decoder-specific changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqpbVJwKZmNp"
      },
      "outputs": [],
      "source": [
        "d_model = 128\n",
        "num_heads = 2\n",
        "num_layers = 3\n",
        "widening_factor = 2\n",
        "LR = 1e-3\n",
        "\n",
        "batch_size = 64\n",
        "seq_length = 32\n",
        "generative = True\n",
        "\n",
        "# set up the data\n",
        "train_dataset = AsciiDatasetForInversionTask(\"input.txt\", batch_size, seq_length)\n",
        "vocab_size = train_dataset.vocab_size\n",
        "batch = next(train_dataset)\n",
        "\n",
        "# initialise model\n",
        "rng = jax.random.PRNGKey(42)\n",
        "forward_fn = hk.transform(\n",
        "    build_forward_fn(\n",
        "        vocab_size, d_model, num_heads, num_layers, widening_factor, generative\n",
        "    )\n",
        ")\n",
        "params = forward_fn.init(rng, batch)\n",
        "\n",
        "# set up the optimiser\n",
        "optimiser = optax.chain(optax.clip_by_global_norm(1), optax.adam(LR, b1=0.9, b2=0.99))\n",
        "opt_state = optimiser.init(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_2ljKq8aRst"
      },
      "outputs": [],
      "source": [
        "plotlosses = PlotLosses()\n",
        "\n",
        "MAX_STEPS = 2500\n",
        "LOG_EVERY = 32\n",
        "losses = []\n",
        "\n",
        "key = jax.random.PRNGKey(42)\n",
        "# Training & evaluation loop.\n",
        "for step in range(MAX_STEPS):\n",
        "    data = next(train_dataset)\n",
        "    params, opt_state = update(params, opt_state, data)\n",
        "    losses.append(calculate_batch_loss(params, data, vocab_size))\n",
        "\n",
        "    if step % LOG_EVERY == 0:\n",
        "        loss_ = jnp.array(losses).mean()\n",
        "        plotlosses.update(\n",
        "            {\n",
        "                \"loss\": loss_,\n",
        "            }\n",
        "        )\n",
        "        plotlosses.send()\n",
        "        losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IP7crkAoaqZ-"
      },
      "outputs": [],
      "source": [
        "data = next(train_dataset)\n",
        "\n",
        "predicted_string, encoder_weights, decoder_weights = get_model_prediction(\n",
        "    params, data, False\n",
        ")\n",
        "target_string = [ord_to_char(t) for t in data[\"target\"]]\n",
        "input_string = [ord_to_char(t) for t in data[\"input\"]]\n",
        "\n",
        "for y_hat, y, input_ in zip(predicted_string[:2], target_string[:2], input_string[:2]):\n",
        "    print(\"-\" * 10, \"Input\", \"-\" * 11)\n",
        "    print(input_)\n",
        "    print(\"-\" * 10, \"Target\", \"-\" * 10)\n",
        "    print(y)\n",
        "    print(\"-\" * 10, \"Prediction\", \"-\" * 10)\n",
        "    print(y_hat)\n",
        "    print()\n",
        "    print(\"*\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fzq6UbqaACQv"
      },
      "outputs": [],
      "source": [
        "mean_encoder_weights = jnp.mean(encoder_weights, axis=0)\n",
        "plot_weights(mean_encoder_weights, num_layers, num_heads, block=\"Encoder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_YmMCTLYYDU"
      },
      "outputs": [],
      "source": [
        "mean_decoder_weights = jnp.mean(decoder_weights, axis=0)\n",
        "plot_weights(\n",
        "    jnp.where(mean_decoder_weights == 1, 0.5, mean_decoder_weights),\n",
        "    num_layers,\n",
        "    num_heads,\n",
        "    block=\"Decoder\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOvwOBchRX-N"
      },
      "source": [
        "When we generated the output above, we actually cheated a bit. We employed a technique called teacher forcing, where no matter what the decoder predicted, we will feed in the correct token at the next inference step. \n",
        "\n",
        "Let's now build a new sampling method, where we generate the input to the decoder over time as though we did not have the answer beforehand. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlJrzHTGRX-N"
      },
      "outputs": [],
      "source": [
        "# start by feeding in no tokens, except the special start token add in automatically\n",
        "data = next(train_dataset)\n",
        "data_no_target = {'input':data['input'], 'target':data['target'][:,:0]}\n",
        "\n",
        "print('Encoder input:')\n",
        "target_string = [ord_to_char(t) for t in data_no_target[\"input\"]]\n",
        "print(target_string[0])\n",
        "print('*'*50)\n",
        "\n",
        "# example of making one inference step\n",
        "new_input = data_no_target\n",
        "predicted_string, logits, encoder_weights, decoder_weights = get_model_prediction(\n",
        "    params, new_input, True\n",
        ")\n",
        "# addin in the new logits to data\n",
        "new_input['target'] = logits\n",
        "\n",
        "# now keep adding until we have reached the input sequence lenght\n",
        "print('Decoder prediction at each step:')\n",
        "for i in range(seq_length):\n",
        "  predicted_string, logits, encoder_weights, decoder_weights = get_model_prediction(\n",
        "      params, new_input, True\n",
        "  )\n",
        "\n",
        "  # addin in the new logits to data\n",
        "  new_input['target'] = logits\n",
        "  print(predicted_string[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8a9jwiW0E85"
      },
      "source": [
        "Finally, we implemented everything above by taking the token ID with the maximum probability of being correct. This is greedy decoding, as we only took the most likely token. It worked well in this use case, but there are cases where we will see a degrading performance when taking this greedy approach, specifically when we are interested in generating realistic text.\n",
        "\n",
        "Other methods exist for sampling from the decoder, with a famous algorithm being beam search. We provide resources below for anyone interested in learning more about this. \n",
        "\n",
        "[Greedy Decoding](https://www.youtube.com/watch?v=DW5C3eqAFQM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=4)\n",
        "\n",
        "[Beam Search](https://www.youtube.com/watch?v=uG3xoYNo3HM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35IPv-Ujjta4"
      },
      "source": [
        "### Decoder / Encoder only models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA6Kto9lCyRT"
      },
      "source": [
        "In this practical, we introduced what is known as a vanilla (original) transformer, where a decoder and encoder work together. However, there is no need to have this architecture, as both the encoder and decoder blocks can also operate separately on their own. Most of today's well-known transformers fall into decoder/encoder-only transformers. \n",
        "\n",
        "We are usually interested in training our model to extract semantics or representations from our data when we use only encoder blocks. We are interested in generative models when we use decoder-only blocks. These encoder blocks are, in many cases but not all cases, encoder blocks with masks, i.e. not two MHA blocks). Famous models for all three cases are listed below. \n",
        "\n",
        "**Vanilla transformers**:\n",
        "* [Original attention is all you need MLT](https://arxiv.org/abs/1706.03762?amp=1)\n",
        "* [T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html)\n",
        "* [BART](https://arxiv.org/abs/1910.13461)\n",
        "\n",
        "**Encoder only**:\n",
        "* [Bert](https://arxiv.org/abs/1810.04805)\n",
        "* [RoBERTa](https://arxiv.org/abs/1907.11692)\n",
        "* [ELECTRA](https://openreview.net/pdf?id=r1xMH1BtvB)\n",
        "\n",
        "**Decoder only**:\n",
        "* [GPT-3](https://arxiv.org/abs/2005.14165)\n",
        "* [CTRL](https://arxiv.org/abs/1909.05858)\n",
        "* [Transformer-XL](https://arxiv.org/abs/1901.02860)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE6hEihPhAhK"
      },
      "source": [
        "### Section Quiz \n",
        "\n",
        "Optional end of section quiz. Below is an example of an assessment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "L03B3HKwhAhK"
      },
      "outputs": [],
      "source": [
        "# @title Generate Quiz Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe \n",
        "\tsrc=\"https://forms.gle/xmH41DJL2C4pRrVu8\",\n",
        "  width=\"80%\" \n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rectkTs9iFHg"
      },
      "source": [
        "## **Hugging Face** - OPTIONAL (but recommended)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFBw8kRx-4Mk"
      },
      "source": [
        "\n",
        "\n",
        "<img src=\"data:image/png;base64,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\" width=\"10%\" />\n",
        "\n",
        "\n",
        "[Hugging Face](https://huggingface.co/) is a startup founded in 2016 and, in their own words: \"are on a mission to democratize good machine learning, one commit at a time.\"\n",
        "\n",
        "They have developed various open-source packages and allow users to easily interact with a large corpus of pretrained transformer models (across all modalities) and datasets to train or fine-tune pre-trained transformers.\n",
        "\n",
        "Their software is used widely in industry and research. The following sections show how one can interact with their various features, access SOTA models, and train your own model.\n",
        "\n",
        "This is an optional section for the practical. Still, working through it in the practical or afterwards is highly recommended. Understanding Hugging Face will allow you to build a very quick proof of concept system to test out various hypotheses, whereafter, the system you build can simply be used, or a new system can be developed with the knowledge that the original idea has merit. \n",
        "\n",
        "It should also be noted that various languages are still severely under-resourced, even in this ecosystem. See it as an opportunity also to see where the gaps are and how we as a community can reduce this gap. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq_pX3XTefu3"
      },
      "source": [
        "### Datasets Package - <font color='blue'>`Beginner`</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn-vfzXyehPu"
      },
      "source": [
        "Along with all the models being available, datasets are also hosted on the Hugging Face hub. One can visit the [dataset hub](https://huggingface.co/datasets), and browse for interesting datasets and very easilly access it through the [datasets](https://github.com/huggingface/datasets) package.\n",
        "\n",
        "Let's say for instance we want to build a text intent classification model. What we do then is to go to the link above, and use the search tags to find a dataset that seems like a good fit for us. By doing this, we find the[`banking77`](https://huggingface.co/datasets/banking77) dataset. Below we then load in this dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4StYbstjeitO"
      },
      "outputs": [],
      "source": [
        "# just for notebook cleanliness\n",
        "from datasets.utils.logging import set_verbosity_error\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"banking77\")\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMh4aox5eh8h"
      },
      "source": [
        "Here we see that the API returns a variable of DatasetDict type, which contains our dataset that has split into two datasets, i.e. train and test splits. We also see that each of these datasets contains `text` and `label` features. \n",
        "\n",
        "Let's investigate how the data looks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qFr15bkemIu"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzyH1lNWen3X"
      },
      "source": [
        "We see that our dataset splits are of type Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEQFsm0HepIm"
      },
      "outputs": [],
      "source": [
        "train_intents = dataset[\"train\"]\n",
        "test_intents = dataset[\"test\"]\n",
        "\n",
        "print(\"Text: \", train_intents[\"text\"][0])\n",
        "print(\"Label: \", train_intents[\"label\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7--kHR7eqej"
      },
      "source": [
        "Sometimes we want to work with only a subset of the dataset and we thus need to filter out the rest. Luckily, the datasets have an easy-to-use filter functionality. Below we filter to only see text which relates to label 11."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehbEghPSetvI"
      },
      "outputs": [],
      "source": [
        "# we are only the first 5 text samples\n",
        "train_intents.filter(lambda data: data[\"label\"] == 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvcEMvEDevSn"
      },
      "source": [
        "We can also apply a function to each data item by mapping the function to each \"row\" in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-0tUb6sewij"
      },
      "outputs": [],
      "source": [
        "# creating small dataset as example of using select\n",
        "example_dataset = train_intents.select(range(10))\n",
        "\n",
        "# printing first character of a text example\n",
        "example_dataset.map(lambda example: print(example[\"text\"][0]));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgt4MFR4ex14"
      },
      "source": [
        "Using the map function, we can also add a new column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gtdQeKVezT4"
      },
      "outputs": [],
      "source": [
        "def add_sentence_len(example):\n",
        "    example[\"lenght\"] = len(example[\"text\"])\n",
        "    return example\n",
        "\n",
        "\n",
        "# add new column\n",
        "example_dataset = example_dataset.map(lambda example: add_sentence_len(example))\n",
        "\n",
        "print(example_dataset)\n",
        "print(\"New data:\", example_dataset[\"lenght\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ_qP_Jqe0r5"
      },
      "source": [
        "**Code Task**: \n",
        "- We want to build a classifier out of this. We need to investigate the distribution of classes to see if our dataset is balanced or not. Write code that generates a dictionary containing the count of each class for both the train and test dataset.\n",
        "- Filter out classes with less than 150 classes in the training set, and ensure only the remaining classes are in the test set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42qddlWve2Zy"
      },
      "outputs": [],
      "source": [
        "# task 1\n",
        "unique_labels = # FINISH ME\n",
        "total_unique_labels = # FINISH ME\n",
        "\n",
        "train_counts = # FINISH ME\n",
        "test_counts = # FINISH ME\n",
        "\n",
        "# task 2\n",
        "passed_labels = [label for label, count in train_counts.items() if count>=150]\n",
        "train_intents = # FINISH ME\n",
        "test_intents = # FINISH ME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "jVHd4B1MnsNa"
      },
      "outputs": [],
      "source": [
        "# @title Run me to test your code\n",
        "real_total_unique_labels = 77\n",
        "test_counts_25 = 153\n",
        "num_test_rows = 960\n",
        "\n",
        "assert total_unique_labels == real_total_unique_labels, \"Task 1 failed\"\n",
        "assert test_counts_25 == test_counts[25], \"Task 2 failed\"\n",
        "assert num_test_rows == test_intents.num_rows, \"Task 2 failed\"\n",
        "\n",
        "print(\"It seems correct. Look at the answer below to compare methods.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ABrM5iAIe4YF"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "\n",
        "# # task 1\n",
        "unique_labels = np.unique(train_intents[\"label\"])\n",
        "total_unique_labels = len(unique_labels)\n",
        "\n",
        "train_counts = {label: sum(label == train_intents[\"label\"]) for label in unique_labels}\n",
        "test_counts = {label: sum(label == train_intents[\"label\"]) for label in unique_labels}\n",
        "\n",
        "# task 2\n",
        "passed_labels = [label for label, count in train_counts.items() if count >= 150]\n",
        "train_intents = train_intents.filter(lambda data: data[\"label\"] in passed_labels)\n",
        "test_intents = test_intents.filter(lambda data: data[\"label\"] in passed_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hz4U0IZ8e34k"
      },
      "source": [
        "**Other modalities**\n",
        "\n",
        "Text is not the only data that can be accessed, audio and image data can be accessed just as easily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPCHECJQiFU7"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "\n",
        "dataset = load_dataset(\"cgarciae/cartoonset\")\n",
        "Image.open(BytesIO(dataset[\"train\"][\"img_bytes\"][231]))\n",
        "\n",
        "# # free up a bit of space\n",
        "del dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL4j1JZse8wi"
      },
      "source": [
        "### Transformers Package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bKe1WDEe9eX"
      },
      "source": [
        "Now that we have a way of accessing data, let's shift our intention to accessing the hundreds of pretrained transformers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po9R6qiSe_HT"
      },
      "source": [
        "#### Pipeline - <font color='blue'>`Beginner`</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcK8-KpZfA7L"
      },
      "source": [
        "The easiest method to access a vast range of pre-trained models and use tasks is through the `pipeline` API.\n",
        "\n",
        "Pipelines group together a pretrained model found on their models hub with the preprocessing that was used during that model's training. To use the pipeline, one must import it from the [transformers](https://github.com/huggingface/transformers) library and specify the task and model you want.\n",
        "\n",
        "For a list of models, visit [this](https://huggingface.co/models) page that allows you to search through all models currently on the hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjTFOuylfDKH"
      },
      "outputs": [],
      "source": [
        "# When calling the function for the first time, the model, and its tokenizer, will be automatically downloaded\n",
        "sentiment_model = pipeline(\n",
        "    task=\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        ")\n",
        "print(sentiment_model(\"I love this practical!\"))\n",
        "print(sentiment_model(\"I hate this practical!\"))\n",
        "\n",
        "# passing more than one sentence\n",
        "sentence_batch = [\n",
        "    \"This is much quicker and easier to build a POC with than training everything from scratch\",\n",
        "    \"It really hurts when I stub my toe\",\n",
        "    \"I want to get ice cream\",\n",
        "]\n",
        "print(\"\\nBatch output:\")\n",
        "sentiment_model(sentence_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNYlFKWNfCL7"
      },
      "source": [
        "Notice that the first sentence we process in our batch of sentences is predicted to be Negative, with a relatively low score of 0.51, even though we feel this should be more neutral? The low score indicates this and we can interpret that when scores are low the actual label was meant to be Neutral, but this model was trained to do a binary prediction only.\n",
        "\n",
        "This model you just used is a Distellbert model, which was trained on *8 16GB V100s for 90 hours*, and you could use it as quickly as that.\n",
        "\n",
        "**Code Task:** Apply the zero-shot model to all of our test intent chatbot examples, extracting the predicted sentiment label into a new column, using the map function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00lgxVY8fKPw"
      },
      "outputs": [],
      "source": [
        "def get_sentiment(example):\n",
        "  # FINISH ME\n",
        "  return sentiment\n",
        "\n",
        "train_intents = # FINISH ME\n",
        "test_intents = # FINISH ME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KY_pOGR6fLYJ"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "def get_sentiment(example):\n",
        "    example[\"sentiment\"] = sentiment_model(example[\"text\"])[0][\"label\"]\n",
        "    return example\n",
        "\n",
        "\n",
        "test_intents = test_intents.map(lambda example: get_sentiment(example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txcTUTbUfMnT"
      },
      "outputs": [],
      "source": [
        "plt.title(\"Count of positive versus negative text inputs\")\n",
        "sns.countplot(test_intents[\"sentiment\"]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg3ti5iEfGIv"
      },
      "source": [
        "**Group code task (optional depending on time)**: (Hint: use the tags when searching the [model hub](https://huggingface.co/models))\n",
        "- Search for other pipeline tasks available, and discuss with your friend what you did and found. ([Hint](https://huggingface.co/docs/transformers/v4.20.1/en/main_classes/pipelines#pipelines))\n",
        "- Play with different language models and see how they perform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jeMfN56EfS7c"
      },
      "outputs": [],
      "source": [
        "your_pipeline = pipeline(\n",
        "    task=\"text-generation\",  # CHANGE ME TO OTHER STUFF\n",
        "    model=\"gpt2\",  # CHANGE ME AS WELL\n",
        ")\n",
        "text = \"I like ice-cream and \"\n",
        "your_pipeline(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSu5FY7CfRJ6"
      },
      "outputs": [],
      "source": [
        "# freeing up memory for future tasks\n",
        "del your_pipeline\n",
        "del sentiment_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSJ-Fh49fU-9"
      },
      "source": [
        "#### Training a chatbot intent model  - <font color='blue'>`Advanced`</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHzc2lpOfWVa"
      },
      "source": [
        "If you want more control than the pipeline API provides you, you can also use the predefined model classes.\n",
        "\n",
        "To showcase this, we will be training a custom model on top of a large transformer.\n",
        "\n",
        "To set the scene, let's say for instance that we want to train an intent model that can be used along with a chatbot. This intent model will be responsible to predict the true underlying intent found within the text.\n",
        "\n",
        "Your imaginary friend has built an intent model using TF-IDF techniques, but you think that you can use transformers for this task and that it will perform better.\n",
        "\n",
        "You have heard about the famous [DistilBERT](https://arxiv.org/pdf/1910.01108.pdf) model that was trained to extract text features, and you think this model can be a perfect fit to extract features for your intent model, as it is an encoder-only transformer architecture that produces strong token representations.\n",
        "\n",
        "To start your training process, you have two steps to follow:\n",
        "\n",
        "* Get the tokenizer\n",
        "* Get the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MBCwSG5i38v"
      },
      "source": [
        "###### **Getting the tokenizer**\n",
        "\n",
        "A tokenizer is responsible for preprocessing text into a format that is understandable to the model. It is very important to use the same tokenizer as the model you will be finetuning.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7ufSoUxi4uP"
      },
      "outputs": [],
      "source": [
        "# we want the Distilbert model, thus we import the correct tokenizer the model train\n",
        "from transformers import DistilBertTokenizer\n",
        "\n",
        "# we specify a specif distilbert\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "print(\"Tokenizer output:\")\n",
        "output = tokenizer(\"This is example text\")\n",
        "print(output)\n",
        "\n",
        "print(\"Tokens converted back to string\")\n",
        "print(tokenizer.decode(output[\"input_ids\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zW_VglCQfXfn"
      },
      "source": [
        "Looking at the output from the above, we that the special tokens are the `[CLS]` and `[SEP]` tokens. This is important to note, as we will be using the final output of the model for the `[CLS]` token when predicting the intent. \n",
        "\n",
        "This is a very common thing to do, where the token that indicates the start of the sentence is used when making predictions on the sentence. Seeing as our system is built in Jax, we will need to tell the tokenizer to return the data in the correct format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eR0yqg-mfZjF"
      },
      "outputs": [],
      "source": [
        "token_batch = tokenizer(\n",
        "    [\n",
        "        \"We are very happy to show you the 🤗 Transformers library.\",\n",
        "        \"We hope you don't hate it.\",\n",
        "    ],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"jax\",\n",
        ")\n",
        "print(token_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqBOZ4YLfa0H"
      },
      "source": [
        "Seeing as we want to use these tokens throughout our training process and for processing the embeddings,  let's map the tokenizer output to a new column for our train and test splits.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyDoAqXvfc_K"
      },
      "source": [
        "##### **Getting the transformer and generating embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxSkOij_fexe"
      },
      "source": [
        "Now that we can encode our text quickly into data that our transformer can process, let's gather and download the pretrained transformer model from the hub and generate representations for each text example.\n",
        "\n",
        "As was mentioned above, we are interested for now only in the `[CLS]` token embedding. One can of course look at averaging overall token (being sure to only use tokens that are not masked) and see how it compares. We leave this as an exercise for the reader to compare how it changes the performance of the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DFhSfCOKfgA5"
      },
      "outputs": [],
      "source": [
        "from transformers import FlaxDistilBertModel\n",
        "\n",
        "# we use FlaxDistillBertModel because we are in the JAX world\n",
        "distell_bert_model = FlaxDistilBertModel.from_pretrained(model_name)\n",
        "\n",
        "tokens = tokenizer([train_intents[0][\"text\"]])\n",
        "embeddings = distell_bert_model(**tokens)[0][:, 0]\n",
        "embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKgHz1_tfh10"
      },
      "source": [
        "Applying one by one is extremely slow, so lets rather infer in batches, using the `batch` flag in the map function. It is important here that the function that is being mapped works with batches and return the data in the correct format, i.e a dictionary with the new column name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPkm_JdsfjIX"
      },
      "outputs": [],
      "source": [
        "def get_embedding(batch):\n",
        "\n",
        "    text = batch[\"text\"]\n",
        "    tokens = tokenizer(\n",
        "        text,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=50,\n",
        "        return_tensors=\"jax\",\n",
        "    )\n",
        "\n",
        "    cls_embeddings = np.array(distell_bert_model(**tokens)[0][:, 0])\n",
        "    return {\"embedding\": cls_embeddings}\n",
        "\n",
        "\n",
        "train_intents = train_intents.map(lambda batch: get_embedding(batch), batched=True)\n",
        "test_intents = test_intents.map(lambda batch: get_embedding(batch), batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDcAZzm3fkfm"
      },
      "source": [
        "To see whether these embeddings are in any way useful, let's plot a few projected embeddings and their labels and see how it looks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbCy-UK7flz9"
      },
      "outputs": [],
      "source": [
        "# Sampling to get a clear picture with less data.\n",
        "sample_labels = [5, 11, 20, 28, 34, 45, 51, 76]\n",
        "plot_data = train_intents.filter(lambda data: data[\"label\"] in sample_labels)\n",
        "plot_projected_embeddings(plot_data[\"embedding\"], [str(l) for l in plot_data[\"label\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8agDZwufoFR"
      },
      "source": [
        "We see there are clear clusters forming, but there is definitely still some work that can be done here.\n",
        "\n",
        "Let's thus train a non-linear model on this data to try and find something that separates this intent.\n",
        "\n",
        "We have a choice really of what we want to do, given data our new dataset can just be interpreted as a database. We can either train another neural network, or we can train anything ranging from a logistic regression model to an XGBoost model.\n",
        "\n",
        "Let's start by training a neural network, as we are at the deep learning indaba ;)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KohN7yfbfmOP"
      },
      "outputs": [],
      "source": [
        "# extracting data into more usable state for all methods\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()  # labels should be from 0 - N\n",
        "\n",
        "train_embeddings = train_intents[\"embedding\"]\n",
        "train_labels = jnp.array(le.fit_transform(train_intents[\"label\"]))\n",
        "test_embeddings = test_intents[\"embedding\"]\n",
        "test_labels = jnp.array(le.transform(test_intents[\"label\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LxKYDyYfr3c"
      },
      "source": [
        "To load batches of embedding, we feed our extracted embeddings and labels into tensorflow datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og_F4Hyrfquy"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# creating tensorflow dataset loaders\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_embeddings, train_labels))\n",
        "train_ds = train_ds.shuffle(\n",
        "    buffer_size=len(train_embeddings), reshuffle_each_iteration=True\n",
        ").batch(64)\n",
        "\n",
        "# we do not want to shuffle test data\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((test_embeddings, test_labels))\n",
        "test_ds = test_ds.batch(64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW5-9GbNf1nX"
      },
      "source": [
        "##### **Training intent model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-UfWogVf3zc"
      },
      "source": [
        "Even though the methods below only show us updating the weights or parameters of downstream models, there is no reason that one can not finetune the entire Distilbert model on the new dataset and task. This will just require much more compute and in many cases the extra costs are not linearly correlated with improved performance. This is why in this practical, why only train downstream models utilising the pretrained embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekey34T1f5jK"
      },
      "source": [
        "###### MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFetHB7tf641"
      },
      "source": [
        "Our intent model will be 2 layer MLP. \n",
        "\n",
        "**Code task:** Finish the 2 layer MLP Haiku Module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie5_Hf8Ff75r"
      },
      "outputs": [],
      "source": [
        "# build a training model\n",
        "\n",
        "class IntentClassifier(hk.Module):\n",
        "  \"\"\"A MLP which predicts intent from transformer embeddings\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self._init_scale = .5\n",
        "    self.number_classes = #FILL ME IN\n",
        "\n",
        "  def __call__(self, embeddings):\n",
        "    embedding_size = # FILL ME IN\n",
        "    initializer = hk.initializers.VarianceScaling(self._init_scale)\n",
        "    projection_layer = hk.Linear(embedding_size, w_init=initializer)\n",
        "    classification_layer = # FILL ME IN\n",
        "\n",
        "    projections = jax.nn.relu(projection_layer(embeddings))\n",
        "\n",
        "    logits = # FILL ME IN\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Kjq62LOFf9Ik"
      },
      "outputs": [],
      "source": [
        "# @title Answer to code task (Try not to peek until you've given it a good try!'). This answer will be based on our previous answer.\n",
        "\n",
        "# build a training model\n",
        "\n",
        "\n",
        "class IntentClassifier(hk.Module):\n",
        "    \"\"\"A MLP which predicts intent from transformer embeddings\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self._init_scale = 0.5\n",
        "        self.number_classes = 38\n",
        "\n",
        "    def __call__(self, embeddings):\n",
        "        embedding_size = embeddings.shape[-1]\n",
        "        initializer = hk.initializers.VarianceScaling(self._init_scale)\n",
        "        projection_layer = hk.Linear(embedding_size, w_init=initializer)\n",
        "        classification_layer = hk.Linear(self.number_classes, w_init=initializer)\n",
        "\n",
        "        projections = jax.nn.relu(projection_layer(embeddings))\n",
        "        logits = classification_layer(projections)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bS8bmMangAOh"
      },
      "source": [
        "Next we build the Haiku training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZp6hQMNf_Dv"
      },
      "outputs": [],
      "source": [
        "# initiliase model and optmiser\n",
        "\n",
        "\n",
        "def classify_intent(embeddings):\n",
        "    model = IntentClassifier()\n",
        "    return model(embeddings)\n",
        "\n",
        "\n",
        "classify_intent = hk.transform(classify_intent)\n",
        "\n",
        "# initialise model\n",
        "rng = jax.random.PRNGKey(42)\n",
        "x = np.zeros([1, 768])\n",
        "params = classify_intent.init(rng, x)\n",
        "\n",
        "optimiser = optax.adam(1e-3)\n",
        "opt_state = optimiser.init(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwY8XsRMgEET"
      },
      "outputs": [],
      "source": [
        "# calculate loss\n",
        "key = jax.random.PRNGKey(42)\n",
        "\n",
        "\n",
        "def loss(params, batch):\n",
        "    \"\"\"Cross-entropy classification loss\"\"\"\n",
        "    batch_size = len(batch[\"labels\"])\n",
        "    logits = classify_intent.apply(params, key, batch[\"embeddings\"])\n",
        "    labels = jax.nn.one_hot(batch[\"labels\"], num_classes=38)\n",
        "    log_likelihood = jnp.sum(labels * jax.nn.log_softmax(logits))\n",
        "    return -log_likelihood / batch_size\n",
        "\n",
        "\n",
        "# update weights\n",
        "@jax.jit\n",
        "def update(params, opt_state, batch):\n",
        "    # get data neded for training\n",
        "    grads = jax.grad(loss)(params, batch)\n",
        "    updates, opt_state = optimiser.update(grads, opt_state)\n",
        "    params = optax.apply_updates(params, updates)\n",
        "    return params, opt_state\n",
        "\n",
        "\n",
        "# calculate accuracy per batch\n",
        "@jax.jit\n",
        "def accuracy(params, batch):\n",
        "    predictions = classify_intent.apply(params, key, batch[\"embeddings\"])\n",
        "    print(predictions)\n",
        "    return jnp.mean(jnp.argmax(predictions, axis=-1) == batch[\"labels\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOY2yfJun2Nd"
      },
      "outputs": [],
      "source": [
        "# Training & evaluation loop.\n",
        "from tqdm import tqdm\n",
        "\n",
        "for epoch in range(20):\n",
        "\n",
        "    train_accs = 0\n",
        "    total_calcs = 0\n",
        "    for batch in tqdm(train_ds, desc=\"Train steps\", leave=False):\n",
        "        batch = {\"embeddings\": jnp.array(batch[0]), \"labels\": jnp.array(batch[1])}\n",
        "        params, opt_state = update(params, opt_state, batch)\n",
        "        train_accs += accuracy(params, batch)\n",
        "        total_calcs += 1\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "        print(f\"At epoch: {epoch}\")\n",
        "        train_accs /= round(total_calcs, 2)\n",
        "\n",
        "        test_accs = 0\n",
        "        total_calcs = 0\n",
        "\n",
        "        for batch in tqdm(test_ds, desc=\"Test steps\", leave=False):\n",
        "            batch = {\"embeddings\": jnp.array(batch[0]), \"labels\": jnp.array(batch[1])}\n",
        "            test_accs += accuracy(params, batch)\n",
        "            total_calcs += 1\n",
        "\n",
        "        test_accs /= round(total_calcs, 2)\n",
        "\n",
        "        print(f\"\\nTrain accuracy:{train_accs}\")\n",
        "        print(f\"Test accuracy:{test_accs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCAs9gLVgDnq"
      },
      "source": [
        "Now that we have a trained MLP that can predict the intent, we need to write code that given new text, will classify an intent. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiCROb4lgZhb"
      },
      "outputs": [],
      "source": [
        "def predict_new_text_mlp(text):\n",
        "    embedding = get_embedding({\"text\": [text]})[\"embedding\"]\n",
        "    logits = classify_intent.apply(params, key, embedding)\n",
        "    predicted_intent = jnp.argmax(jax.nn.softmax(logits))\n",
        "    converted_back_intent = le.inverse_transform([predicted_intent])[0]\n",
        "    print(f'Predicted intent for input: \"{text}\" is {int(converted_back_intent)}')\n",
        "\n",
        "    index = jnp.where(train_labels == predicted_intent)[0]\n",
        "    print(f'Example training data from this class: {train_intents[index][\"text\"][0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9o8sN2-gbIw"
      },
      "outputs": [],
      "source": [
        "predict_new_text_mlp(\"Can I get a refund please\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvyQtDmggcvn"
      },
      "source": [
        "###### Logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5TgD99zgeQO"
      },
      "source": [
        "As a final experiment, we will see how a logistic regression model performs against our MLP, and how quick it can be to get very quick results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxVGB2PGggQ4"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "model = LogisticRegression(max_iter=6000)\n",
        "model.fit(train_embeddings, train_labels)\n",
        "y_hat_train = model.predict(train_embeddings)\n",
        "y_hat_test = model.predict(test_embeddings)\n",
        "\n",
        "train_accuracy = jnp.sum(y_hat_train == train_labels) / len(y_hat_train)\n",
        "test_accuracy = jnp.sum(y_hat_test == test_labels) / len(y_hat_test)\n",
        "\n",
        "print(f\"Train accuracy:{train_accuracy}\")\n",
        "print(f\"Test accuracy:{test_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQvBBHhYgkv0"
      },
      "source": [
        "As you can see, in just those few lines of code we have coded a logistic regression model that can classify the intent of a given user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovi4HNFfgiUc"
      },
      "outputs": [],
      "source": [
        "def predict_new_text_lr(text):\n",
        "    embedding = get_embedding({\"text\": [text]})[\"embedding\"]\n",
        "    predicted_intent = model.predict(embedding)\n",
        "    converted_back_intent = le.inverse_transform([predicted_intent])[0]\n",
        "    print(f'Predicted intent for input: \"{text}\" is {int(converted_back_intent)}')\n",
        "\n",
        "    index = jnp.where(train_labels == predicted_intent)[0]\n",
        "    print(f'Example training data from this class: {train_intents[index][\"text\"][0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJMvLsDEgmj_"
      },
      "outputs": [],
      "source": [
        "predict_new_text_lr(\"Can I please get a refund\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "## Conclusion\n",
        "**Summary:**\n",
        "\n",
        "Welcome to the end of the practical! Let us list everything you have accomplished:\n",
        "\n",
        "* Learned about attention and how it improved sequence-to-sequence modeling.\n",
        "* Implemented your very own implementation of MHA\n",
        "* Learned about the inner working of a transformer, building each block on your own\n",
        "* Combined all your work into a single transformer, which you trained to reverse a sequence.\n",
        "* Learned about Huggingface and used a pre-trained Roberta model to train your own banking chatbot intent model.\n",
        "\n",
        "**Next Steps:** \n",
        "\n",
        "Go down the rabbit hole of the references given below.\n",
        "\n",
        "**References:** \n",
        "\n",
        "Attention section:\n",
        "* [NLP817](https://www.kamperh.com/nlp817/)\n",
        "* [Attention is All You Need](https://arxiv.org/abs/1706.03762?amp=1)\n",
        "* [Attention? Attention!](https://lilianweng.github.io/posts/2018-06-24-attention/)\n",
        "* [Neural Machine Translation By Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf)\n",
        "* [Illustrated word2vec ](https://jalammar.github.io/illustrated-word2vec/)\n",
        "* [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)\n",
        "* [Lena Viota Seq2seq and Attention course](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)\n",
        "  \n",
        "Transformers sections:\n",
        "* [Deepmind Haiku example](https://github.com/deepmind/dm-haiku/tree/main/examples/transformer)\n",
        "* [Haiku Documentation](https://dm-haiku.readthedocs.io/en/latest/)\n",
        "* [Hugging Face Preprocess tutorial](https://huggingface.co/docs/transformers/preprocessing)\n",
        "* [Hugging Face Tokenizer summary ](https://huggingface.co/docs/transformers/tokenizer_summary)\n",
        "* [BERT](https://arxiv.org/abs/1810.04805)\n",
        "* [Transformer Architecture: The Positional Encoding](https://kazemnejad.com/blog/transformer_architecture_positional_encoding/)\n",
        "* [Andrej Karpathy char-rnn repo](https://github.com/karpathy/char-rnn)\n",
        "\n",
        "\n",
        "Transformer papers:\n",
        "* [Original attention is all you need MLT](https://arxiv.org/abs/1706.03762?amp=1)\n",
        "* [T5](https://arxiv.org/abs/1910.10683)\n",
        "* [BART](https://arxiv.org/abs/1910.13461)\n",
        "* [Bert](https://arxiv.org/abs/1810.04805)\n",
        "* [RoBERTa](https://arxiv.org/abs/1907.11692)\n",
        "* [ELECTRA](https://openreview.net/pdf?id=r1xMH1BtvB)\n",
        "* [GPT-3](https://arxiv.org/abs/2005.14165)\n",
        "* [CTRL](https://arxiv.org/abs/1909.05858)\n",
        "* [Transformer-XL](https://arxiv.org/abs/1901.02860)\n",
        "\n",
        "For other practicals from the Deep Learning Indaba, please visit [here](https://github.com/deep-learning-indaba/indaba-pracs-2022)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1ndpYE50BpG"
      },
      "source": [
        "## Feedback\n",
        "\n",
        "Please provide feedback that we can use to improve our practicals in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "OIZvkhfRz9Jz"
      },
      "outputs": [],
      "source": [
        "# @title Generate Feedback Form. (Run Cell)\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\n",
        "    \"\"\"\n",
        "<iframe \n",
        "\tsrc=\"https://forms.gle/bvLLPX74LMGrFefo9\",\n",
        "  width=\"80%\" \n",
        "\theight=\"1200px\" >\n",
        "\tLoading...\n",
        "</iframe>\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oglV4kHMWnIN"
      },
      "source": [
        "<img src=\"https://baobab.deeplearningindaba.com/static/media/indaba-logo-dark.d5a6196d.png\" width=\"50%\" />"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "6EqhIg1odqg0",
        "1jWreoeTBUCD",
        "B3e4e_vSP8qM"
      ],
      "name": "attention_and_transformers.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.9 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
